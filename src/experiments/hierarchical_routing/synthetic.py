"""
Large-Scale Classification: Hierarchical Routing vs. Softmax
============================================================

This experiment conducts a rigorous comparison between the `HierarchicalRoutingLayer`
and a standard `Dense` -> `Softmax` output layer on a synthetic, large-scale
classification task with 1024 distinct classes.

The primary objective is to evaluate the trade-offs between computational
performance and classification accuracy when scaling to a large number of
output logits. The logarithmic complexity of the hierarchical approach is
theoretically superior to the linear complexity of softmax, and this
experiment aims to quantify that advantage in a practical setting.

Experimental Design
-------------------

**Dataset**: Synthetic High-Dimensional Data
- A non-trivial classification problem is generated by a "teacher" MLP.
- **Input**: High-dimensional feature vectors (e.g., 256 dimensions).
- **Output**: 1024 unique classes.
- The teacher network creates a complex but learnable mapping from inputs
  to labels, ensuring a meaningful learning task.
- 100,000 training samples and 20,000 test samples.

**Model Architecture**: A consistent MLP backbone is used for both models,
with only the final output layer differing. The base architecture includes:
- An input layer accepting 256-dimensional vectors.
- A series of dense hidden layers (e.g., [512, 512]) with ReLU activations.
- Batch normalization and dropout for regularization.

**Output Layers Evaluated**:

1. **Standard Softmax**: The baseline. A `Dense` layer with 1024 units
   produces logits, followed by a `Softmax` activation.
   - Computational Complexity: O(D * N), where D is the feature dimension
     and N is the number of classes.

2. **Hierarchical Routing**: The challenger. A `HierarchicalRoutingLayer`
   with `output_dim=1024`.
   - Computational Complexity: O(D * logâ‚‚N), offering a significant
     theoretical speedup for large N.

Comprehensive Analysis Pipeline
------------------------------

This experiment goes beyond accuracy to provide a holistic comparison:

**1. Performance Profiling (Key Focus)**:
- **Training Time**: Total wall-clock time to train each model to convergence.
- **Inference Speed**: Average time to predict on a batch of test data,
  measured over multiple runs to ensure stability.

**2. Model Accuracy Evaluation**:
- Test set accuracy and top-5 accuracy.
- Final loss values on the test set.

**3. Calibration and Prediction Analysis**:
- Expected Calibration Error (ECE) and Brier Score.
- Entropy analysis to compare the confidence of the output distributions.

Expected Outcomes and Insights
------------------------------

This experiment is designed to answer critical questions for practitioners:

1. **Quantifiable Speedup**: What is the real-world reduction in training and
   inference time when using hierarchical routing for 1024 classes?

2. **Accuracy Impact**: Is there a statistically significant drop in accuracy
   when replacing the exhaustive softmax layer with the probabilistic tree?

3. **Training Dynamics**: Does the hierarchical decision process alter
   convergence behavior or stability?

4. **Scalability Verdict**: Based on the results, can `HierarchicalRoutingLayer`
   be recommended as a viable, high-performance alternative for tasks like
   large-vocabulary NLP or fine-grained image recognition?
"""

# ==============================================================================
# IMPORTS AND DEPENDENCIES
# ==============================================================================

import gc
import time
import keras
import numpy as np
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, field
from typing import Dict, Any, List, Tuple

from dl_techniques.utils.logger import logger
from dl_techniques.utils.train import TrainingConfig, train_model
from dl_techniques.layers.hierarchical_routing import HierarchicalRoutingLayer

# Visualization imports
from dl_techniques.visualization import (
    VisualizationManager,
    TrainingHistory,
    TrainingCurvesVisualization,
)

# Analysis imports
from dl_techniques.analyzer import (
    ModelAnalyzer,
    AnalysisConfig,
    DataInput
)


# ==============================================================================
# DATA GENERATION UTILITIES
# ==============================================================================

@dataclass
class SyntheticData:
    """Container for the synthetic dataset."""
    x_train: np.ndarray
    y_train: np.ndarray
    x_test: np.ndarray
    y_test: np.ndarray
    feature_dim: int
    num_classes: int


def generate_synthetic_data(
    num_samples: int,
    feature_dim: int,
    num_classes: int,
    test_split: float = 0.2
) -> SyntheticData:
    """
    Generates a synthetic dataset for large-scale classification.

    A "teacher" MLP model is used to generate labels for random input data,
    creating a complex but learnable classification task.

    Args:
        num_samples: Total number of samples to generate.
        feature_dim: The dimensionality of the input feature vectors.
        num_classes: The number of output classes.
        test_split: The fraction of data to reserve for the test set.

    Returns:
        A SyntheticData object containing the train and test splits.
    """
    logger.info(
        f"Generating synthetic data: {num_samples} samples, "
        f"{feature_dim} features, {num_classes} classes..."
    )

    # 1. Define a "teacher" model to create a non-linear mapping
    teacher_input = keras.layers.Input(shape=(feature_dim,))
    x = keras.layers.Dense(512, activation='relu', kernel_initializer='he_normal')(teacher_input)
    x = keras.layers.BatchNormalization()(x)
    x = keras.layers.Dense(num_classes, kernel_initializer='glorot_uniform')(x)
    teacher_model = keras.Model(inputs=teacher_input, outputs=x)

    # 2. Generate random input data
    X = np.random.randn(num_samples, feature_dim).astype("float32")

    # 3. Generate labels using the teacher model
    logger.info("Using teacher model to generate labels...")
    logits = teacher_model.predict(X, batch_size=256, verbose=0)
    y_indices = np.argmax(logits, axis=1)

    # 4. One-hot encode the labels
    y = keras.utils.to_categorical(y_indices, num_classes=num_classes)

    # 5. Split into training and testing sets
    num_test = int(num_samples * test_split)
    x_train, x_test = X[:-num_test], X[-num_test:]
    y_train, y_test = y[:-num_test], y[-num_test:]

    logger.info("Synthetic data generation complete.")
    logger.info(f"Train set: X={x_train.shape}, Y={y_train.shape}")
    logger.info(f"Test set:  X={x_test.shape}, Y={y_test.shape}")

    return SyntheticData(
        x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test,
        feature_dim=feature_dim, num_classes=num_classes
    )


# ==============================================================================
# EXPERIMENT CONFIGURATION
# ==============================================================================

@dataclass
class ExperimentConfig:
    """Configuration for the large-scale classification experiment."""

    # --- Dataset Configuration ---
    dataset_name: str = "synthetic_large_scale"
    num_classes: int = 1024
    feature_dim: int = 256
    num_samples: int = 100_000
    input_shape: Tuple[int, ...] = (feature_dim,)

    # --- Model Architecture Parameters (MLP) ---
    dense_units: List[int] = field(default_factory=lambda: [512, 512])
    dropout_rate: float = 0.3
    weight_decay: float = 1e-5
    kernel_initializer: str = 'he_normal'
    use_batch_norm: bool = True

    # --- Training Parameters ---
    epochs: int = 50
    batch_size: int = 256
    learning_rate: float = 0.001
    early_stopping_patience: int = 10
    monitor_metric: str = 'val_accuracy'
    loss_function: keras.losses.Loss = field(
        default_factory=lambda: keras.losses.CategoricalCrossentropy(from_logits=False)
    )

    # --- Models to Evaluate ---
    model_types: List[str] = field(default_factory=lambda: ['Softmax', 'HierarchicalRouting'])

    # --- Experiment Configuration ---
    output_dir: Path = Path("results")
    experiment_name: str = "large_scale_routing_vs_softmax"
    random_seed: int = 42

    # --- Analysis Configuration ---
    analyzer_config: AnalysisConfig = field(default_factory=lambda: AnalysisConfig(
        analyze_weights=False,
        analyze_calibration=True,
        calibration_bins=20,
        save_plots=True,
        plot_style='publication',
    ))


# ==============================================================================
# MODEL ARCHITECTURE BUILDER
# ==============================================================================

def build_model(config: ExperimentConfig, model_type: str, name: str) -> keras.Model:
    """
    Builds a complete MLP model with a specified output layer type.

    Args:
        config: The experiment configuration object.
        model_type: 'Softmax' or 'HierarchicalRouting'.
        name: A name prefix for the model and its layers.

    Returns:
        A compiled Keras model.
    """
    inputs = keras.layers.Input(shape=config.input_shape, name=f'{name}_input')
    x = inputs

    # MLP backbone (identical for all models)
    for i, units in enumerate(config.dense_units):
        x = keras.layers.Dense(
            units=units,
            kernel_initializer=config.kernel_initializer,
            kernel_regularizer=keras.regularizers.L2(config.weight_decay),
            name=f'dense_backbone_{i}'
        )(x)
        if config.use_batch_norm:
            x = keras.layers.BatchNormalization()(x)
        x = keras.layers.Activation('relu')(x)
        if config.dropout_rate > 0:
            x = keras.layers.Dropout(config.dropout_rate)(x)

    # --- Interchangeable Output Layer ---
    logger.info(f"Building model with '{model_type}' output layer.")
    if model_type == 'Softmax':
        logits = keras.layers.Dense(
            units=config.num_classes,
            kernel_initializer='glorot_uniform',
            name='logits'
        )(x)
        predictions = keras.layers.Activation('softmax', name='predictions')(logits)
    elif model_type == 'HierarchicalRouting':
        predictions = HierarchicalRoutingLayer(
            output_dim=config.num_classes,
            name='predictions'
        )(x)
    else:
        raise ValueError(f"Unknown model_type: {model_type}")

    model = keras.Model(inputs=inputs, outputs=predictions, name=f'{name}_model')

    optimizer = keras.optimizers.AdamW(learning_rate=config.learning_rate)
    model.compile(
        optimizer=optimizer,
        loss=config.loss_function,
        metrics=[
            keras.metrics.CategoricalAccuracy(name='accuracy'),
            keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')
        ]
    )
    return model


# ==============================================================================
# MAIN EXPERIMENT RUNNER
# ==============================================================================

def run_experiment(config: ExperimentConfig) -> Dict[str, Any]:
    """
    Runs the complete large-scale classification experiment.
    """
    keras.utils.set_random_seed(config.random_seed)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    experiment_dir = config.output_dir / f"{config.experiment_name}_{timestamp}"
    experiment_dir.mkdir(parents=True, exist_ok=True)

    vis_manager = VisualizationManager(
        experiment_name=config.experiment_name,
        output_dir=experiment_dir / "visualizations"
    )
    vis_manager.register_template("training_curves", TrainingCurvesVisualization)

    logger.info("Starting Large-Scale Classification Experiment")
    logger.info(f"Comparing: {config.model_types}")
    logger.info(f"Results will be saved to: {experiment_dir}")
    logger.info("=" * 80)

    # --- Data Generation ---
    data = generate_synthetic_data(
        num_samples=config.num_samples,
        feature_dim=config.feature_dim,
        num_classes=config.num_classes
    )

    # --- Model Training and Profiling ---
    trained_models = {}
    all_histories = {}
    performance_profiling = {}

    for model_type in config.model_types:
        logger.info(f"--- Processing model: {model_type} ---")
        model = build_model(config, model_type, model_type)
        model.summary(print_fn=logger.info)

        training_config = TrainingConfig(
            epochs=config.epochs, batch_size=config.batch_size,
            early_stopping_patience=config.early_stopping_patience,
            monitor_metric=config.monitor_metric, model_name=model_type,
            output_dir=experiment_dir / "training_plots" / model_type
        )

        # Profile training time
        start_time = time.perf_counter()
        history = train_model(
            model, data.x_train, data.y_train, data.x_test, data.y_test, training_config
        )
        end_time = time.perf_counter()
        training_time = end_time - start_time
        logger.info(f"[{model_type}] Total training time: {training_time:.2f} seconds")

        # Profile inference time
        logger.info(f"[{model_type}] Profiling inference speed...")
        inference_times = []
        for _ in range(20):  # Multiple runs for stable measurement
            start_inf = time.perf_counter()
            _ = model.predict(data.x_test, batch_size=config.batch_size, verbose=0)
            end_inf = time.perf_counter()
            inference_times.append(end_inf - start_inf)
        avg_inference_time = np.mean(inference_times)
        logger.info(f"[{model_type}] Average inference time: {avg_inference_time * 1000:.2f} ms")

        # Store results
        trained_models[model_type] = model
        all_histories[model_type] = history.history
        performance_profiling[model_type] = {
            'training_time_sec': training_time,
            'avg_inference_time_ms': avg_inference_time * 1000,
        }
        logger.info(f"--- Finished processing {model_type} ---")

    gc.collect()

    # --- Analysis and Visualization ---
    logger.info("Performing analysis with ModelAnalyzer...")
    analyzer = ModelAnalyzer(
        models=trained_models, config=config.analyzer_config,
        output_dir=experiment_dir / "model_analysis"
    )
    model_analysis_results = analyzer.analyze(data=DataInput.from_object(data))

    logger.info("Generating training history plots...")
    training_histories = {
        name: TrainingHistory.from_keras_history(hist)
        for name, hist in all_histories.items()
    }
    if training_histories:
        vis_manager.visualize(
            data=training_histories, plugin_name="training_curves",
            metrics_to_plot=['accuracy', 'loss'], show=False
        )

    # --- Final Evaluation ---
    logger.info("Evaluating final model accuracy on test set...")
    performance_results = {}
    for name, model in trained_models.items():
        eval_results = model.evaluate(data.x_test, data.y_test, verbose=0)
        performance_results[name] = dict(zip(model.metrics_names, eval_results))

    # --- Compile and Report Results ---
    results_payload = {
        'performance_profiling': performance_profiling,
        'performance_analysis': performance_results,
        'model_analysis': model_analysis_results,
        'histories': all_histories,
    }
    print_experiment_summary(results_payload)
    return results_payload


# ==============================================================================
# RESULTS REPORTING
# ==============================================================================

def print_experiment_summary(results: Dict[str, Any]) -> None:
    """Prints a comprehensive summary of the experimental results."""
    logger.info("=" * 80)
    logger.info(f"EXPERIMENT SUMMARY: Large-Scale Classification (1024 Classes)")
    logger.info("=" * 80)

    # --- Performance Profiling Section (Most Important) ---
    if 'performance_profiling' in results:
        logger.info("PERFORMANCE PROFILING:")
        logger.info(f"{'Model':<25} {'Training Time (s)':<20} {'Inference Time (ms)':<20}")
        logger.info("-" * 70)
        for name, metrics in results['performance_profiling'].items():
            train_time = metrics.get('training_time_sec', 0.0)
            infer_time = metrics.get('avg_inference_time_ms', 0.0)
            logger.info(f"{name:<25} {train_time:<20.2f} {infer_time:<20.2f}")

    # --- Accuracy Metrics Section ---
    if 'performance_analysis' in results:
        logger.info("\nACCURACY METRICS (on Full Test Set):")
        logger.info(f"{'Model':<25} {'Accuracy':<12} {'Top-5 Acc':<12} {'Loss':<12}")
        logger.info("-" * 65)
        for name, metrics in results['performance_analysis'].items():
            acc = metrics.get('accuracy', 0.0)
            top5 = metrics.get('top_5_accuracy', 0.0)
            loss = metrics.get('loss', 0.0)
            logger.info(f"{name:<25} {acc:<12.4f} {top5:<12.4f} {loss:<12.4f}")

    # --- Calibration Metrics Section ---
    model_analysis = results.get('model_analysis')
    if model_analysis and model_analysis.calibration_metrics:
        logger.info("\nCALIBRATION METRICS:")
        logger.info(f"{'Model':<25} {'ECE':<12} {'Brier Score':<15}")
        logger.info("-" * 55)
        for name, metrics in model_analysis.calibration_metrics.items():
            ece = metrics.get('ece', 0.0)
            brier = metrics.get('brier_score', 0.0)
            logger.info(f"{name:<25} {ece:<12.4f} {brier:<15.4f}")

    logger.info("=" * 80)


# ==============================================================================
# MAIN EXECUTION
# ==============================================================================

def main() -> None:
    """Main execution function for running the experiment."""
    config = ExperimentConfig()

    logger.info("EXPERIMENT CONFIGURATION:")
    logger.info(f"   Task: {config.num_classes}-class classification")
    logger.info(f"   Model Types: {config.model_types}")
    logger.info(f"   Epochs: {config.epochs}, Batch Size: {config.batch_size}")
    logger.info(f"   Backbone: {len(config.dense_units)} hidden layers")
    logger.info("")

    try:
        _ = run_experiment(config)
        logger.info("Experiment completed successfully!")
    except Exception as e:
        logger.error(f"Experiment failed with error: {e}", exc_info=True)
        raise


# ==============================================================================
# SCRIPT ENTRY POINT
# ==============================================================================

if __name__ == "__main__":
    main()