"""
Quantify the learning process by analyzing training history.

This analyzer distills raw, epoch-by-epoch training logs into a compact
set of quantitative metrics that characterize a model's learning behavior.
It moves beyond simple visual inspection of training curves to provide
objective, comparable measures of training efficiency, stability, and the
degree of overfitting.

Architecture
------------
The analyzer operates on the `history` object generated by Keras during
model training. Its core function is to transform these time-series data
(loss and accuracy over epochs) into a set of scalar metrics. Each metric
is designed to answer a specific question about the training process: How
fast did the model learn? Was the convergence smooth or erratic? Did the
model overfit to the training data? By computing these metrics, it provides
a high-level summary that is ideal for comparing multiple models or the
results of hyperparameter sweeps.

Foundational Mathematics and Concepts
-------------------------------------
The metrics are derived from fundamental concepts in statistical learning
theory and time-series analysis:

-   **Overfitting Index**: This metric directly quantifies the generalization
    gap, a central concept in machine learning. It is calculated as the
    average difference between the validation loss and the training loss over
    the final phase of training: `Index = E[loss_val - loss_train]`. A
    positive value indicates that the model performs better on data it has
    seen than on new data, a hallmark of overfitting (high variance).
    Analyzing the final portion of training focuses the metric on the model's
    state near convergence, where overfitting is most pronounced.

-   **Training Stability**: This is measured by the standard deviation (`σ`)
    of the validation loss over a recent window of epochs. It serves as a
    proxy for the volatility of the optimization process. A high standard
    deviation suggests an unstable process, where the optimizer's steps may
    be too large (e.g., high learning rate), causing the validation loss to
    oscillate erratically within a local minimum of the loss landscape. A low
    value signifies a smooth and stable convergence.

-   **Epochs to Convergence**: This heuristic measures training efficiency by
    identifying the first epoch at which the model achieves a significant
    fraction (e.g., 95%) of its peak validation performance. It provides a
    practical measure of how quickly a model reaches its effective potential,
    distinguishing between models that learn rapidly and those that require
    prolonged training.

-   **Curve Smoothing**: To aid visualization and analysis, the raw, often
    noisy, training curves are smoothed using a moving average filter. This is
    a standard signal processing technique that removes high-frequency noise
    to reveal the underlying trends in the learning trajectory.

References
----------
1.  Goodfellow, I., Bengio, Y., & Courville, A. (2016). *Deep Learning*.
    MIT Press. (Chapters 5 and 7 discuss overfitting, the generalization
    gap, and optimization dynamics).
2.  Domhan, T., Springenberg, J. T., & Hutter, F. (2015). "Speeding up
    Automatic Hyperparameter Optimization of Deep Neural Networks by
    Extrapolation of Learning Curves." IJCAI. (Discusses the quantitative
    analysis of learning curves for model evaluation).

"""

import numpy as np
from typing import Dict, Any, Optional, List

# ---------------------------------------------------------------------
# local imports
# ---------------------------------------------------------------------

from dl_techniques.utils.logger import logger
from .base import BaseAnalyzer
from ..data_types import AnalysisResults, DataInput, TrainingMetrics
from ..constants import (
    CONVERGENCE_THRESHOLD, TRAINING_STABILITY_WINDOW, OVERFITTING_ANALYSIS_FRACTION,
    LOSS_PATTERNS, VAL_LOSS_PATTERNS, ACC_PATTERNS, VAL_ACC_PATTERNS
)
from ..utils import find_metric_in_history, smooth_curve

# ---------------------------------------------------------------------

class TrainingDynamicsAnalyzer(BaseAnalyzer):
    """Analyzes training dynamics from history."""

    def requires_data(self) -> bool:
        """Training dynamics analysis doesn't require input data."""
        return False

    def analyze(self, results: AnalysisResults, data: Optional[DataInput] = None,
                cache: Optional[Dict[str, Dict[str, Any]]] = None) -> None:
        """Analyze training history to understand how models learned."""
        logger.info("Analyzing training dynamics...")

        if not results.training_history:
            logger.warning("No training history available for analysis")
            return

        # Initialize training metrics container
        results.training_metrics = TrainingMetrics()

        for model_name, history in results.training_history.items():
            if not history:
                logger.warning(f"No training history available for {model_name}")
                continue

            # Compute quantitative metrics
            self._compute_training_metrics(model_name, history, results.training_metrics)

            # Apply smoothing if requested
            if self.config.smooth_training_curves:
                self._smooth_training_curves(model_name, history, results.training_metrics)

    def _compute_training_metrics(
            self,
            model_name: str,
            history: Dict[str, List[float]],
            metrics: TrainingMetrics
    ) -> None:
        """Compute quantitative metrics from training history with robust length handling."""
        # Extract metrics using flexible pattern matching
        train_loss = find_metric_in_history(history, LOSS_PATTERNS)
        val_loss = find_metric_in_history(history, VAL_LOSS_PATTERNS)
        train_acc = find_metric_in_history(history, ACC_PATTERNS)
        val_acc = find_metric_in_history(history, VAL_ACC_PATTERNS)

        # Epochs to convergence (95% of max validation accuracy)
        if val_acc is not None and len(val_acc) > 0:
            max_val_acc = max(val_acc)
            threshold = CONVERGENCE_THRESHOLD * max_val_acc
            epochs_to_conv = next((i for i, acc in enumerate(val_acc) if acc >= threshold), len(val_acc))
            metrics.epochs_to_convergence[model_name] = epochs_to_conv

        # Training stability score (lower is more stable)
        if val_loss and len(val_loss) > TRAINING_STABILITY_WINDOW:
            recent_losses = val_loss[-TRAINING_STABILITY_WINDOW:]
            stability_score = np.std(recent_losses)
            metrics.training_stability_score[model_name] = stability_score

        # Overfitting index with robust length handling
        if train_loss and val_loss:
            # Use the minimum length to ensure we're comparing the same epochs
            min_length = min(len(train_loss), len(val_loss))

            if min_length < 3:  # Need at least 3 epochs for meaningful analysis
                logger.warning(f"Insufficient epochs ({min_length}) for overfitting analysis of {model_name}")
                metrics.overfitting_index[model_name] = 0.0
                metrics.final_gap[model_name] = 0.0
            else:
                # Truncate both to same length
                train_loss_aligned = train_loss[:min_length]
                val_loss_aligned = val_loss[:min_length]

                # Calculate final third based on aligned length
                final_third_start = int(min_length * (1 - OVERFITTING_ANALYSIS_FRACTION))
                final_third_start = max(final_third_start, 1)  # Ensure at least 1 epoch in final third

                train_final = np.mean(train_loss_aligned[final_third_start:])
                val_final = np.mean(val_loss_aligned[final_third_start:])
                overfitting_index = val_final - train_final

                metrics.overfitting_index[model_name] = overfitting_index

                # Final gap using aligned losses
                metrics.final_gap[model_name] = val_loss_aligned[-1] - train_loss_aligned[-1]

                # Log if we had to truncate
                if len(train_loss) != len(val_loss):
                    logger.info(f"Aligned train/val losses for {model_name}: "
                              f"train={len(train_loss)} val={len(val_loss)} → {min_length} epochs")

        # Peak performance
        if val_acc is not None and len(val_acc) > 0:
            # Explicitly align val_acc and val_loss to prevent IndexErrors
            val_loss_safe = val_loss if val_loss is not None else []
            min_len = min(len(val_acc), len(val_loss_safe))
            val_acc_aligned = val_acc[:min_len]
            val_loss_aligned = val_loss_safe[:min_len]

            if val_acc_aligned:
                best_epoch = np.argmax(val_acc_aligned)
                peak_performance = {
                    'epoch': best_epoch,
                    'val_accuracy': val_acc_aligned[best_epoch]
                }

                # Add validation loss at best epoch if available
                if val_loss_aligned:
                    peak_performance['val_loss'] = val_loss_aligned[best_epoch]
                else:
                    peak_performance['val_loss'] = None

                metrics.peak_performance[model_name] = peak_performance

        # Log warning if no metrics found
        if not any([train_loss, val_loss, train_acc, val_acc]):
            logger.warning(
                f"No recognized training metrics found for {model_name}. Available keys: {list(history.keys())}")

    def _smooth_training_curves(self, model_name: str, history: Dict[str, List[float]],
                                metrics: TrainingMetrics) -> None:
        """Apply smoothing to training curves for cleaner visualization."""
        smoothed = {}

        for metric_name, values in history.items():
            if isinstance(values, list) and len(values) > self.config.smoothing_window:
                smoothed_values = smooth_curve(np.array(values), self.config.smoothing_window)
                smoothed[metric_name] = smoothed_values
            else:
                smoothed[metric_name] = values

        metrics.smoothed_curves[model_name] = smoothed

# ---------------------------------------------------------------------
