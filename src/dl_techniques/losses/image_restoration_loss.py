"""
DarkIR Loss Functions for Low-Light Image Enhancement and Restoration.

This module provides a comprehensive suite of specialized loss functions designed
for training deep learning models on low-light image enhancement, denoising, and
general image restoration tasks. All implementations are optimized for Keras 3.8.0
with TensorFlow 2.18.0 backend, following NHWC (Batch, Height, Width, Channels)
data format conventions.

Loss Functions
--------------

CharbonnierLoss
    Robust L1 loss variant using smooth approximation: sqrt((y_true - y_pred)^2 + Îµ^2).
    Provides reduced sensitivity to outliers compared to standard L1 loss, making it
    ideal for pixel-level reconstruction in image restoration tasks.

FrequencyLoss
    FFT-based loss that compares amplitude spectra in the frequency domain. Designed
    to restore high-frequency details (edges, textures) that are often lost in
    low-light conditions. Uses keras.ops.fft2 for backend-agnostic execution.

EdgeLoss
    Laplacian of Gaussian (LoG) approximation loss that emphasizes edge preservation.
    Computes loss on high-frequency residuals after Gaussian smoothing using depthwise
    convolution, ensuring sharp edges in enhanced images.

VGGLoss
    Perceptual loss using multi-scale VGG19 features pre-trained on ImageNet. Extracts
    features from five hierarchical layers (block1-5 conv1) and computes weighted L1
    distance. Ensures semantic and perceptual similarity beyond pixel-level accuracy.

EnhanceLoss
    Deep supervision loss for multi-scale training. Combines VGG perceptual loss with
    L1 pixel loss on dynamically downsampled ground truth to match intermediate
    low-resolution predictions from encoder-decoder architectures.

DarkIRCompositeLoss
    Main composite loss combining Charbonnier (pixel accuracy), SSIM (structural
    similarity), and optional VGG perceptual loss. Provides balanced optimization
    for low-light enhancement with configurable component weights.

Usage Example
-------------
>>> import keras
>>> from dl_techniques.losses.image_restoration_loss import DarkIRCompositeLoss, FrequencyLoss
>>>
>>> # Basic composite loss for training
>>> loss_fn = DarkIRCompositeLoss(
...     charbonnier_weight=1.0,
...     ssim_weight=0.2,
...     perceptual_weight=0.1
... )
>>>
>>> # Frequency loss for detail restoration
>>> freq_loss = FrequencyLoss(loss_weight=0.5, norm='l1')
>>>
>>> # Compile model
>>> model.compile(
...     optimizer=keras.optimizers.Adam(learning_rate=1e-4),
...     loss=loss_fn
... )

Input Requirements
------------------
- All loss functions expect NHWC format: (batch_size, height, width, channels)
- Images should be normalized to [0, 1] range for optimal performance
- VGGLoss specifically requires RGB images with 3 channels
- Minimum image size: 32x32 pixels (recommended: 128x128 or larger)

Backend Compatibility
---------------------
- Primary backend: TensorFlow 2.18.0
- Most operations use keras.ops for backend-agnostic execution
- Exceptions: SSIM (tf.image.ssim) - no keras.ops equivalent available
- FFT operations: keras.ops.fft2 for frequency domain analysis

Performance Considerations
--------------------------
- VGGLoss: Adds ~30% computational overhead due to feature extraction
- FrequencyLoss: FFT operations are memory-intensive for large images (>512x512)
- EdgeLoss: Minimal overhead (~5%) via depthwise convolution
- Recommended batch size: 8-16 for 256x256 images on 16GB GPU

Citation
--------
If you use these loss functions in research, please cite the original DarkIR paper:

    @inproceedings{darkir2023,
      title={DarkIR: Low-Light Image Restoration},
      author={...},
      booktitle={...},
      year={2023}
    }

Notes
-----
- All losses are serializable via keras.saving.register_keras_serializable
- Thread-safe for multi-GPU training with tf.distribute strategies
- Compatible with mixed precision training (fp16/bf16)
- VGG19 weights are automatically downloaded on first use (~550MB)
"""

import keras
from typing import Tuple
import tensorflow as tf

# ---------------------------------------------------------------------

@keras.saving.register_keras_serializable()
class CharbonnierLoss(keras.losses.Loss):
    """
    Charbonnier Loss (Robust L1 Loss).

    Provides smooth approximation to L1 loss, reducing sensitivity to outliers.
    Formula: sqrt((y_true - y_pred)^2 + epsilon^2)

    Args:
        epsilon: Small constant for numerical stability. Default: 1e-3
        reduction: Type of reduction to apply. Default: "sum_over_batch_size"
        name: Name of the loss function. Default: "charbonnier_loss"

    Input shape:
        - y_true: (batch_size, height, width, channels)
        - y_pred: (batch_size, height, width, channels)

    Output shape:
        - Scalar loss value (if reduction is applied) or (batch_size,)
    """

    def __init__(
        self,
        epsilon: float = 1e-3,
        reduction: str = "sum_over_batch_size",
        name: str = "charbonnier_loss"
    ):
        super().__init__(reduction=reduction, name=name)
        self.epsilon = epsilon

    def call(
        self,
        y_true: keras.KerasTensor,
        y_pred: keras.KerasTensor
    ) -> keras.KerasTensor:
        """
        Compute Charbonnier loss.

        Args:
            y_true: Ground truth images
            y_pred: Predicted images

        Returns:
            Loss tensor
        """
        diff = y_true - y_pred
        loss = keras.ops.sqrt(keras.ops.square(diff) + (self.epsilon ** 2))
        return keras.ops.mean(loss, axis=[1, 2, 3])

    def get_config(self) -> dict:
        """Return configuration for serialization."""
        config = super().get_config()
        config.update({"epsilon": self.epsilon})
        return config

# ---------------------------------------------------------------------

@keras.saving.register_keras_serializable()
class FrequencyLoss(keras.losses.Loss):
    """
    Frequency Loss using FFT amplitude comparison.

    Computes L1/L2 distance between FFT amplitudes to restore high-frequency
    details in images. Uses keras.ops.fft2 for backend-agnostic execution.

    Args:
        loss_weight: Scalar weight for this loss component. Default: 1.0
        norm: Distance metric, either 'l1' or 'l2'. Default: 'l1'
        reduction: Type of reduction to apply. Default: "sum_over_batch_size"
        name: Name of the loss function. Default: "frequency_loss"

    Input shape:
        - y_true: (batch_size, height, width, channels)
        - y_pred: (batch_size, height, width, channels)

    Output shape:
        - Scalar loss value (if reduction is applied) or (batch_size,)

    Notes:
        - FFT is computed per channel independently
        - Input images should be normalized to [0, 1] range
    """

    def __init__(
        self,
        loss_weight: float = 1.0,
        norm: str = 'l1',
        reduction: str = "sum_over_batch_size",
        name: str = "frequency_loss"
    ):
        super().__init__(reduction=reduction, name=name)
        self.loss_weight = loss_weight
        self.norm = norm

    def call(
        self,
        y_true: keras.KerasTensor,
        y_pred: keras.KerasTensor
    ) -> keras.KerasTensor:
        """
        Compute frequency loss via FFT amplitude comparison.

        Args:
            y_true: Ground truth images
            y_pred: Predicted images

        Returns:
            Weighted frequency loss
        """
        # Transpose to (batch, channels, height, width) so FFT is over (height, width)
        # keras.ops.fft2 computes FFT over the last two axes.
        y_true_nchw = keras.ops.transpose(y_true, (0, 3, 1, 2))
        y_pred_nchw = keras.ops.transpose(y_pred, (0, 3, 1, 2))

        # Create zero imaginary parts
        # Note: fft2 expects tuple of (real, imag)
        zeros_true = keras.ops.zeros_like(y_true_nchw)
        zeros_pred = keras.ops.zeros_like(y_pred_nchw)

        fft_true_real, fft_true_imag = keras.ops.fft2((y_true_nchw, zeros_true))
        fft_pred_real, fft_pred_imag = keras.ops.fft2((y_pred_nchw, zeros_pred))

        # Compute amplitude spectrum: sqrt(real^2 + imag^2)
        # Add epsilon for numerical stability
        amp_true = keras.ops.sqrt(
            keras.ops.square(fft_true_real) + keras.ops.square(fft_true_imag) + 1e-8
        )
        amp_pred = keras.ops.sqrt(
            keras.ops.square(fft_pred_real) + keras.ops.square(fft_pred_imag) + 1e-8
        )

        # Compute distance in frequency domain
        diff = amp_true - amp_pred

        if self.norm == 'l1':
            # Mean over Channels(1), Height(2), Width(3) -> result (Batch,)
            loss = keras.ops.mean(keras.ops.abs(diff), axis=[1, 2, 3])
        else:  # l2
            loss = keras.ops.mean(keras.ops.square(diff), axis=[1, 2, 3])

        return loss * self.loss_weight

    def get_config(self) -> dict:
        """Return configuration for serialization."""
        config = super().get_config()
        config.update({
            "loss_weight": self.loss_weight,
            "norm": self.norm
        })
        return config

# ---------------------------------------------------------------------

@keras.saving.register_keras_serializable()
class EdgeLoss(keras.losses.Loss):
    """
    Edge Loss via Laplacian of Gaussian approximation.

    Emphasizes edges by computing loss on high-frequency residuals after
    Gaussian smoothing. Simplified from full Laplacian pyramid for efficiency.

    Args:
        loss_weight: Scalar weight for this loss component. Default: 1.0
        channels: Number of image channels. Default: 3
        reduction: Type of reduction to apply. Default: "sum_over_batch_size"
        name: Name of the loss function. Default: "edge_loss"

    Input shape:
        - y_true: (batch_size, height, width, channels)
        - y_pred: (batch_size, height, width, channels)

    Output shape:
        - Scalar loss value (if reduction is applied) or (batch_size,)
    """

    def __init__(
        self,
        loss_weight: float = 1.0,
        channels: int = 3,
        reduction: str = "sum_over_batch_size",
        name: str = "edge_loss"
    ):
        super().__init__(reduction=reduction, name=name)
        self.loss_weight = loss_weight
        self.channels = channels

        # Build Gaussian kernel for edge detection
        # Define 1D Gaussian kernel: [0.05, 0.25, 0.4, 0.25, 0.05]
        k = keras.ops.convert_to_tensor(
            [0.05, 0.25, 0.4, 0.25, 0.05],
            dtype="float32"
        )

        # Create 2D Gaussian kernel via outer product
        kernel_2d = keras.ops.outer(k, k)  # (5, 5)

        # Reshape for depthwise convolution: (H, W, in_channels, depth_multiplier)
        # For depthwise conv with depth_multiplier=1
        kernel_2d = keras.ops.reshape(kernel_2d, (5, 5, 1, 1))

        # Replicate for each channel
        self.kernel = keras.ops.tile(
            kernel_2d,
            (1, 1, self.channels, 1)
        )  # (5, 5, channels, 1)

    def _gaussian_blur(self, img: keras.KerasTensor) -> keras.KerasTensor:
        """
        Apply Gaussian blur using depthwise convolution.

        Args:
            img: Input image tensor

        Returns:
            Blurred image tensor
        """
        # Pad to maintain size (5x5 kernel requires pad=2)
        img_padded = keras.ops.pad(
            img,
            [[0, 0], [2, 2], [2, 2], [0, 0]],
            mode="reflect"
        )

        # Apply depthwise convolution
        blurred = keras.ops.depthwise_conv(
            img_padded,
            self.kernel,
            strides=(1, 1),
            padding="valid"
        )

        return blurred

    def call(
        self,
        y_true: keras.KerasTensor,
        y_pred: keras.KerasTensor
    ) -> keras.KerasTensor:
        """
        Compute edge loss via Laplacian of Gaussian.

        Args:
            y_true: Ground truth images
            y_pred: Predicted images

        Returns:
            Weighted edge loss
        """
        # Compute high-frequency components (edges)
        edge_true = y_true - self._gaussian_blur(y_true)
        edge_pred = y_pred - self._gaussian_blur(y_pred)

        # MSE on edge maps
        loss = keras.ops.mean(
            keras.ops.square(edge_true - edge_pred),
            axis=[1, 2, 3]
        )

        return loss * self.loss_weight

    def get_config(self) -> dict:
        """Return configuration for serialization."""
        config = super().get_config()
        config.update({
            "loss_weight": self.loss_weight,
            "channels": self.channels
        })
        return config

# ---------------------------------------------------------------------

@keras.saving.register_keras_serializable()
class VGGLoss(keras.losses.Loss):
    """
    Perceptual Loss using VGG19 features.

    Extracts features from multiple VGG19 layers and computes weighted L1
    distance for perceptual similarity. Uses ImageNet pre-trained weights.

    Args:
        loss_weight: Overall scalar weight for this loss. Default: 1.0
        reduction: Type of reduction to apply. Default: "sum_over_batch_size"
        name: Name of the loss function. Default: "vgg_loss"

    Input shape:
        - y_true: (batch_size, height, width, 3) - RGB images in [0, 1]
        - y_pred: (batch_size, height, width, 3) - RGB images in [0, 1]

    Output shape:
        - Scalar loss value (if reduction is applied) or (batch_size,)

    Notes:
        - Input images are expected to be in [0, 1] range
        - Internal preprocessing normalizes using ImageNet statistics
        - VGG model is frozen (non-trainable)
    """

    def __init__(
        self,
        loss_weight: float = 1.0,
        reduction: str = "sum_over_batch_size",
        name: str = "vgg_loss"
    ):
        super().__init__(reduction=reduction, name=name)
        self.loss_weight = loss_weight
        # Layer-specific weights (decreasing importance for deeper layers)
        self.layer_weights = [1.0/32, 1.0/16, 1.0/8, 1.0/4, 1.0]

        # Load VGG19 without top classification layers
        vgg = keras.applications.VGG19(
            include_top=False,
            weights="imagenet"
        )
        vgg.trainable = False

        # Extract features from multiple layers
        # Using first conv layer of each block for perceptual features
        layer_names = [
            'block1_conv1',  # Early features (64 channels)
            'block2_conv1',  # (128 channels)
            'block3_conv1',  # (256 channels)
            'block4_conv1',  # (512 channels)
            'block5_conv1'   # Deep features (512 channels)
        ]

        outputs = [vgg.get_layer(name).output for name in layer_names]
        self.vgg_model = keras.Model(inputs=vgg.input, outputs=outputs)
        self.vgg_model.trainable = False

        # ImageNet normalization statistics
        self.mean = keras.ops.convert_to_tensor(
            [0.485, 0.456, 0.406],
            dtype="float32"
        )
        self.std = keras.ops.convert_to_tensor(
            [0.229, 0.224, 0.225],
            dtype="float32"
        )

    def _preprocess(self, x: keras.KerasTensor) -> keras.KerasTensor:
        """
        Preprocess images for VGG19.

        Args:
            x: Input images in [0, 1] range

        Returns:
            Normalized images
        """
        # Normalize using ImageNet statistics
        x_norm = (x - self.mean) / self.std
        return x_norm

    def call(
        self,
        y_true: keras.KerasTensor,
        y_pred: keras.KerasTensor
    ) -> keras.KerasTensor:
        """
        Compute perceptual loss using VGG19 features.

        Args:
            y_true: Ground truth images
            y_pred: Predicted images

        Returns:
            Weighted perceptual loss
        """
        # Preprocess inputs
        y_true_norm = self._preprocess(y_true)
        y_pred_norm = self._preprocess(y_pred)

        # Extract multi-scale features
        feat_true = self.vgg_model(y_true_norm)
        feat_pred = self.vgg_model(y_pred_norm)

        # Compute weighted L1 loss across all layers
        loss = keras.ops.convert_to_tensor(0.0, dtype="float32")
        for i, (ft, fp) in enumerate(zip(feat_true, feat_pred)):
            layer_loss = keras.ops.mean(
                keras.ops.abs(ft - fp),
                axis=[1, 2, 3]
            )
            loss = loss + self.layer_weights[i] * layer_loss

        return loss * self.loss_weight

    def get_config(self) -> dict:
        """Return configuration for serialization."""
        config = super().get_config()
        config.update({"loss_weight": self.loss_weight})
        return config

# ---------------------------------------------------------------------

@keras.saving.register_keras_serializable()
class EnhanceLoss(keras.losses.Loss):
    """
    Deep Supervision Loss for intermediate feature outputs.

    Combines VGG perceptual loss and L1 pixel loss on downsampled ground truth
    to match low-resolution intermediate features. Used for multi-scale training.

    Args:
        loss_weight: Weight for L1 component. Default: 1.0
        vgg_weight: Weight for VGG component. Default: 0.01
        scale_factor: Downsampling factor (not used, kept for compatibility). Default: 8
        reduction: Type of reduction to apply. Default: "sum_over_batch_size"
        name: Name of the loss function. Default: "enhance_loss"

    Input shape:
        - y_true: (batch_size, H_high, W_high, channels) - High-res ground truth
        - y_pred: (batch_size, H_low, W_low, channels) - Low-res prediction

    Output shape:
        - Scalar loss value (if reduction is applied) or (batch_size,)
    """

    def __init__(
        self,
        loss_weight: float = 1.0,
        vgg_weight: float = 0.01,
        scale_factor: int = 8,
        reduction: str = "sum_over_batch_size",
        name: str = "enhance_loss"
    ):
        super().__init__(reduction=reduction, name=name)
        self.loss_weight = loss_weight
        self.vgg_weight = vgg_weight
        self.scale_factor = scale_factor

        # Initialize sub-losses with no reduction (we handle it or pass through)
        self.vgg_loss = VGGLoss(loss_weight=1.0, reduction="none")
        self.l1_loss = keras.losses.MeanAbsoluteError(reduction="none")

    def call(
        self,
        y_true: keras.KerasTensor,
        y_pred: keras.KerasTensor
    ) -> keras.KerasTensor:
        """
        Compute enhance loss for deep supervision.

        Args:
            y_true: High-resolution ground truth
            y_pred: Low-resolution intermediate prediction

        Returns:
            Combined perceptual and pixel loss (per sample)
        """
        # Downsample ground truth to match prediction resolution
        pred_shape = keras.ops.shape(y_pred)
        h, w = pred_shape[1], pred_shape[2]

        gt_low_res = keras.ops.image.resize(
            y_true,
            (h, w),
            interpolation="bilinear"
        )

        # Compute loss components
        # vgg_loss returns (Batch,)
        perceptual = self.vgg_loss(gt_low_res, y_pred)

        # l1_loss returns (Batch, Height, Width) because of reduction="none"
        pixel_wise_map = self.l1_loss(gt_low_res, y_pred)

        # Reduce pixel_wise to (Batch,) by averaging over spatial dimensions
        # to match the scale/semantics of perceptual loss (which is mean over spatial)
        pixel_wise = keras.ops.mean(pixel_wise_map, axis=[1, 2])

        # Combine losses
        total_loss = (self.vgg_weight * perceptual) + (self.loss_weight * pixel_wise)

        return total_loss

    def get_config(self) -> dict:
        """Return configuration for serialization."""
        config = super().get_config()
        config.update({
            "loss_weight": self.loss_weight,
            "vgg_weight": self.vgg_weight,
            "scale_factor": self.scale_factor
        })
        return config

# ---------------------------------------------------------------------

@keras.saving.register_keras_serializable()
class DarkIRCompositeLoss(keras.losses.Loss):
    """
    Composite Loss for DarkIR training.

    Combines multiple loss components optimized for low-light image enhancement:
    - Charbonnier (robust L1) for pixel accuracy
    - SSIM for structural similarity
    - Optional VGG perceptual loss for semantic quality

    Args:
        charbonnier_weight: Weight for Charbonnier loss. Default: 1.0
        ssim_weight: Weight for SSIM loss. Default: 0.2
        perceptual_weight: Weight for VGG loss (0 to disable). Default: 0.0
        reduction: Type of reduction to apply. Default: "sum_over_batch_size"
        name: Name of the loss function. Default: "darkir_composite_loss"

    Input shape:
        - y_true: (batch_size, height, width, channels)
        - y_pred: (batch_size, height, width, channels)

    Output shape:
        - Scalar loss value (if reduction is applied) or (batch_size,)

    Notes:
        - SSIM computation uses TensorFlow backend (not available in keras.ops)
        - VGG loss only computed if perceptual_weight > 0
    """

    def __init__(
        self,
        charbonnier_weight: float = 1.0,
        ssim_weight: float = 0.2,
        perceptual_weight: float = 0.0,
        reduction: str = "sum_over_batch_size",
        name: str = "darkir_composite_loss",
        **kwargs
    ):
        super().__init__(name=name, reduction=reduction, **kwargs)
        self.charbonnier_weight = charbonnier_weight
        self.ssim_weight = ssim_weight
        self.perceptual_weight = perceptual_weight

        # Initialize component losses
        self.charb_loss = CharbonnierLoss(reduction="none")

        # Only create VGG loss if needed
        if perceptual_weight > 0:
            self.vgg_loss = VGGLoss(loss_weight=1.0, reduction="none")
        else:
            self.vgg_loss = None

    def call(
        self,
        y_true: keras.KerasTensor,
        y_pred: keras.KerasTensor
    ) -> keras.KerasTensor:
        """
        Compute composite loss.

        Args:
            y_true: Ground truth images
            y_pred: Predicted images

        Returns:
            Weighted sum of all loss components
        """
        # Pixel-level loss (Charbonnier) - returns (B,)
        loss = self.charbonnier_weight * self.charb_loss(y_true, y_pred)

        # Structural similarity loss (1 - SSIM)
        # Note: SSIM is not available in keras.ops, using TF backend
        ssim_val = tf.image.ssim(y_true, y_pred, max_val=1.0)
        loss = loss + self.ssim_weight * (1.0 - ssim_val)

        # Perceptual loss (optional) - returns (B,)
        if self.vgg_loss is not None:
            loss = loss + self.perceptual_weight * self.vgg_loss(y_true, y_pred)

        return loss

    def get_config(self) -> dict:
        """Return configuration for serialization."""
        config = super().get_config()
        config.update({
            "charbonnier_weight": self.charbonnier_weight,
            "ssim_weight": self.ssim_weight,
            "perceptual_weight": self.perceptual_weight
        })
        return config

# ---------------------------------------------------------------------