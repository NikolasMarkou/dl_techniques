"""
Holonomic Field Embeddings - Geometric Safety Through Lie Group Theory.

This module implements field embeddings based on the Special Orthogonal Group SO(n),
where tokens are represented as geometric transformations (rotation matrices) rather
than static vectors. This approach enforces geometric constraints that provide
inherent safety properties.

Theoretical Foundation
----------------------
Traditional embeddings map tokens to points in R^n. Field embeddings instead map
tokens to operators (rotation matrices in SO(n)) that transform a semantic reference
frame. This ensures:

1. **Energy Conservation**: Rotation matrices preserve norms (||Rx|| = ||x||)
2. **Non-Abelian Composition**: Order matters (AB ≠ BA), enforcing sequence dependencies
3. **Geometric Safety**: Semantic transitions must be smooth on the manifold
4. **Curvature Detection**: Sharp semantic changes create detectable "stress"

The implementation uses:
- Lie Algebra so(n): Skew-symmetric matrices as tangent space
- Lie Group SO(n): Rotation matrices via matrix exponential
- Path-ordered integration: Sequential composition of transformations
- Manifold stress: Measures geometric smoothness of semantic paths

References
----------
Based on theoretical deconstruction of "Holonomic AI" concepts:
- Gauge-invariant holonomy loops for prompt injection resistance
- Field embeddings with curvature-based provenance
- Non-Abelian metrics for adversarial robustness
"""

import keras
import tensorflow as tf
from typing import Optional, Union, Tuple, List
import numpy as np


@keras.saving.register_keras_serializable(package="dl_techniques")
class LieGroupEmbedding(keras.layers.Layer):
    """
    Maps token IDs to rotation matrices in SO(n) using Lie algebra exponential map.

    This layer replaces traditional embedding lookups with geometric transformations.
    Each token is represented as a rotation matrix generated by exponentiating a
    skew-symmetric matrix from the Lie algebra so(n).

    Mathematical Formulation
    ------------------------
    For input token index i:
    1. Retrieve weight matrix W_i ∈ R^(d×d)
    2. Enforce skew-symmetry: A_i = (W_i - W_i^T) / 2
    3. Map to Lie group: R_i = exp(A_i) ∈ SO(n)

    Properties:
    - R_i^T R_i = I (orthogonality)
    - det(R_i) = 1 (orientation preservation)
    - ||R_i x|| = ||x|| for all x (norm preservation)

    Parameters
    ----------
    vocab_size : int
        Size of the token vocabulary.
    embed_dim : int
        Dimension of the rotation matrices (must be same for both dimensions).
    use_expm : bool, default=True
        Whether to use matrix exponential (tf.linalg.expm) for accurate SO(n) mapping.
        If False, uses Padé approximation for efficiency (less geometrically pure).
    max_norm : Optional[float], default=None
        If provided, clips the Frobenius norm of algebra matrices before exponentiation.
    kernel_initializer : Union[str, keras.initializers.Initializer], default="glorot_uniform"
        Initializer for the Lie algebra weight matrices.
    kernel_regularizer : Optional[keras.regularizers.Regularizer], default=None
        Regularizer for the Lie algebra weights.
    name : Optional[str], default=None
        Name for the layer.
    **kwargs : dict
        Additional keyword arguments for the base Layer class.

    Attributes
    ----------
    algebra_weights : keras.Variable
        Learnable weight matrices of shape (vocab_size, embed_dim, embed_dim)
        representing elements of the Lie algebra so(n).

    Examples
    --------
    >>> # Create field embeddings for 1000 tokens with 16-dimensional rotations
    >>> embedding_layer = LieGroupEmbedding(vocab_size=1000, embed_dim=16)
    >>>
    >>> # Input: token indices
    >>> tokens = keras.random.randint((32, 128), minval=0, maxval=1000)
    >>>
    >>> # Output: rotation matrices
    >>> rotations = embedding_layer(tokens)
    >>> print(rotations.shape)  # (32, 128, 16, 16)
    >>>
    >>> # Verify orthogonality (approximately)
    >>> R = rotations[0, 0]  # Single rotation matrix
    >>> identity = keras.ops.matmul(keras.ops.transpose(R), R)
    >>> # Should be close to identity matrix

    Notes
    -----
    - Matrix exponential (expm) is computationally expensive but geometrically accurate
    - For production, consider using Padé approximation or caching for fixed vocabularies
    - The output tensors have shape (batch, seq_len, embed_dim, embed_dim)
    - Memory usage scales as O(vocab_size × embed_dim²)
    """

    def __init__(
            self,
            vocab_size: int,
            embed_dim: int,
            use_expm: bool = True,
            max_norm: Optional[float] = None,
            kernel_initializer: Union[str, keras.initializers.Initializer] = "glorot_uniform",
            kernel_regularizer: Optional[keras.regularizers.Regularizer] = None,
            name: Optional[str] = None,
            **kwargs
    ):
        super().__init__(name=name, **kwargs)

        if vocab_size <= 0:
            raise ValueError(f"vocab_size must be positive, got {vocab_size}")
        if embed_dim <= 0:
            raise ValueError(f"embed_dim must be positive, got {embed_dim}")

        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.use_expm = use_expm
        self.max_norm = max_norm
        self.kernel_initializer = keras.initializers.get(kernel_initializer)
        self.kernel_regularizer = kernel_regularizer

    def build(self, input_shape: Tuple[Optional[int], ...]) -> None:
        """
        Create the Lie algebra weight variables.

        Parameters
        ----------
        input_shape : tuple
            Shape of input tensor (batch_size, seq_len).
        """
        # Initialize Lie algebra weights (tangent space)
        # Shape: (vocab_size, embed_dim, embed_dim)
        self.algebra_weights = self.add_weight(
            shape=(self.vocab_size, self.embed_dim, self.embed_dim),
            initializer=self.kernel_initializer,
            regularizer=self.kernel_regularizer,
            trainable=True,
            name="lie_algebra_weights"
        )

        super().build(input_shape)

    def call(
            self,
            inputs: keras.KerasTensor,
            training: Optional[bool] = None
    ) -> keras.KerasTensor:
        """
        Transform token indices to rotation matrices via exponential map.

        Parameters
        ----------
        inputs : keras.KerasTensor
            Integer tensor of token indices with shape (batch_size, seq_len).
        training : Optional[bool], default=None
            Whether in training mode (unused but kept for API consistency).

        Returns
        -------
        keras.KerasTensor
            Rotation matrices with shape (batch_size, seq_len, embed_dim, embed_dim).
            Each matrix R satisfies R^T R = I and det(R) = 1 (approximately).
        """
        # 1. Retrieve algebra matrices for input tokens
        # Shape: (batch_size, seq_len, embed_dim, embed_dim)
        raw_matrices = keras.ops.take(self.algebra_weights, inputs, axis=0)

        # 2. Enforce skew-symmetry: A = (W - W^T) / 2
        # This ensures the matrix lies in the Lie algebra so(n)
        raw_matrices_t = keras.ops.transpose(raw_matrices, axes=[0, 1, 3, 2])
        skew_symmetric = 0.5 * (raw_matrices - raw_matrices_t)

        # 3. Optional: Clip norm for numerical stability
        if self.max_norm is not None:
            # Compute Frobenius norm per matrix
            norms = keras.ops.sqrt(
                keras.ops.sum(keras.ops.square(skew_symmetric), axis=[-2, -1], keepdims=True)
            )
            # Clip to max_norm
            scale = keras.ops.minimum(1.0, self.max_norm / (norms + 1e-8))
            skew_symmetric = skew_symmetric * scale

        # 4. Map to Lie Group SO(n) via matrix exponential: R = exp(A)
        # Note: tf.linalg.expm must be used as keras.ops doesn't have this operation
        if self.use_expm:
            # Accurate but computationally expensive
            embeddings = tf.linalg.expm(skew_symmetric)
        else:
            # Padé approximation: faster but less accurate
            # exp(A) ≈ (I + A/2)(I - A/2)^(-1) for small ||A||
            identity = keras.ops.eye(
                self.embed_dim,
                batch_shape=keras.ops.shape(skew_symmetric)[:-2]
            )
            half_A = 0.5 * skew_symmetric
            numerator = identity + half_A
            denominator = identity - half_A

            # Solve linear system: denominator @ X = numerator
            # X = denominator^(-1) @ numerator
            embeddings = tf.linalg.solve(denominator, numerator)

        return embeddings

    def compute_output_shape(self, input_shape: Tuple[Optional[int], ...]) -> Tuple[Optional[int], ...]:
        """
        Compute output shape given input shape.

        Parameters
        ----------
        input_shape : tuple
            Shape of input tensor (batch_size, seq_len).

        Returns
        -------
        tuple
            Output shape (batch_size, seq_len, embed_dim, embed_dim).
        """
        return input_shape + (self.embed_dim, self.embed_dim)

    def get_config(self) -> dict:
        """
        Return configuration dictionary for serialization.

        Returns
        -------
        dict
            Configuration parameters.
        """
        config = super().get_config()
        config.update({
            "vocab_size": self.vocab_size,
            "embed_dim": self.embed_dim,
            "use_expm": self.use_expm,
            "max_norm": self.max_norm,
            "kernel_initializer": keras.initializers.serialize(self.kernel_initializer),
            "kernel_regularizer": (
                keras.regularizers.serialize(self.kernel_regularizer)
                if self.kernel_regularizer is not None
                else None
            ),
        })
        return config

    @classmethod
    def from_config(cls, config: dict) -> "LieGroupEmbedding":
        """
        Create layer from configuration dictionary.

        Parameters
        ----------
        config : dict
            Configuration dictionary from get_config().

        Returns
        -------
        LieGroupEmbedding
            Reconstructed layer instance.
        """
        # Deserialize nested objects
        if "kernel_initializer" in config and config["kernel_initializer"] is not None:
            config["kernel_initializer"] = keras.initializers.deserialize(
                config["kernel_initializer"]
            )
        if "kernel_regularizer" in config and config["kernel_regularizer"] is not None:
            config["kernel_regularizer"] = keras.regularizers.deserialize(
                config["kernel_regularizer"]
            )
        return cls(**config)


@keras.saving.register_keras_serializable(package="dl_techniques")
class HolonomicPathIntegrator(keras.layers.Layer):
    """
    Computes the path-ordered integral of rotation matrices along a sequence.

    This layer implements the core "holonomic" computation: tracking how a semantic
    reference frame evolves as it's transformed by each token's rotation matrix.
    Unlike RNNs that use additive updates, this uses multiplicative composition,
    which is non-commutative (order-dependent) and geometrically meaningful.

    Mathematical Formulation
    ------------------------
    Given a sequence of rotation matrices R_1, R_2, ..., R_T:

    State evolution:
        S_0 = I (identity matrix)
        S_t = R_t @ S_{t-1}

    Final state: S_T = R_T @ R_{T-1} @ ... @ R_2 @ R_1

    This is the discrete path-ordered exponential:
        P exp(∫ R(s) ds) ≈ ∏_{t=1}^T R_t

    The non-commutativity (R_i @ R_j ≠ R_j @ R_i) means that reordering tokens
    produces fundamentally different geometric states.

    Parameters
    ----------
    return_sequences : bool, default=True
        If True, returns all intermediate states (shape: batch, seq_len, dim, dim).
        If False, returns only the final state (shape: batch, dim, dim).
    name : Optional[str], default=None
        Name for the layer.
    **kwargs : dict
        Additional keyword arguments for the base Layer class.

    Attributes
    ----------
    return_sequences : bool
        Whether to return all states or just the final one.

    Examples
    --------
    >>> # Create embeddings and integrator
    >>> embedding = LieGroupEmbedding(vocab_size=100, embed_dim=8)
    >>> integrator = HolonomicPathIntegrator(return_sequences=True)
    >>>
    >>> # Process sequence
    >>> tokens = keras.random.randint((4, 10), minval=0, maxval=100)
    >>> rotations = embedding(tokens)
    >>> trajectory = integrator(rotations)
    >>>
    >>> print(trajectory.shape)  # (4, 10, 8, 8) - all intermediate states
    >>>
    >>> # Get only final state
    >>> integrator_final = HolonomicPathIntegrator(return_sequences=False)
    >>> final_state = integrator_final(rotations)
    >>> print(final_state.shape)  # (4, 8, 8)

    Notes
    -----
    - Computation is sequential and cannot be easily parallelized
    - Memory usage scales linearly with sequence length if return_sequences=True
    - The operation is equivalent to matrix power tower: R_n @ (R_{n-1} @ (... @ R_1))
    - Order sensitivity provides natural defense against token reordering attacks
    """

    def __init__(
            self,
            return_sequences: bool = True,
            name: Optional[str] = None,
            **kwargs
    ):
        super().__init__(name=name, **kwargs)
        self.return_sequences = return_sequences

    def call(
            self,
            inputs: keras.KerasTensor,
            training: Optional[bool] = None
    ) -> keras.KerasTensor:
        """
        Compute path-ordered integral of rotation matrices.

        Parameters
        ----------
        inputs : keras.KerasTensor
            Sequence of rotation matrices with shape (batch, seq_len, dim, dim).
        training : Optional[bool], default=None
            Whether in training mode (unused but kept for API consistency).

        Returns
        -------
        keras.KerasTensor
            If return_sequences=True: All states (batch, seq_len, dim, dim)
            If return_sequences=False: Final state (batch, dim, dim)
        """
        # Extract dimensions
        batch_size = keras.ops.shape(inputs)[0]
        seq_len = keras.ops.shape(inputs)[1]
        dim = keras.ops.shape(inputs)[2]

        # Initial state: Identity matrix (neutral reference frame)
        # Shape: (batch_size, dim, dim)
        initial_state = keras.ops.eye(dim, batch_shape=[batch_size])

        # Define step function for sequential composition
        # state: current accumulated rotation (batch, dim, dim)
        # current_rotation: next rotation to apply (batch, dim, dim)
        def step(state: keras.KerasTensor, current_rotation: keras.KerasTensor) -> keras.KerasTensor:
            """Apply current rotation to accumulated state via matrix multiplication."""
            # Non-commutative composition: new_state = current @ old_state
            return keras.ops.matmul(current_rotation, state)

        # Execute path integral using tf.scan (Keras doesn't have scan operation)
        # We need to transpose to put sequence dimension first for tf.scan
        inputs_time_major = keras.ops.transpose(inputs, axes=[1, 0, 2, 3])

        # Accumulate rotations: produces (seq_len, batch, dim, dim)
        path_history = tf.scan(
            fn=step,
            elems=inputs_time_major,
            initializer=initial_state
        )

        # Transpose back to (batch, seq_len, dim, dim)
        path_history = keras.ops.transpose(path_history, axes=[1, 0, 2, 3])

        if self.return_sequences:
            return path_history
        else:
            # Return only the final state
            return path_history[:, -1, :, :]

    def compute_output_shape(self, input_shape: Tuple[Optional[int], ...]) -> Tuple[Optional[int], ...]:
        """
        Compute output shape given input shape.

        Parameters
        ----------
        input_shape : tuple
            Shape of input (batch, seq_len, dim, dim).

        Returns
        -------
        tuple
            Output shape depends on return_sequences flag.
        """
        if self.return_sequences:
            return input_shape  # Same as input
        else:
            # Remove sequence dimension
            return (input_shape[0], input_shape[2], input_shape[3])

    def get_config(self) -> dict:
        """
        Return configuration dictionary for serialization.

        Returns
        -------
        dict
            Configuration parameters.
        """
        config = super().get_config()
        config.update({
            "return_sequences": self.return_sequences,
        })
        return config


@keras.saving.register_keras_serializable(package="dl_techniques")
class ManifoldStressMonitor(keras.layers.Layer):
    """
    Measures geometric "stress" in semantic trajectories for anomaly detection.

    This layer implements the "RAGU" (Manifold Stress Monitor) concept: detecting
    when semantic transitions are geometrically inconsistent. It computes the
    relative rotation between consecutive states and measures deviation from
    smooth transitions.

    Mathematical Formulation
    ------------------------
    For consecutive rotation matrices R_t and R_{t+1}:

    1. Compute relative transition: ΔR = R_{t+1} @ R_t^T
    2. Measure deviation from identity: stress = ||ΔR - I||_F
    3. Average over sequence: mean_stress = (1/T) Σ stress_t

    High stress indicates:
    - Sharp semantic changes (potential jailbreak attempts)
    - Adversarial perturbations (token reordering)
    - Out-of-distribution inputs
    - Data poisoning

    Parameters
    ----------
    aggregation : str, default="mean"
        How to aggregate stress over sequence: "mean", "max", or "sum".
    epsilon : float, default=1e-8
        Small constant for numerical stability.
    name : Optional[str], default=None
        Name for the layer.
    **kwargs : dict
        Additional keyword arguments for the base Layer class.

    Attributes
    ----------
    aggregation : str
        Method for aggregating stress values.
    epsilon : float
        Numerical stability constant.

    Examples
    --------
    >>> # Create full pipeline
    >>> embedding = LieGroupEmbedding(vocab_size=100, embed_dim=8)
    >>> integrator = HolonomicPathIntegrator(return_sequences=True)
    >>> monitor = ManifoldStressMonitor(aggregation="mean")
    >>>
    >>> # Compute stress for sequence
    >>> tokens = keras.random.randint((4, 20), minval=0, maxval=100)
    >>> rotations = embedding(tokens)
    >>> trajectory = integrator(rotations)
    >>> stress = monitor(trajectory)
    >>>
    >>> print(stress.shape)  # (4, 1) - stress per batch sample
    >>>
    >>> # High stress indicates anomalies
    >>> threshold = 0.5
    >>> anomalies = keras.ops.greater(stress, threshold)

    Notes
    -----
    - Stress is scale-dependent: normalize or use relative thresholds
    - Can be used as auxiliary loss to encourage smooth semantic paths
    - Compatible with any sequence of rotation matrices (not just from integrator)
    - Lower stress = smoother, more "natural" semantic transitions
    """

    def __init__(
            self,
            aggregation: str = "mean",
            epsilon: float = 1e-8,
            name: Optional[str] = None,
            **kwargs
    ):
        super().__init__(name=name, **kwargs)

        valid_aggregations = ["mean", "max", "sum"]
        if aggregation not in valid_aggregations:
            raise ValueError(
                f"aggregation must be one of {valid_aggregations}, got {aggregation}"
            )

        self.aggregation = aggregation
        self.epsilon = epsilon

    def call(
            self,
            inputs: keras.KerasTensor,
            training: Optional[bool] = None
    ) -> keras.KerasTensor:
        """
        Compute manifold stress for rotation matrix trajectories.

        Parameters
        ----------
        inputs : keras.KerasTensor
            Sequence of rotation matrices with shape (batch, seq_len, dim, dim).
        training : Optional[bool], default=None
            Whether in training mode (unused but kept for API consistency).

        Returns
        -------
        keras.KerasTensor
            Aggregated stress values with shape (batch, 1).
            Higher values indicate greater geometric inconsistency.
        """
        # Extract consecutive pairs of rotation matrices
        # R_t: states at time t (batch, seq_len-1, dim, dim)
        # R_{t+1}: states at time t+1 (batch, seq_len-1, dim, dim)
        current_step = inputs[:, 1:, :, :]  # R_{t+1}
        prev_step = inputs[:, :-1, :, :]  # R_t

        # Compute relative rotation: ΔR = R_{t+1} @ R_t^T
        # R_t^T is the inverse since R is orthogonal
        relative_transition = keras.ops.matmul(
            current_step,
            keras.ops.transpose(prev_step, axes=[0, 1, 3, 2])
        )

        # Create identity matrix for comparison
        batch_size = keras.ops.shape(inputs)[0]
        seq_len_minus_1 = keras.ops.shape(current_step)[1]
        dim = keras.ops.shape(inputs)[-1]

        identity = keras.ops.eye(
            dim,
            batch_shape=[batch_size, seq_len_minus_1]
        )

        # Compute stress: deviation from identity
        # ||ΔR - I||_F = sqrt(Σ (ΔR_ij - I_ij)²)
        deviation = relative_transition - identity
        stress_per_transition = keras.ops.sqrt(
            keras.ops.sum(keras.ops.square(deviation), axis=[-2, -1]) + self.epsilon
        )
        # Shape: (batch, seq_len-1)

        # Aggregate over sequence
        if self.aggregation == "mean":
            stress = keras.ops.mean(stress_per_transition, axis=1, keepdims=True)
        elif self.aggregation == "max":
            stress = keras.ops.max(stress_per_transition, axis=1, keepdims=True)
        elif self.aggregation == "sum":
            stress = keras.ops.sum(stress_per_transition, axis=1, keepdims=True)

        return stress

    def compute_output_shape(self, input_shape: Tuple[Optional[int], ...]) -> Tuple[Optional[int], ...]:
        """
        Compute output shape given input shape.

        Parameters
        ----------
        input_shape : tuple
            Shape of input (batch, seq_len, dim, dim).

        Returns
        -------
        tuple
            Output shape (batch, 1).
        """
        return (input_shape[0], 1)

    def get_config(self) -> dict:
        """
        Return configuration dictionary for serialization.

        Returns
        -------
        dict
            Configuration parameters.
        """
        config = super().get_config()
        config.update({
            "aggregation": self.aggregation,
            "epsilon": self.epsilon,
        })
        return config


@keras.saving.register_keras_serializable(package="dl_techniques")
class HolonomicFieldProjection(keras.layers.Layer):
    """
    Projects final rotation matrix state to a feature vector for downstream tasks.

    After path integration, we have a final rotation matrix representing the
    accumulated semantic state. This layer converts that matrix to a vector
    that can be used for classification, regression, or other tasks.

    Projection Methods
    ------------------
    1. **Reference Vector**: Project onto a fixed reference direction
       output = R @ reference_vector

    2. **Diagonal**: Extract diagonal elements (trace-like)
       output = diag(R)

    3. **Flatten**: Flatten entire matrix (high-dimensional)
       output = flatten(R)

    4. **Mean Pool**: Average over rows or columns
       output = mean(R, axis=-1)

    Parameters
    ----------
    projection_type : str, default="reference"
        Method for projection: "reference", "diagonal", "flatten", or "mean_pool".
    reference_vector : Optional[Union[List[float], np.ndarray]], default=None
        Custom reference vector for "reference" projection.
        If None, uses [1, 0, 0, ..., 0].
    name : Optional[str], default=None
        Name for the layer.
    **kwargs : dict
        Additional keyword arguments for the base Layer class.

    Attributes
    ----------
    projection_type : str
        The projection method to use.
    reference_vector_init : Optional[Union[List[float], np.ndarray]]
        Initial reference vector configuration.
    reference_vector : Optional[keras.Variable]
        The reference vector used for projection (if applicable).

    Examples
    --------
    >>> # After path integration, project to feature vector
    >>> integrator = HolonomicPathIntegrator(return_sequences=False)
    >>> projection = HolonomicFieldProjection(projection_type="reference")
    >>>
    >>> # Full forward pass
    >>> rotations = embedding(tokens)
    >>> final_state = integrator(rotations)  # (batch, dim, dim)
    >>> features = projection(final_state)   # (batch, dim)
    >>>
    >>> # Use for classification
    >>> logits = keras.layers.Dense(num_classes)(features)

    Notes
    -----
    - "reference" maintains geometric meaning but loses some information
    - "flatten" preserves all information but creates high-dimensional output
    - "diagonal" extracts rotation eigenvalues (rotation angles)
    - "mean_pool" provides balanced trade-off
    """

    def __init__(
            self,
            projection_type: str = "reference",
            reference_vector: Optional[Union[List[float], np.ndarray]] = None,
            name: Optional[str] = None,
            **kwargs
    ):
        super().__init__(name=name, **kwargs)

        valid_types = ["reference", "diagonal", "flatten", "mean_pool"]
        if projection_type not in valid_types:
            raise ValueError(
                f"projection_type must be one of {valid_types}, got {projection_type}"
            )

        self.projection_type = projection_type
        self.reference_vector_init = reference_vector
        self.reference_vector = None

    def build(self, input_shape: Tuple[Optional[int], ...]) -> None:
        """
        Create reference vector variable if needed.

        Parameters
        ----------
        input_shape : tuple
            Shape of input tensor (batch, dim, dim).
        """
        dim = input_shape[-1]

        if self.projection_type == "reference":
            if self.reference_vector_init is not None:
                # Use provided reference vector
                ref_vec = np.array(self.reference_vector_init, dtype=np.float32)
                if ref_vec.shape != (dim,):
                    raise ValueError(
                        f"reference_vector must have shape ({dim},), "
                        f"got {ref_vec.shape}"
                    )
                initializer = keras.initializers.Constant(ref_vec)
            else:
                # Default: [1, 0, 0, ..., 0]
                ref_vec = np.zeros(dim, dtype=np.float32)
                ref_vec[0] = 1.0
                initializer = keras.initializers.Constant(ref_vec)

            # Create as trainable or non-trainable based on initialization
            self.reference_vector = self.add_weight(
                shape=(dim, 1),
                initializer=initializer,
                trainable=False,  # Usually fixed, but can be made trainable
                name="reference_vector"
            )

        super().build(input_shape)

    def call(
            self,
            inputs: keras.KerasTensor,
            training: Optional[bool] = None
    ) -> keras.KerasTensor:
        """
        Project rotation matrix to feature vector.

        Parameters
        ----------
        inputs : keras.KerasTensor
            Final rotation matrix with shape (batch, dim, dim).
        training : Optional[bool], default=None
            Whether in training mode (unused but kept for API consistency).

        Returns
        -------
        keras.KerasTensor
            Feature vector with shape depending on projection_type:
            - "reference": (batch, dim)
            - "diagonal": (batch, dim)
            - "flatten": (batch, dim * dim)
            - "mean_pool": (batch, dim)
        """
        if self.projection_type == "reference":
            # Project onto reference vector: features = R @ ref
            features = keras.ops.matmul(inputs, self.reference_vector)
            # Remove last dimension: (batch, dim, 1) -> (batch, dim)
            features = keras.ops.squeeze(features, axis=-1)

        elif self.projection_type == "diagonal":
            # Extract diagonal elements
            features = keras.ops.diagonal(inputs, axis1=-2, axis2=-1)

        elif self.projection_type == "flatten":
            # Flatten matrix to vector
            batch_size = keras.ops.shape(inputs)[0]
            features = keras.ops.reshape(inputs, (batch_size, -1))

        elif self.projection_type == "mean_pool":
            # Average over last dimension (mean of each row)
            features = keras.ops.mean(inputs, axis=-1)

        return features

    def compute_output_shape(self, input_shape: Tuple[Optional[int], ...]) -> Tuple[Optional[int], ...]:
        """
        Compute output shape given input shape.

        Parameters
        ----------
        input_shape : tuple
            Shape of input (batch, dim, dim).

        Returns
        -------
        tuple
            Output shape depends on projection_type.
        """
        dim = input_shape[-1]

        if self.projection_type in ["reference", "diagonal", "mean_pool"]:
            return (input_shape[0], dim)
        elif self.projection_type == "flatten":
            return (input_shape[0], dim * dim)

    def get_config(self) -> dict:
        """
        Return configuration dictionary for serialization.

        Returns
        -------
        dict
            Configuration parameters.
        """
        config = super().get_config()
        config.update({
            "projection_type": self.projection_type,
            "reference_vector": (
                self.reference_vector_init.tolist()
                if isinstance(self.reference_vector_init, np.ndarray)
                else self.reference_vector_init
            ),
        })
        return config


def build_holonomic_field_model(
        vocab_size: int,
        embed_dim: int,
        seq_len: int,
        num_classes: int,
        use_stress_loss: bool = True,
        stress_weight: float = 0.5,
        use_expm: bool = True,
        projection_type: str = "reference",
        name: Optional[str] = None
) -> Tuple[keras.Model, Optional[str]]:
    """
    Build a complete holonomic field embedding model for sequence classification.

    This function assembles the full pipeline:
    1. Field embeddings (token -> rotation matrix)
    2. Path integration (compose rotations)
    3. Stress monitoring (detect anomalies)
    4. Projection (matrix -> vector)
    5. Classification head

    Parameters
    ----------
    vocab_size : int
        Size of token vocabulary.
    embed_dim : int
        Dimension of rotation matrices.
    seq_len : int
        Maximum sequence length.
    num_classes : int
        Number of output classes.
    use_stress_loss : bool, default=True
        Whether to include stress as auxiliary loss.
    stress_weight : float, default=0.5
        Weight for stress loss term.
    use_expm : bool, default=True
        Whether to use accurate matrix exponential.
    projection_type : str, default="reference"
        Method for projecting rotation matrix to vector.
    name : Optional[str], default=None
        Name for the model.

    Returns
    -------
    keras.Model
        Complete model with inputs and outputs.
    Optional[str]
        Name of stress output if use_stress_loss=True, else None.

    Examples
    --------
    >>> # Build model for binary classification
    >>> model, stress_output_name = build_holonomic_field_model(
    ...     vocab_size=10000,
    ...     embed_dim=32,
    ...     seq_len=128,
    ...     num_classes=2,
    ...     use_stress_loss=True,
    ...     stress_weight=0.3
    ... )
    >>>
    >>> # Compile with multiple losses
    >>> model.compile(
    ...     optimizer=keras.optimizers.Adam(learning_rate=1e-3),
    ...     loss={
    ...         'classification': 'binary_crossentropy',
    ...         stress_output_name: 'mse'  # Drive stress to zero
    ...     },
    ...     loss_weights={
    ...         'classification': 1.0,
    ...         stress_output_name: 0.3
    ...     },
    ...     metrics={'classification': ['accuracy']}
    ... )
    >>>
    >>> # Train
    >>> model.fit(x_train, y_train, epochs=10, batch_size=32)

    Notes
    -----
    - Larger embed_dim increases expressiveness but also memory usage
    - Stress loss encourages smooth semantic paths during training
    - For production, consider use_expm=False for efficiency
    - The model is fully differentiable and trainable end-to-end
    """
    # Input layer
    inputs = keras.layers.Input(shape=(seq_len,), dtype="int32", name="token_ids")

    # 1. Field Embeddings: Token -> Rotation Matrix
    field_embeddings = LieGroupEmbedding(
        vocab_size=vocab_size,
        embed_dim=embed_dim,
        use_expm=use_expm,
        name="field_embedding"
    )(inputs)

    # 2. Path Integration: Compose Rotations Sequentially
    trajectory = HolonomicPathIntegrator(
        return_sequences=True,
        name="path_integrator"
    )(field_embeddings)

    # 3. Stress Monitor: Detect Geometric Anomalies
    stress_level = ManifoldStressMonitor(
        aggregation="mean",
        name="manifold_stress"
    )(trajectory)

    # 4. Projection: Final Matrix -> Feature Vector
    final_state = trajectory[:, -1, :, :]  # Extract final state
    features = HolonomicFieldProjection(
        projection_type=projection_type,
        name="field_projection"
    )(final_state)

    # 5. Classification Head
    classification_output = keras.layers.Dense(
        num_classes,
        activation="softmax" if num_classes > 2 else "sigmoid",
        name="classification"
    )(features)

    # Build model with appropriate outputs
    if use_stress_loss:
        outputs = {
            "classification": classification_output,
            "stress": stress_level
        }
        stress_output_name = "stress"
    else:
        outputs = classification_output
        stress_output_name = None

    model = keras.Model(inputs=inputs, outputs=outputs, name=name or "holonomic_field_model")

    return model, stress_output_name


# Example usage and utility functions

def verify_rotation_matrix(
        rotation: np.ndarray,
        tolerance: float = 1e-5
) -> Tuple[bool, dict]:
    """
    Verify that a matrix is a valid rotation (orthogonal with det=1).

    Parameters
    ----------
    rotation : np.ndarray
        Matrix to verify, shape (dim, dim).
    tolerance : float, default=1e-5
        Numerical tolerance for checks.

    Returns
    -------
    bool
        True if valid rotation matrix.
    dict
        Dictionary with verification details.
    """
    # Check orthogonality: R^T R = I
    identity_check = rotation.T @ rotation
    dim = rotation.shape[0]
    identity = np.eye(dim)
    orthogonality_error = np.linalg.norm(identity_check - identity)
    is_orthogonal = orthogonality_error < tolerance

    # Check determinant = 1
    det = np.linalg.det(rotation)
    det_error = abs(det - 1.0)
    is_det_one = det_error < tolerance

    is_valid = is_orthogonal and is_det_one

    details = {
        "is_valid": is_valid,
        "is_orthogonal": is_orthogonal,
        "is_det_one": is_det_one,
        "orthogonality_error": orthogonality_error,
        "det_error": det_error,
        "determinant": det
    }

    return is_valid, details


def visualize_stress_trajectory(
        stress_values: np.ndarray,
        threshold: Optional[float] = None
) -> None:
    """
    Visualize stress values over a batch to identify anomalies.

    Parameters
    ----------
    stress_values : np.ndarray
        Array of stress values, shape (batch_size,) or (batch_size, 1).
    threshold : Optional[float], default=None
        Threshold for marking anomalies. If None, uses mean + 2*std.
    """
    try:
        import matplotlib.pyplot as plt
    except ImportError:
        print("matplotlib required for visualization")
        return

    stress_values = stress_values.flatten()

    if threshold is None:
        threshold = np.mean(stress_values) + 2 * np.std(stress_values)

    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(stress_values, marker='o', linestyle='-', alpha=0.7)
    plt.axhline(y=threshold, color='r', linestyle='--', label=f'Threshold: {threshold:.4f}')
    plt.xlabel('Sample Index')
    plt.ylabel('Manifold Stress')
    plt.title('Stress Trajectory')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.subplot(1, 2, 2)
    plt.hist(stress_values, bins=30, alpha=0.7, edgecolor='black')
    plt.axvline(x=threshold, color='r', linestyle='--', label=f'Threshold: {threshold:.4f}')
    plt.xlabel('Manifold Stress')
    plt.ylabel('Frequency')
    plt.title('Stress Distribution')
    plt.legend()
    plt.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()