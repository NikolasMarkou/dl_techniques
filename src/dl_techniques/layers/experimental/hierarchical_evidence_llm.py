"""
Hierarchical Evidence Support System for LLM Token Generation

This module implements a novel approach to creating support evidence embeddings
for each token generated by language models. The system creates a hierarchy of
evidence at multiple levels (local, global, external) to provide interpretable
and reliable token generation with explicit reasoning support.

The system is inspired by:
- Retrieval-Augmented Generation (RAG)
- Multi-level attention mechanisms
- Evidence-based reasoning in NLP
- Hierarchical representation learning
"""

import keras
import numpy as np
from typing import Optional, List, Dict, Any


@keras.saving.register_keras_serializable()
class EvidenceEncoder(keras.layers.Layer):
    """
    Multi-level evidence encoder for token generation support.

    This layer encodes different types of evidence that can support token generation:
    - Local evidence: immediate context, recent tokens, syntactic patterns
    - Global evidence: document-level semantics, topic coherence, discourse structure
    - External evidence: retrieved knowledge, factual support, world knowledge

    **Architecture**:
    ```
    Input Context → [Local Encoder, Global Encoder, External Encoder]
                           ↓              ↓              ↓
                    Local Evidence  Global Evidence  External Evidence
                           ↓              ↓              ↓
                    Evidence Fusion → Combined Evidence Representation
    ```

    **Mathematical Operations**:
    - Local: e_local = LocalEncoder(context[-window:])
    - Global: e_global = GlobalEncoder(context)
    - External: e_external = ExternalEncoder(retrieved_docs)
    - Fusion: evidence = MLP([e_local; e_global; e_external])

    Args:
        embed_dim: Integer, embedding dimension for all evidence types.
            Must be positive. Defaults to 768.
        local_window: Integer, context window size for local evidence.
            Must be positive. Defaults to 32.
        num_heads: Integer, number of attention heads for evidence encoding.
            Must be positive. Defaults to 12.
        dropout_rate: Float, dropout rate for regularization.
            Must be between 0 and 1. Defaults to 0.1.
        use_external: Boolean, whether to include external evidence encoding.
            Defaults to True.
        **kwargs: Additional arguments for Layer base class.

    Input shape:
        Dictionary with keys:
        - 'context': 3D tensor (batch_size, seq_len, embed_dim) - input context
        - 'external': 3D tensor (batch_size, num_docs, embed_dim) - external docs (optional)

    Output shape:
        Dictionary with keys:
        - 'local': 3D tensor (batch_size, seq_len, embed_dim) - local evidence
        - 'global': 3D tensor (batch_size, seq_len, embed_dim) - global evidence
        - 'external': 3D tensor (batch_size, seq_len, embed_dim) - external evidence
        - 'combined': 3D tensor (batch_size, seq_len, embed_dim) - fused evidence
    """

    def __init__(
            self,
            embed_dim: int = 768,
            local_window: int = 32,
            num_heads: int = 12,
            dropout_rate: float = 0.1,
            use_external: bool = True,
            **kwargs: Any
    ) -> None:
        super().__init__(**kwargs)

        if embed_dim <= 0:
            raise ValueError(f"embed_dim must be positive, got {embed_dim}")
        if local_window <= 0:
            raise ValueError(f"local_window must be positive, got {local_window}")
        if num_heads <= 0:
            raise ValueError(f"num_heads must be positive, got {num_heads}")
        if not (0.0 <= dropout_rate <= 1.0):
            raise ValueError(f"dropout_rate must be between 0 and 1, got {dropout_rate}")

        self.embed_dim = embed_dim
        self.local_window = local_window
        self.num_heads = num_heads
        self.dropout_rate = dropout_rate
        self.use_external = use_external

        # Local evidence encoder (focuses on immediate context)
        self.local_attention = keras.layers.MultiHeadAttention(
            num_heads=num_heads,
            key_dim=embed_dim // num_heads,
            dropout=dropout_rate,
            name='local_attention'
        )
        self.local_norm = keras.layers.LayerNormalization(name='local_norm')

        # Global evidence encoder (captures document-level patterns)
        self.global_attention = keras.layers.MultiHeadAttention(
            num_heads=num_heads,
            key_dim=embed_dim // num_heads,
            dropout=dropout_rate,
            name='global_attention'
        )
        self.global_norm = keras.layers.LayerNormalization(name='global_norm')

        # External evidence encoder (integrates retrieved knowledge)
        if use_external:
            self.external_attention = keras.layers.MultiHeadAttention(
                num_heads=num_heads,
                key_dim=embed_dim // num_heads,
                dropout=dropout_rate,
                name='external_attention'
            )
            self.external_norm = keras.layers.LayerNormalization(name='external_norm')

        # Evidence fusion
        self.fusion_dense = keras.layers.Dense(
            embed_dim,
            activation='gelu',
            name='fusion_dense'
        )
        self.fusion_dropout = keras.layers.Dropout(dropout_rate, name='fusion_dropout')

    def call(
            self,
            inputs: Dict[str, keras.KerasTensor],
            training: Optional[bool] = None
    ) -> Dict[str, keras.KerasTensor]:
        """Encode evidence at multiple levels."""
        context = inputs['context']  # (batch, seq_len, embed_dim)

        # Local evidence: focus on recent context within window
        local_evidence = self._encode_local_evidence(context, training=training)

        # Global evidence: capture document-level patterns
        global_evidence = self._encode_global_evidence(context, training=training)

        # External evidence: integrate retrieved documents
        if self.use_external and 'external' in inputs:
            external_evidence = self._encode_external_evidence(
                context, inputs['external'], training=training
            )
        else:
            external_evidence = keras.ops.zeros_like(context)

        # Fuse evidence types
        if self.use_external:
            combined_input = keras.ops.concatenate([
                local_evidence, global_evidence, external_evidence
            ], axis=-1)
        else:
            combined_input = keras.ops.concatenate([
                local_evidence, global_evidence
            ], axis=-1)

        combined_evidence = self.fusion_dense(combined_input)
        combined_evidence = self.fusion_dropout(combined_evidence, training=training)

        return {
            'local': local_evidence,
            'global': global_evidence,
            'external': external_evidence,
            'combined': combined_evidence
        }

    def _encode_local_evidence(
            self,
            context: keras.KerasTensor,
            training: Optional[bool] = None
    ) -> keras.KerasTensor:
        """Encode local evidence from recent context using a sliding window attention."""
        seq_len = keras.ops.shape(context)[1]

        # Create a sliding window causal mask.
        # A token at position `i` can attend to tokens in `[max(0, i - window + 1), i]`.
        i = keras.ops.arange(seq_len)[:, None]
        j = keras.ops.arange(seq_len)
        mask = (j <= i) & (i - j < self.local_window)

        # Local attention with the restricted window mask
        local_output = self.local_attention(
            query=context,
            value=context,
            key=context,
            attention_mask=mask,
            training=training
        )

        # Residual connection and normalization
        local_evidence = self.local_norm(context + local_output)

        return local_evidence

    def _encode_global_evidence(
            self,
            context: keras.KerasTensor,
            training: Optional[bool] = None
    ) -> keras.KerasTensor:
        """Encode global evidence from full context."""
        # Global self-attention
        global_output = self.global_attention(
            query=context,
            value=context,
            key=context,
            training=training
        )

        # Residual connection and normalization
        global_evidence = self.global_norm(context + global_output)

        return global_evidence

    def _encode_external_evidence(
            self,
            context: keras.KerasTensor,
            external_docs: keras.KerasTensor,
            training: Optional[bool] = None
    ) -> keras.KerasTensor:
        """Encode external evidence from retrieved documents."""
        # This layer is guarded by `self.use_external` in the call method.
        # Cross-attention: context queries external documents
        external_output = self.external_attention(
            query=context,
            value=external_docs,
            key=external_docs,
            training=training
        )

        # Residual connection and normalization
        external_evidence = self.external_norm(context + external_output)

        return external_evidence

    def get_config(self) -> Dict[str, Any]:
        config = super().get_config()
        config.update({
            'embed_dim': self.embed_dim,
            'local_window': self.local_window,
            'num_heads': self.num_heads,
            'dropout_rate': self.dropout_rate,
            'use_external': self.use_external,
        })
        return config


@keras.saving.register_keras_serializable()
class HierarchicalEvidenceAggregator(keras.layers.Layer):
    """
    Hierarchical aggregator for evidence-based token generation support.

    This layer creates a hierarchy of evidence support by aggregating evidence
    at multiple temporal and semantic scales. It builds evidence pyramids that
    capture reasoning patterns at different levels of abstraction.

    **Architecture**:
    ```
    Evidence Input → [Token-level, Phrase-level, Sentence-level, Document-level]
                           ↓              ↓              ↓              ↓
                    Hierarchical Pooling and Attention Across Scales
                           ↓
                    Multi-scale Evidence Representation
                           ↓
                    Support Score Computation → Evidence Weights
    ```

    **Hierarchy Levels**:
    1. **Token-level**: Individual token evidence and local dependencies
    2. **Phrase-level**: Short phrase patterns and syntactic structures
    3. **Sentence-level**: Sentence semantics and discourse relations
    4. **Document-level**: Global coherence and topic consistency

    Args:
        embed_dim: Integer, embedding dimension. Must be positive. Defaults to 768.
        num_levels: Integer, number of hierarchy levels. Must be positive. Defaults to 4.
        pooling_sizes: List of integers, pooling window sizes for each level.
            Length must match num_levels. Defaults to [1, 4, 16, 64].
        num_heads: Integer, attention heads for cross-level attention.
            Must be positive. Defaults to 8.
        dropout_rate: Float, dropout rate. Must be between 0 and 1. Defaults to 0.1.
        **kwargs: Additional arguments for Layer base class.

    Input shape:
        3D tensor with shape: (batch_size, seq_len, embed_dim)

    Output shape:
        Dictionary with keys:
        - 'hierarchical_evidence': 4D tensor (batch_size, num_levels, seq_len, embed_dim)
        - 'support_scores': 3D tensor (batch_size, seq_len, num_levels)
        - 'aggregated_support': 3D tensor (batch_size, seq_len, embed_dim)
    """

    def __init__(
            self,
            embed_dim: int = 768,
            num_levels: int = 4,
            pooling_sizes: List[int] = [1, 4, 16, 64],
            num_heads: int = 8,
            dropout_rate: float = 0.1,
            **kwargs: Any
    ) -> None:
        super().__init__(**kwargs)

        if embed_dim <= 0:
            raise ValueError(f"embed_dim must be positive, got {embed_dim}")
        if num_levels <= 0:
            raise ValueError(f"num_levels must be positive, got {num_levels}")
        if len(pooling_sizes) != num_levels:
            raise ValueError(f"pooling_sizes length ({len(pooling_sizes)}) must match num_levels ({num_levels})")
        if any(size <= 0 for size in pooling_sizes):
            raise ValueError("All pooling sizes must be positive")
        if num_heads <= 0:
            raise ValueError(f"num_heads must be positive, got {num_heads}")
        if not (0.0 <= dropout_rate <= 1.0):
            raise ValueError(f"dropout_rate must be between 0 and 1, got {dropout_rate}")

        self.embed_dim = embed_dim
        self.num_levels = num_levels
        self.pooling_sizes = pooling_sizes
        self.num_heads = num_heads
        self.dropout_rate = dropout_rate

        # Multi-level pooling layers
        self.pooling_layers = []
        for i, pool_size in enumerate(pooling_sizes):
            if pool_size == 1:
                # Identity pooling for token level
                self.pooling_layers.append(keras.layers.Lambda(lambda x: x, name=f'pool_level_{i}'))
            else:
                # Average pooling for higher levels
                self.pooling_layers.append(
                    keras.layers.AveragePooling1D(
                        pool_size=pool_size,
                        strides=1,
                        padding='same',
                        name=f'pool_level_{i}'
                    )
                )

        # Cross-level attention for evidence integration
        self.cross_level_attention = keras.layers.MultiHeadAttention(
            num_heads=num_heads,
            key_dim=embed_dim // num_heads,
            dropout=dropout_rate,
            name='cross_level_attention'
        )
        self.cross_level_norm = keras.layers.LayerNormalization(name='cross_level_norm')

        # Final aggregation
        self.aggregation_dense = keras.layers.Dense(
            embed_dim,
            activation='tanh',
            name='aggregation_dense'
        )
        self.aggregation_dropout = keras.layers.Dropout(dropout_rate, name='aggregation_dropout')

    def call(
            self,
            evidence: keras.KerasTensor,
            training: Optional[bool] = None
    ) -> Dict[str, keras.KerasTensor]:
        """Aggregate evidence hierarchically."""
        original_shape = keras.ops.shape(evidence)
        batch_size, seq_len = original_shape[0], original_shape[1]

        # Extract multi-level evidence by pooling
        hierarchical_evidence = []
        for pooling_layer in self.pooling_layers:
            level_evidence = pooling_layer(evidence)
            hierarchical_evidence.append(level_evidence)

        # Stack hierarchical evidence: (batch, num_levels, seq_len, embed_dim)
        hierarchical_stack = keras.ops.stack(hierarchical_evidence, axis=1)

        # Prepare for token-wise attention across hierarchy levels.
        # Query: original evidence (batch*seq_len, 1, embed_dim)
        query_reshaped = keras.ops.reshape(evidence, (batch_size * seq_len, 1, self.embed_dim))

        # Key/Value: hierarchical representations (batch*seq_len, num_levels, embed_dim)
        value_transposed = keras.ops.transpose(hierarchical_stack, [0, 2, 1, 3])
        value_reshaped = keras.ops.reshape(value_transposed, (batch_size * seq_len, self.num_levels, self.embed_dim))

        # Each token's representation attends to its own hierarchy of evidence
        attention_output, attention_scores = self.cross_level_attention(
            query=query_reshaped,
            value=value_reshaped,
            key=value_reshaped,
            training=training,
            return_attention_scores=True
        )

        # Reshape attention output back to (batch, seq_len, embed_dim)
        aggregated_support = keras.ops.reshape(attention_output, (batch_size, seq_len, self.embed_dim))

        # Residual connection and normalization
        aggregated_support = self.cross_level_norm(evidence + aggregated_support)

        # Compute support scores from attention weights
        # attention_scores shape: (batch*seq_len, num_heads, 1, num_levels)
        support_scores = keras.ops.mean(attention_scores, axis=1)  # Average over heads
        support_scores = keras.ops.squeeze(support_scores, axis=1)  # Remove query_len dim
        support_scores = keras.ops.reshape(support_scores, (batch_size, seq_len, self.num_levels))

        # Final transformation
        aggregated_support = self.aggregation_dense(aggregated_support)
        aggregated_support = self.aggregation_dropout(aggregated_support, training=training)

        return {
            'hierarchical_evidence': hierarchical_stack,
            'support_scores': support_scores,
            'aggregated_support': aggregated_support
        }

    def get_config(self) -> Dict[str, Any]:
        config = super().get_config()
        config.update({
            'embed_dim': self.embed_dim,
            'num_levels': self.num_levels,
            'pooling_sizes': self.pooling_sizes,
            'num_heads': self.num_heads,
            'dropout_rate': self.dropout_rate,
        })
        return config


@keras.saving.register_keras_serializable()
class SupportEmbeddingLayer(keras.layers.Layer):
    """
    Support embedding layer for evidence-based token generation.

    This layer creates rich support embeddings that encode multiple types of
    evidence and reasoning patterns for each potential next token. It computes
    confidence scores, uncertainty estimates, and reasoning chain representations
    to provide comprehensive support for generation decisions.

    **Architecture**:
    ```
    [Combined Evidence, Hierarchical Support]
                    ↓
    Support Embedding Computation
                    ↓
    [Confidence, Uncertainty, Reasoning] → Support Representation
                    ↓
    Token Support Scores → Generation Support
    ```

    **Support Components**:
    - **Confidence**: How certain the model is about each token choice
    - **Uncertainty**: Epistemic and aleatoric uncertainty estimates
    - **Reasoning**: Chain-of-thought style reasoning embeddings
    - **Evidence**: Source and quality of supporting evidence

    Args:
        vocab_size: Integer, vocabulary size for token support. Must be positive.
        embed_dim: Integer, embedding dimension. Must be positive. Defaults to 768.
        support_dim: Integer, support embedding dimension. Must be positive. Defaults to 256.
        num_reasoning_steps: Integer, reasoning chain length. Must be positive. Defaults to 4.
        dropout_rate: Float, dropout rate. Must be between 0 and 1. Defaults to 0.1.
        **kwargs: Additional arguments for Layer base class.

    Input shape:
        Dictionary with keys:
        - 'evidence': 3D tensor (batch_size, seq_len, embed_dim) - evidence embeddings
        - 'hierarchical_support': 3D tensor (batch_size, seq_len, embed_dim) - hierarchical support

    Output shape:
        Dictionary with keys:
        - 'token_support': 3D tensor (batch_size, seq_len, vocab_size) - per-token support scores
        - 'confidence': 2D tensor (batch_size, seq_len) - confidence scores
        - 'uncertainty': 2D tensor (batch_size, seq_len) - uncertainty estimates
        - 'reasoning_chain': 4D tensor (batch_size, seq_len, num_reasoning_steps, support_dim)
        - 'support_embeddings': 3D tensor (batch_size, seq_len, support_dim) - final support
    """

    def __init__(
            self,
            vocab_size: int,
            embed_dim: int = 768,
            support_dim: int = 256,
            num_reasoning_steps: int = 4,
            dropout_rate: float = 0.1,
            **kwargs: Any
    ) -> None:
        super().__init__(**kwargs)

        if vocab_size <= 0:
            raise ValueError(f"vocab_size must be positive, got {vocab_size}")
        if embed_dim <= 0:
            raise ValueError(f"embed_dim must be positive, got {embed_dim}")
        if support_dim <= 0:
            raise ValueError(f"support_dim must be positive, got {support_dim}")
        if num_reasoning_steps <= 0:
            raise ValueError(f"num_reasoning_steps must be positive, got {num_reasoning_steps}")
        if not (0.0 <= dropout_rate <= 1.0):
            raise ValueError(f"dropout_rate must be between 0 and 1, got {dropout_rate}")

        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.support_dim = support_dim
        self.num_reasoning_steps = num_reasoning_steps
        self.dropout_rate = dropout_rate

        # Evidence fusion
        self.evidence_fusion = keras.layers.Dense(
            support_dim,
            activation='gelu',
            name='evidence_fusion'
        )

        # Confidence estimation
        self.confidence_estimator = keras.layers.Dense(
            1,
            activation='sigmoid',
            name='confidence_estimator'
        )

        # Uncertainty estimation (epistemic + aleatoric)
        self.uncertainty_estimator = keras.layers.Dense(
            2,  # [epistemic, aleatoric]
            activation='softplus',
            name='uncertainty_estimator'
        )

        # Reasoning chain generation
        self.reasoning_layers = []
        for i in range(num_reasoning_steps):
            self.reasoning_layers.append(
                keras.layers.Dense(
                    support_dim,
                    activation='tanh',
                    name=f'reasoning_step_{i}'
                )
            )

        # Token support scoring
        self.token_support_projector = keras.layers.Dense(
            vocab_size,
            name='token_support_projector'
        )

        # Final support embedding
        self.support_embedder = keras.layers.Dense(
            support_dim,
            activation='relu',
            name='support_embedder'
        )
        self.support_dropout = keras.layers.Dropout(dropout_rate, name='support_dropout')

    def call(
            self,
            inputs: Dict[str, keras.KerasTensor],
            training: Optional[bool] = None
    ) -> Dict[str, keras.KerasTensor]:
        """Generate support embeddings for token generation."""
        evidence = inputs['evidence']
        hierarchical_support = inputs['hierarchical_support']

        # Fuse evidence sources
        combined_input = keras.ops.concatenate([evidence, hierarchical_support], axis=-1)
        fused_evidence = self.evidence_fusion(combined_input)

        # Compute confidence scores
        confidence = keras.ops.squeeze(self.confidence_estimator(fused_evidence), axis=-1)

        # Estimate uncertainty
        uncertainty_scores = self.uncertainty_estimator(fused_evidence)
        uncertainty = keras.ops.sum(uncertainty_scores, axis=-1)  # Total uncertainty

        # Generate reasoning chain
        reasoning_chain = []
        reasoning_input = fused_evidence

        for reasoning_layer in self.reasoning_layers:
            reasoning_step = reasoning_layer(reasoning_input)
            reasoning_chain.append(reasoning_step)
            # Residual connection for reasoning flow
            reasoning_input = reasoning_input + reasoning_step

        reasoning_stack = keras.ops.stack(reasoning_chain, axis=2)  # (batch, seq, steps, dim)

        # Compute token support scores
        token_support = self.token_support_projector(reasoning_input)

        # Generate final support embeddings
        support_embeddings = self.support_embedder(reasoning_input)
        support_embeddings = self.support_dropout(support_embeddings, training=training)

        return {
            'token_support': token_support,
            'confidence': confidence,
            'uncertainty': uncertainty,
            'reasoning_chain': reasoning_stack,
            'support_embeddings': support_embeddings
        }

    def get_config(self) -> Dict[str, Any]:
        config = super().get_config()
        config.update({
            'vocab_size': self.vocab_size,
            'embed_dim': self.embed_dim,
            'support_dim': self.support_dim,
            'num_reasoning_steps': self.num_reasoning_steps,
            'dropout_rate': self.dropout_rate,
        })
        return config


@keras.saving.register_keras_serializable()
class EvidenceSupportedTokenGeneration(keras.layers.Layer):
    """
    Complete evidence-supported token generation system.

    This layer implements a full pipeline for generating tokens with hierarchical
    evidence support. It combines multi-level evidence encoding, hierarchical
    aggregation, and support embedding generation to produce tokens with
    interpretable reasoning chains and confidence estimates.

    **Full Architecture**:
    ```
    Input Context + External Docs
            ↓
    Multi-level Evidence Encoding
            ↓
    Hierarchical Evidence Aggregation
            ↓
    Support Embedding Generation
            ↓
    Evidence-Weighted Token Probabilities
            ↓
    [Tokens + Evidence + Confidence + Reasoning]
    ```

    **Key Features**:
    - Interpretable token generation with evidence trails
    - Multi-level confidence and uncertainty estimation
    - Reasoning chain visualization for each token
    - Support for external knowledge integration
    - Hierarchical evidence aggregation across temporal scales

    Args:
        vocab_size: Integer, vocabulary size. Must be positive.
        embed_dim: Integer, embedding dimension. Must be positive. Defaults to 768.
        max_seq_len: Integer, maximum sequence length. Must be positive. Defaults to 512.
        num_heads: Integer, attention heads. Must be positive. Defaults to 12.
        num_evidence_levels: Integer, evidence hierarchy levels. Defaults to 4.
        support_dim: Integer, support embedding dimension. Defaults to 256.
        dropout_rate: Float, dropout rate. Must be between 0 and 1. Defaults to 0.1.
        use_external_evidence: Boolean, enable external evidence. Defaults to True.
        **kwargs: Additional arguments for Layer base class.

    Input shape:
        Dictionary with keys:
        - 'input_ids': 2D tensor (batch_size, seq_len) - input token ids
        - 'external_docs': 3D tensor (batch_size, num_docs, embed_dim) - external docs (optional)

    Output shape:
        Dictionary with keys:
        - 'logits': 3D tensor (batch_size, seq_len, vocab_size) - token probabilities
        - 'evidence_support': 3D tensor (batch_size, seq_len, embed_dim) - evidence embeddings
        - 'confidence_scores': 2D tensor (batch_size, seq_len) - confidence per position
        - 'uncertainty_scores': 2D tensor (batch_size, seq_len) - uncertainty per position
        - 'reasoning_chains': 4D tensor (batch_size, seq_len, steps, support_dim) - reasoning
        - 'hierarchical_evidence': 4D tensor (batch_size, levels, seq_len, embed_dim) - evidence pyramid
    """

    def __init__(
            self,
            vocab_size: int,
            embed_dim: int = 768,
            max_seq_len: int = 512,
            num_heads: int = 12,
            num_evidence_levels: int = 4,
            support_dim: int = 256,
            dropout_rate: float = 0.1,
            use_external_evidence: bool = True,
            **kwargs: Any
    ) -> None:
        super().__init__(**kwargs)

        if vocab_size <= 0:
            raise ValueError(f"vocab_size must be positive, got {vocab_size}")
        if embed_dim <= 0:
            raise ValueError(f"embed_dim must be positive, got {embed_dim}")
        if max_seq_len <= 0:
            raise ValueError(f"max_seq_len must be positive, got {max_seq_len}")
        if num_heads <= 0:
            raise ValueError(f"num_heads must be positive, got {num_heads}")
        if num_evidence_levels <= 0:
            raise ValueError(f"num_evidence_levels must be positive, got {num_evidence_levels}")
        if support_dim <= 0:
            raise ValueError(f"support_dim must be positive, got {support_dim}")
        if not (0.0 <= dropout_rate <= 1.0):
            raise ValueError(f"dropout_rate must be between 0 and 1, got {dropout_rate}")

        self.vocab_size = vocab_size
        self.embed_dim = embed_dim
        self.max_seq_len = max_seq_len
        self.num_heads = num_heads
        self.num_evidence_levels = num_evidence_levels
        self.support_dim = support_dim
        self.dropout_rate = dropout_rate
        self.use_external_evidence = use_external_evidence

        # Token embeddings
        self.token_embeddings = keras.layers.Embedding(
            vocab_size,
            embed_dim,
            name='token_embeddings'
        )
        self.position_embeddings = keras.layers.Embedding(
            max_seq_len,
            embed_dim,
            name='position_embeddings'
        )

        # Evidence encoding system
        self.evidence_encoder = EvidenceEncoder(
            embed_dim=embed_dim,
            num_heads=num_heads,
            dropout_rate=dropout_rate,
            use_external=use_external_evidence,
            name='evidence_encoder'
        )

        # Hierarchical evidence aggregation
        self.hierarchical_aggregator = HierarchicalEvidenceAggregator(
            embed_dim=embed_dim,
            num_levels=num_evidence_levels,
            num_heads=num_heads,
            dropout_rate=dropout_rate,
            name='hierarchical_aggregator'
        )

        # Support embedding generation
        self.support_embedding_layer = SupportEmbeddingLayer(
            vocab_size=vocab_size,
            embed_dim=embed_dim,
            support_dim=support_dim,
            dropout_rate=dropout_rate,
            name='support_embedding_layer'
        )

        # Final output projection
        self.output_projection = keras.layers.Dense(
            vocab_size,
            name='output_projection'
        )

    def call(
            self,
            inputs: Dict[str, keras.KerasTensor],
            training: Optional[bool] = None
    ) -> Dict[str, keras.KerasTensor]:
        """Generate tokens with hierarchical evidence support."""
        input_ids = inputs['input_ids']
        seq_len = keras.ops.shape(input_ids)[1]

        # Token and position embeddings
        token_embeds = self.token_embeddings(input_ids)
        positions = keras.ops.arange(seq_len)
        position_embeds = self.position_embeddings(positions)
        context = token_embeds + position_embeds

        # Prepare evidence encoding inputs
        evidence_inputs = {'context': context}
        if self.use_external_evidence and 'external_docs' in inputs:
            evidence_inputs['external'] = inputs['external_docs']

        # Encode multi-level evidence
        evidence_outputs = self.evidence_encoder(evidence_inputs, training=training)
        combined_evidence = evidence_outputs['combined']

        # Hierarchical evidence aggregation
        hierarchy_outputs = self.hierarchical_aggregator(
            combined_evidence, training=training
        )
        hierarchical_support = hierarchy_outputs['aggregated_support']

        # Generate support embeddings
        support_inputs = {
            'evidence': combined_evidence,
            'hierarchical_support': hierarchical_support
        }
        support_outputs = self.support_embedding_layer(support_inputs, training=training)

        # Combine evidence with support for final token prediction
        evidence_weighted_context = context + combined_evidence + hierarchical_support
        logits = self.output_projection(evidence_weighted_context)

        # Apply support scores to logits (evidence-weighted probabilities)
        token_support = support_outputs['token_support']
        evidence_weighted_logits = logits + 0.1 * token_support  # Weighted combination

        return {
            'logits': evidence_weighted_logits,
            'evidence_support': combined_evidence,
            'confidence_scores': support_outputs['confidence'],
            'uncertainty_scores': support_outputs['uncertainty'],
            'reasoning_chains': support_outputs['reasoning_chain'],
            'hierarchical_evidence': hierarchy_outputs['hierarchical_evidence'],
            'raw_logits': logits,
            'token_support_scores': token_support
        }

    def get_config(self) -> Dict[str, Any]:
        config = super().get_config()
        config.update({
            'vocab_size': self.vocab_size,
            'embed_dim': self.embed_dim,
            'max_seq_len': self.max_seq_len,
            'num_heads': self.num_heads,
            'num_evidence_levels': self.num_evidence_levels,
            'support_dim': self.support_dim,
            'dropout_rate': self.dropout_rate,
            'use_external_evidence': self.use_external_evidence,
        })
        return config


# Utility functions for evidence analysis and visualization

def analyze_evidence_support(
        outputs: Dict[str, keras.KerasTensor],
        input_tokens: List[str],
        vocab: Dict[int, str]
) -> Dict[str, Any]:
    """
    Analyze evidence support for generated tokens for the first item in a batch.

    Args:
        outputs: Output dictionary from EvidenceSupportedTokenGeneration
        input_tokens: List of input token strings
        vocab: Vocabulary mapping from token IDs to strings

    Returns:
        Dictionary with evidence analysis results
    """
    confidence = keras.ops.convert_to_numpy(outputs['confidence_scores'])
    uncertainty = keras.ops.convert_to_numpy(outputs['uncertainty_scores'])
    logits = keras.ops.convert_to_numpy(outputs['logits'])
    support_scores = keras.ops.convert_to_numpy(outputs['token_support_scores'])

    batch_size, seq_len = confidence.shape

    analysis = {
        'per_token_analysis': [],
        'confidence_stats': {
            'mean': float(np.mean(confidence)),
            'std': float(np.std(confidence)),
            'min': float(np.min(confidence)),
            'max': float(np.max(confidence))
        },
        'uncertainty_stats': {
            'mean': float(np.mean(uncertainty)),
            'std': float(np.std(uncertainty)),
            'min': float(np.min(uncertainty)),
            'max': float(np.max(uncertainty))
        }
    }

    # Analyze each position for the first batch item
    for i in range(min(seq_len, len(input_tokens))):
        top_k_indices = np.argsort(logits[0, i])[-5:][::-1]  # Top 5 predictions
        top_k_support = np.argsort(support_scores[0, i])[-5:][::-1]  # Top 5 by support

        token_analysis = {
            'position': i,
            'input_token': input_tokens[i] if i < len(input_tokens) else '<UNK>',
            'confidence': float(confidence[0, i]),
            'uncertainty': float(uncertainty[0, i]),
            'top_predictions': [
                {
                    'token': vocab.get(idx, f'<UNK_{idx}>'),
                    'logit_score': float(logits[0, i, idx]),
                    'support_score': float(support_scores[0, i, idx])
                }
                for idx in top_k_indices
            ],
            'top_supported': [
                {
                    'token': vocab.get(idx, f'<UNK_{idx}>'),
                    'logit_score': float(logits[0, i, idx]),
                    'support_score': float(support_scores[0, i, idx])
                }
                for idx in top_k_support
            ]
        }
        analysis['per_token_analysis'].append(token_analysis)

    return analysis


def create_evidence_supported_language_model(
        vocab_size: int,
        embed_dim: int = 768,
        max_seq_len: int = 512,
        **kwargs: Any
) -> keras.Model:
    """
    Create a complete evidence-supported language model.

    Args:
        vocab_size: Vocabulary size
        embed_dim: Embedding dimension
        max_seq_len: Maximum sequence length
        **kwargs: Additional arguments for EvidenceSupportedTokenGeneration

    Returns:
        Keras Model with evidence-supported token generation
    """
    # Input layers
    input_ids = keras.layers.Input(
        shape=(None,),  # Allow for variable sequence length
        dtype='int32',
        name='input_ids'
    )

    # Instantiate the core layer, which holds the config
    evidence_generator = EvidenceSupportedTokenGeneration(
        vocab_size=vocab_size,
        embed_dim=embed_dim,
        max_seq_len=max_seq_len,
        **kwargs
    )

    # Conditionally define the model's inputs based on the layer's config
    model_inputs = [input_ids]
    layer_inputs = {'input_ids': input_ids}

    if evidence_generator.use_external_evidence:
        external_docs = keras.layers.Input(
            shape=(None, embed_dim),
            name='external_docs'
        )
        model_inputs.append(external_docs)
        layer_inputs['external_docs'] = external_docs

    outputs = evidence_generator(layer_inputs)

    # Create model
    model = keras.Model(
        inputs=model_inputs,
        outputs=outputs,
        name='EvidenceSupportedLanguageModel'
    )

    return model