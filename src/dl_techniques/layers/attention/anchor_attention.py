"""
Implements a hierarchical, memory-efficient anchor-based attention.

This layer provides a scalable alternative to standard self-attention by
creating an information bottleneck through a small, fixed set of "anchor"
tokens. It is designed to reduce the quadratic complexity of attention for
long sequences while preserving the model's ability to access global context.

Architecture:
    The architecture transforms the standard all-to-all attention graph
    into a two-tier, hub-and-spoke model. The input sequence is conceptually
    divided into two distinct groups:

    1.  **Anchor Tokens**: A small subset of tokens (e.g., the first `K`
        tokens of the sequence) that perform full, quadratic self-attention
        among themselves. These tokens are tasked with aggregating
        information and forming a compressed, global summary of the entire
        sequence context.

    2.  **Query Tokens**: The remaining tokens in the sequence. To save
        computation, these tokens do not attend to each other. Instead,
        they perform cross-attention *only* to the set of anchor tokens.
        Each query token can read information from the global summary
        created by the anchors but cannot interact directly with other
        query tokens.

    This design reduces the computational complexity from O(N^2) for a
    sequence of length N to a much more manageable O(K^2 + (N-K)*K), which
    is approximately linear in N when K is small and fixed.

Foundational Mathematics:
    The mechanism modifies the standard scaled dot-product attention
    `softmax( (Q @ K.T) / sqrt(d_k) ) @ V`. Let the input sequence `X` be
    partitioned into anchor tokens `X_a` (of length `K`) and query tokens
    `X_q` (of length `N-K`).

    -   The **anchor tokens** generate their own queries, keys, and values
        (`Q_a`, `K_a`, `V_a`) and compute standard self-attention on
        themselves. The attention matrix for this step has a shape of
        (`K`, `K`).
            `Output_a = softmax((Q_a @ K_a.T) / sqrt(d_k)) @ V_a`

    -   The **query tokens** generate only their own queries (`Q_q`). They
        use the keys and values (`K_a`, `V_a`) generated by the anchors to
        perform cross-attention. The attention matrix for this step has a
        shape of (`N-K`, `K`).
            `Output_q = softmax((Q_q @ K_a.T) / sqrt(d_k)) @ V_a`

    The final output is the concatenation of `Output_a` and `Output_q`. This
    avoids computing the large `(N-K) x (N-K)` attention matrix between query
    tokens, which is the source of the quadratic complexity.

References:
    This pattern is a form of sparse attention, related to models that use
    global tokens or inducing points to efficiently handle long sequences.

    -   The use of special tokens with full receptive fields is a key idea in:
        Beltagy, I., Peters, M. E., & Cohan, A. (2020). "Longformer: The
        Long-Document Transformer".

    -   The concept of using a smaller set of elements as an information
        bottleneck is explored in:
        Lee, J., Lee, Y., Kim, J., Kosiorek, A., Choi, S., & Teh, Y. W.
        (2019). "Set Transformer: A Framework for Attention-based
        Permutation-Invariant Neural Networks".
"""

import keras
from typing import Optional, Any, Dict, Tuple, Union

# ---------------------------------------------------------------------

@keras.saving.register_keras_serializable()
class AnchorAttention(keras.layers.Layer):
    """
    Hierarchical attention mechanism where anchor tokens have full self-attention.

    This layer implements a memory-efficient attention mechanism that reduces computational
    complexity for large sequences while maintaining representational power through a
    hierarchical structure:
    - Anchor tokens: Have full self-attention among themselves
    - Query tokens: Only cross-attend to anchor tokens (no self-attention)

    **Intent**: Provide scalable attention for long sequences by creating a two-tier
    attention hierarchy that maintains full expressiveness through anchor tokens while
    reducing computational complexity for query tokens.

    **Architecture**:
    ```
    Standard Mode (num_anchor_tokens=None):
    Input → QKV_projection → MultiHead_SelfAttention → Output_projection → Output

    Hierarchical Mode (num_anchor_tokens=K):
    Anchors[1:K] → QKV_projection → Self_Attention ────┐
                                                        ├→ Combined_Output
    Queries[K+1:N] → Q_projection → Cross_Attention ───┘
    ```

    **Mathematical Formulation**:
    - Standard: Attention(Q, K, V) = softmax(QK^T / √d_k)V
    - Hierarchical: Query tokens attend only to anchor K, V matrices

    **Computational Complexity**:
    - Standard: O(N²d) where N is sequence length
    - Hierarchical: O(K² + (N-K)K)d where K is number of anchors

    Args:
        dim: Integer, input/output dimension of the attention layer. Must be positive
            and divisible by num_heads.
        num_heads: Integer, number of attention heads. Must be positive and divide dim
            evenly. Defaults to 8.
        dropout_rate: Float, dropout rate applied to attention weights. Must be in
            range [0, 1]. Defaults to 0.0.
        use_bias: Boolean, whether to use bias in linear projections. Defaults to True.
        kernel_initializer: String or Initializer instance for kernel weights.
            Defaults to 'glorot_uniform'.
        bias_initializer: String or Initializer instance for bias vectors.
            Defaults to 'zeros'.
        kernel_regularizer: Optional regularizer for kernel weights. Defaults to None.
        bias_regularizer: Optional regularizer for bias weights. Defaults to None.
        **kwargs: Additional keyword arguments for the Layer base class.

    Input shape:
        3D tensor with shape: `(batch_size, sequence_length, dim)`

    Output shape:
        3D tensor with shape: `(batch_size, sequence_length, dim)`

    Call arguments:
        x: Input tensor of shape (batch_size, sequence_length, dim).
        num_anchor_tokens: Optional integer specifying how many tokens from the
            beginning of the sequence are anchor tokens. If None, all tokens
            are treated as anchors (standard self-attention). If provided,
            the first num_anchor_tokens will be anchors with full self-attention,
            and remaining tokens will be queries that only attend to anchors.
        training: Boolean indicating whether the layer should behave in training
            mode (applying dropout) or inference mode.

    Returns:
        Output tensor with same shape as input.

    Raises:
        ValueError: If dim is not divisible by num_heads.
        ValueError: If dim or num_heads is not positive.
        ValueError: If dropout_rate is not in range [0, 1].

    Example:
        ```python
        # Standard self-attention (all tokens are anchors)
        x = keras.random.normal((2, 100, 256))
        attn = AnchorAttention(dim=256, num_heads=8)
        output = attn(x)
        print(output.shape)  # (2, 100, 256)

        # Hierarchical attention (first 20 tokens are anchors)
        output = attn(x, num_anchor_tokens=20)
        print(output.shape)  # (2, 100, 256)
        # First 20 tokens attend to each other, last 80 only attend to first 20

        # With regularization and custom configuration
        attn = AnchorAttention(
            dim=512,
            num_heads=16,
            dropout_rate=0.1,
            kernel_regularizer=keras.regularizers.L2(1e-4),
            kernel_initializer='he_normal'
        )
        ```
    """

    def __init__(
            self,
            dim: int,
            num_heads: int = 8,
            dropout_rate: float = 0.0,
            use_bias: bool = True,
            kernel_initializer: Union[str, keras.initializers.Initializer] = "glorot_uniform",
            bias_initializer: Union[str, keras.initializers.Initializer] = "zeros",
            kernel_regularizer: Optional[keras.regularizers.Regularizer] = None,
            bias_regularizer: Optional[keras.regularizers.Regularizer] = None,
            **kwargs: Any
    ) -> None:
        super().__init__(**kwargs)

        # Validate inputs
        if dim <= 0:
            raise ValueError(f"dim must be positive, got {dim}")
        if num_heads <= 0:
            raise ValueError(f"num_heads must be positive, got {num_heads}")
        if dim % num_heads != 0:
            raise ValueError(f"dim ({dim}) must be divisible by num_heads ({num_heads})")
        if not (0.0 <= dropout_rate <= 1.0):
            raise ValueError(f"dropout_rate must be between 0 and 1, got {dropout_rate}")

        # Store ALL configuration parameters for complete serialization
        self.dim = dim
        self.num_heads = num_heads
        self.head_dim = dim // num_heads
        self.dropout_rate = dropout_rate
        self.use_bias = use_bias
        self.kernel_initializer = keras.initializers.get(kernel_initializer)
        self.bias_initializer = keras.initializers.get(bias_initializer)
        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)
        self.bias_regularizer = keras.regularizers.get(bias_regularizer)

        # Scale factor for attention scores
        self.scale = 1.0 / keras.ops.sqrt(float(self.head_dim))

        # Create ALL sub-layers in __init__ (following modern Keras 3 pattern)
        self.qkv_dense = keras.layers.Dense(
            self.dim * 3,
            use_bias=self.use_bias,
            kernel_initializer=self.kernel_initializer,
            bias_initializer=self.bias_initializer,
            kernel_regularizer=self.kernel_regularizer,
            bias_regularizer=self.bias_regularizer,
            name="qkv"
        )

        # Q projection for query tokens (used only in hierarchical mode)
        self.q_dense = keras.layers.Dense(
            self.dim,
            use_bias=self.use_bias,
            kernel_initializer=self.kernel_initializer,
            bias_initializer=self.bias_initializer,
            kernel_regularizer=self.kernel_regularizer,
            bias_regularizer=self.bias_regularizer,
            name="q_query"
        )

        # Output projection
        self.proj_dense = keras.layers.Dense(
            self.dim,
            use_bias=self.use_bias,
            kernel_initializer=self.kernel_initializer,
            bias_initializer=self.bias_initializer,
            kernel_regularizer=self.kernel_regularizer,
            bias_regularizer=self.bias_regularizer,
            name="proj"
        )

        # Dropout layer (conditional creation)
        if self.dropout_rate > 0.0:
            self.dropout_layer = keras.layers.Dropout(
                self.dropout_rate,
                name="dropout"
            )
        else:
            self.dropout_layer = None

    def build(self, input_shape: Tuple[Optional[int], ...]) -> None:
        """
        Build the layer and all its sub-layers.

        Creates weight variables for both the layer and its sub-layers, ensuring
        proper serialization compatibility by explicitly building each sub-layer
        in computational order.

        Args:
            input_shape: Shape tuple of the input tensor.

        Raises:
            ValueError: If input is not 3D or last dimension doesn't match dim.
        """
        if len(input_shape) != 3:
            raise ValueError(f"Input must be 3D, got shape {input_shape}")

        if input_shape[-1] is None:
            raise ValueError("Last dimension of input must be defined")

        if input_shape[-1] != self.dim:
            raise ValueError(
                f"Last dimension of input ({input_shape[-1]}) "
                f"must match dim ({self.dim})"
            )

        # Build sub-layers explicitly in computational order for robust serialization
        self.qkv_dense.build(input_shape)
        self.q_dense.build(input_shape)

        # proj_dense receives same shape as input (after attention computation)
        self.proj_dense.build(input_shape)

        # Build dropout layer if it exists
        if self.dropout_layer is not None:
            # Dropout operates on attention weights: (batch, num_heads, seq_len, seq_len)
            # We don't need to build it explicitly as it has no weights,
            # but we include it for completeness
            attention_shape = (
                input_shape[0],  # batch_size
                self.num_heads,  # num_heads
                input_shape[1],  # seq_len
                input_shape[1]  # seq_len (for attention matrix)
            )
            self.dropout_layer.build(attention_shape)

        # Always call parent build at the END
        super().build(input_shape)

    def call(
            self,
            x: keras.KerasTensor,
            num_anchor_tokens: Optional[int] = None,
            training: Optional[bool] = None
    ) -> keras.KerasTensor:
        """
        Apply anchor-based attention to input tensor.

        Args:
            x: Input tensor of shape (batch_size, seq_len, dim).
            num_anchor_tokens: Number of anchor tokens from the beginning.
                If None, applies standard self-attention to all tokens.
                If provided, treats first num_anchor_tokens as anchors with
                full self-attention, and remaining tokens as queries that
                only attend to anchors.
            training: Boolean indicating training mode for dropout application.

        Returns:
            Output tensor with same shape as input.
        """
        if num_anchor_tokens is None:
            # Standard self-attention mode - all tokens are anchors
            return self._standard_attention(x, training)
        else:
            # Hierarchical anchor-query attention mode
            return self._hierarchical_attention(x, num_anchor_tokens, training)

    def _standard_attention(
            self,
            x: keras.KerasTensor,
            training: Optional[bool]
    ) -> keras.KerasTensor:
        """Apply standard multi-head self-attention to all tokens."""
        batch_size, seq_len, _ = keras.ops.shape(x)

        # Compute Q, K, V projections
        qkv = self.qkv_dense(x)  # (batch_size, seq_len, dim * 3)
        qkv = keras.ops.reshape(
            qkv, (batch_size, seq_len, 3, self.num_heads, self.head_dim)
        )
        qkv = keras.ops.transpose(
            qkv, (2, 0, 3, 1, 4)
        )  # (3, batch_size, num_heads, seq_len, head_dim)

        q, k, v = qkv[0], qkv[1], qkv[2]

        # Scaled dot-product attention
        scores = keras.ops.matmul(
            q, keras.ops.transpose(k, (0, 1, 3, 2))
        ) * self.scale
        attn_weights = keras.ops.softmax(scores, axis=-1)

        # Apply dropout to attention weights if configured
        if self.dropout_layer is not None:
            attn_weights = self.dropout_layer(attn_weights, training=training)

        # Apply attention to values
        out = keras.ops.matmul(attn_weights, v)
        # Shape: (batch_size, num_heads, seq_len, head_dim)

        # Reshape and project to output
        out = keras.ops.transpose(out, (0, 2, 1, 3))
        # Shape: (batch_size, seq_len, num_heads, head_dim)
        out = keras.ops.reshape(out, (batch_size, seq_len, self.dim))
        out = self.proj_dense(out)

        return out

    def _hierarchical_attention(
            self,
            x: keras.KerasTensor,
            num_anchor_tokens: int,
            training: Optional[bool]
    ) -> keras.KerasTensor:
        """Apply hierarchical anchor-query attention pattern."""
        batch_size, seq_len, _ = keras.ops.shape(x)

        if num_anchor_tokens >= seq_len:
            # All tokens are anchors - fallback to standard attention
            return self._standard_attention(x, training)

        # Split input into anchor and query tokens
        anchor_tokens = x[:, :num_anchor_tokens, :]
        # Shape: (batch_size, num_anchor_tokens, dim)
        query_tokens = x[:, num_anchor_tokens:, :]
        # Shape: (batch_size, num_query_tokens, dim)
        num_query_tokens = seq_len - num_anchor_tokens

        # Process anchor tokens with full self-attention
        anchor_qkv = self.qkv_dense(anchor_tokens)
        # Shape: (batch_size, num_anchor_tokens, dim * 3)
        anchor_qkv = keras.ops.reshape(
            anchor_qkv,
            (batch_size, num_anchor_tokens, 3, self.num_heads, self.head_dim)
        )
        anchor_qkv = keras.ops.transpose(
            anchor_qkv, (2, 0, 3, 1, 4)
        )  # Shape: (3, batch_size, num_heads, num_anchor_tokens, head_dim)

        anchor_q, anchor_k, anchor_v = anchor_qkv[0], anchor_qkv[1], anchor_qkv[2]

        # Process query tokens (only Q projection - they don't self-attend)
        query_q = self.q_dense(query_tokens)
        # Shape: (batch_size, num_query_tokens, dim)
        query_q = keras.ops.reshape(
            query_q, (batch_size, num_query_tokens, self.num_heads, self.head_dim)
        )
        query_q = keras.ops.transpose(
            query_q, (0, 2, 1, 3)
        )  # Shape: (batch_size, num_heads, num_query_tokens, head_dim)

        # Combine Q vectors: anchors first, then queries
        combined_q = keras.ops.concatenate(
            [anchor_q, query_q], axis=2
        )  # Shape: (batch_size, num_heads, seq_len, head_dim)

        # Attention computation: all tokens attend to anchor tokens only
        scores = keras.ops.matmul(
            combined_q, keras.ops.transpose(anchor_k, (0, 1, 3, 2))
        ) * self.scale
        # Shape: (batch_size, num_heads, seq_len, num_anchor_tokens)

        attn_weights = keras.ops.softmax(scores, axis=-1)

        # Apply dropout to attention weights if configured
        if self.dropout_layer is not None:
            attn_weights = self.dropout_layer(attn_weights, training=training)

        # Apply attention to anchor values
        out = keras.ops.matmul(attn_weights, anchor_v)
        # Shape: (batch_size, num_heads, seq_len, head_dim)

        # Reshape and project to output
        out = keras.ops.transpose(out, (0, 2, 1, 3))
        # Shape: (batch_size, seq_len, num_heads, head_dim)
        out = keras.ops.reshape(out, (batch_size, seq_len, self.dim))
        out = self.proj_dense(out)

        return out

    def compute_output_shape(
            self, input_shape: Tuple[Optional[int], ...]
    ) -> Tuple[Optional[int], ...]:
        """
        Compute the output shape of the layer.

        Output shape is identical to input shape since attention preserves
        sequence length and feature dimensions.

        Args:
            input_shape: Shape tuple of the input.

        Returns:
            Output shape tuple, identical to input shape.
        """
        return input_shape

    def get_config(self) -> Dict[str, Any]:
        """
        Return configuration dictionary for serialization.

        This method must include ALL constructor parameters to ensure complete
        serialization and deserialization compatibility.

        Returns:
            Dictionary containing all initialization parameters and their values.
        """
        config = super().get_config()
        config.update({
            "dim": self.dim,
            "num_heads": self.num_heads,
            "dropout_rate": self.dropout_rate,
            "use_bias": self.use_bias,
            "kernel_initializer": keras.initializers.serialize(self.kernel_initializer),
            "bias_initializer": keras.initializers.serialize(self.bias_initializer),
            "kernel_regularizer": keras.regularizers.serialize(self.kernel_regularizer),
            "bias_regularizer": keras.regularizers.serialize(self.bias_regularizer),
        })
        return config

# ---------------------------------------------------------------------

