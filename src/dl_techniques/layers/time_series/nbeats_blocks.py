"""
The foundational block of the N-BEATS architecture.

This layer is the core computational unit of the Neural Basis Expansion
Analysis for Time Series (N-BEATS) model. It performs a functional
decomposition of the input time series by learning coefficients for a set of
basis functions.

Architecture and Design Philosophy:
The N-BEATS block is built on the principle of "basis expansion." Instead of
directly predicting future time steps, the block learns to represent the
input time series in a compact, latent parameter space (`theta`). These
parameters are then used as coefficients to generate both a forecast for the
future and a "backcast" that reconstructs the input.

The architecture consists of two main parts:
1.  A deep, fully-connected stack (MLP) that processes the input time series
    and extracts a high-level feature representation.
2.  Two linear projection heads that map this representation to separate
    `theta` vectors for the backcast and forecast.

The actual time series outputs are then generated by expanding these `theta`
coefficients using a set of basis functions. This layer is an abstract base
class; concrete subclasses (e.g., `TrendBlock`, `SeasonalityBlock`) define
the specific mathematical form of these basis functions (e.g., polynomials,
Fourier series). This design separates the complex, non-linear representation
learning (handled by the MLP) from the structured signal generation (handled
by the basis functions), providing a powerful inductive bias.

A key concept is the block's dual output of `backcast` and `forecast`. The
`backcast` is an approximation of the input signal. In the full N-BEATS
model, blocks are stacked hierarchically. The backcast from a block is
subtracted from its input, and the resulting residual signal is passed to the
next block. This "doubly residual stacking" allows the model to learn a
decomposition of the time series, where each block successively models and
removes a component of the signal (e.g., a trend, then a seasonality),
enabling deeper architectures and improved interpretability.

Foundational Mathematics:
The block approximates a function `f` that maps an input time series `x` of
length `H` to a forecast `y` of length `T`. It does so by decomposing the
mapping into two stages:
1.  `θ = f_θ(x)`: An MLP learns to produce a set of coefficients `θ`.
2.  `y = G(θ)`: A generator function `G` (the basis expansion) produces the
    forecast `y` as a linear combination of basis vectors `v_i` weighted by
    the coefficients `θ_i`:
        `y = Σᵢ θᵢ * vᵢ`

This is analogous to a Fourier or Taylor series expansion, but where the
coefficients `θ` are not fixed but are learned by a deep neural network
conditioned on the input `x`. This allows the model to dynamically adapt the
decomposition to each specific time series instance it encounters.

References:
    - [Oreshkin, B. N., Carpov, D., Chapados, N., & Bengio, Y. (2020).
      N-BEATS: Neural Basis Expansion Analysis for interpretable Time
      Series forecasting. In ICLR.](https://arxiv.org/abs/1905.10437)
"""

import keras
import numpy as np
from keras import ops
from abc import abstractmethod
from typing import Optional, Any, Tuple, Union

# ---------------------------------------------------------------------
# local imports
# ---------------------------------------------------------------------

from dl_techniques.utils.logger import logger
from dl_techniques.layers.norms.rms_norm import RMSNorm

# ---------------------------------------------------------------------


@keras.saving.register_keras_serializable()
class NBeatsBlock(keras.layers.Layer):
    """
    Enhanced N-BEATS block layer with performance optimizations and modern Keras 3 compliance.

    This is the base class for all N-BEATS blocks implementing the fundamental N-BEATS
    architecture with proper sub-layer management, improved initialization strategies,
    and numerical stability improvements. The block consists of a 4-layer fully connected
    stack followed by specialized basis function generation for backcast and forecast.

    **Intent**: Provide the foundational building block for N-BEATS time series forecasting
    models, ensuring proper serialization, gradient flow, and extensibility for different
    basis function types (generic, trend, seasonality).

    **Architecture**:
    ```
    Input(shape=[batch, backcast_length * input_dim])
           ↓
    Dense₁(units, activation)
           ↓
    (Optional) RMSNorm
           ↓
    Dense₂(units, activation)
           ↓
    (Optional) RMSNorm
           ↓
    Dense₃(units, activation)
           ↓
    (Optional) RMSNorm
           ↓
    Dense₄(units, activation)
           ↓
    (Optional) RMSNorm
           ↓
    ┌──────────────────────────────┬───────────────────────────────┐
    ↓                              ↓                               ↓
    ThetaBackcast                 ThetaForecast
    (thetas_dim * input_dim)      (thetas_dim * output_dim)
    ↓                              ↓
    BasisBackcast                 BasisForecast    ← (implemented by subclasses)
    ↓                              ↓
    Output                        Output
    [batch, backcast_len * dim]   [batch, forecast_len * dim]
    ```

    **Multivariate Support**:
    The block handles multivariate inputs/outputs by flattening dimensions in the dense
    stack but preserving the concept of (Time, Feature) separation in the basis projection
    layer. The theta coefficients are generated per-feature.

    Args:
        units: Integer, number of hidden units in the fully connected layers.
            Must be positive. Typical values: 256, 512, 1024.
        thetas_dim: Integer, dimensionality of the theta parameters passed to basis functions.
            Must be positive. Should match the complexity of the targeted pattern.
        backcast_length: Integer, length of the input time series (lookback window).
            Must be positive. Recommended: 3-5 times forecast_length.
        forecast_length: Integer, length of the forecast horizon.
            Must be positive. The number of future time steps to predict.
        input_dim: Integer, number of input features (channels). Defaults to 1.
        output_dim: Integer, number of output features (channels). Defaults to 1.
        share_weights: Boolean, whether to share weights across blocks in the same stack.
            Currently stored but not implemented. Defaults to False.
        activation: String or callable, activation function for hidden layers.
            Defaults to 'silu' for better gradient flow than ReLU.
        use_bias: Boolean, whether to add bias to the dense layers. Defaults to True.
        use_normalization: Boolean, whether to apply RMSNorm
            after each dense layer in the stack. Defaults to True.
        kernel_initializer: String or Initializer, initializer for FC layer weights.
            Defaults to 'he_normal' for better gradient flow with SiLU.
        theta_initializer: String or Initializer, initializer for theta layers.
            Defaults to 'glorot_uniform' for balanced parameter initialization.
        kernel_regularizer: Optional regularizer for FC layer weights.
        theta_regularizer: Optional regularizer for theta layer weights.
        **kwargs: Additional keyword arguments for the Layer parent class.

    Input shape:
        2D tensor with shape: `(batch_size, backcast_length * input_dim)`.

    Output shape:
        Tuple of 2D tensors:
        - Backcast: `(batch_size, backcast_length * input_dim)`
        - Forecast: `(batch_size, forecast_length * output_dim)`

    Attributes:
        dense1, dense2, dense3, dense4: Fully connected feature extraction layers.
        theta_backcast, theta_forecast: Parameter generation layers.

    Example:
        ```python
        # Define concrete subclass (e.g., GenericBlock, TrendBlock, SeasonalityBlock)
        block = GenericBlock(
            units=512,
            thetas_dim=32,
            backcast_length=168,
            forecast_length=24,
            input_dim=5,
            output_dim=5
        )

        inputs = keras.Input(shape=(168 * 5,))
        backcast, forecast = block(inputs)
        ```
    """

    def __init__(
            self,
            units: int,
            thetas_dim: int,
            backcast_length: int,
            forecast_length: int,
            input_dim: int = 1,
            output_dim: int = 1,
            share_weights: bool = False,
            activation: Union[str, callable] = 'relu',
            use_bias: bool = False,
            use_normalization: bool = False,
            kernel_initializer: Union[str, keras.initializers.Initializer] = 'he_normal',
            theta_initializer: Union[str, keras.initializers.Initializer] = 'glorot_uniform',
            kernel_regularizer: Optional[keras.regularizers.Regularizer] = None,
            theta_regularizer: Optional[keras.regularizers.Regularizer] = None,
            **kwargs: Any
    ) -> None:
        super().__init__(**kwargs)

        # Validate inputs with enhanced checks
        if units <= 0:
            raise ValueError(f"units must be positive, got {units}")
        if thetas_dim <= 0:
            raise ValueError(f"thetas_dim must be positive, got {thetas_dim}")
        if backcast_length <= 0:
            raise ValueError(f"backcast_length must be positive, got {backcast_length}")
        if forecast_length <= 0:
            raise ValueError(f"forecast_length must be positive, got {forecast_length}")
        if input_dim <= 0:
            raise ValueError(f"input_dim must be positive, got {input_dim}")
        if output_dim <= 0:
            raise ValueError(f"output_dim must be positive, got {output_dim}")

        # Warn if backcast_length might be too short
        if backcast_length < 2 * forecast_length:
            logger.warning(
                f"backcast_length ({backcast_length}) < 2 * forecast_length ({forecast_length}). "
                f"Consider using backcast_length >= 3-5 * forecast_length for better performance."
            )

        # Store configuration
        self.units = units
        self.thetas_dim = thetas_dim
        self.backcast_length = backcast_length
        self.forecast_length = forecast_length
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.share_weights = share_weights
        self.activation = activation
        self.use_bias = use_bias
        self.use_normalization = use_normalization
        self.kernel_initializer = keras.initializers.get(kernel_initializer)
        self.theta_initializer = keras.initializers.get(theta_initializer)
        self.kernel_regularizer = keras.regularizers.get(kernel_regularizer)
        self.theta_regularizer = keras.regularizers.get(theta_regularizer)

        # Conditionally create normalization layers
        if self.use_normalization:
            self.norm1 = RMSNorm(axis=-1, use_scale=False)
            self.norm2 = RMSNorm(axis=-1, use_scale=False)
            self.norm3 = RMSNorm(axis=-1, use_scale=False)
            self.norm4 = RMSNorm(axis=-1, use_scale=False)
        else:
            self.norm1 = self.norm2 = self.norm3 = self.norm4 = None

        # CREATE all sub-layers in __init__ (modern Keras 3 pattern)
        self.dense1 = keras.layers.Dense(
            self.units,
            use_bias=self.use_bias,
            activation=self.activation,
            kernel_initializer=self.kernel_initializer,
            kernel_regularizer=self.kernel_regularizer,
            name='dense1'
        )
        self.dense2 = keras.layers.Dense(
            self.units,
            use_bias=self.use_bias,
            activation=self.activation,
            kernel_initializer=self.kernel_initializer,
            kernel_regularizer=self.kernel_regularizer,
            name='dense2'
        )
        self.dense3 = keras.layers.Dense(
            self.units,
            use_bias=self.use_bias,
            activation=self.activation,
            kernel_initializer=self.kernel_initializer,
            kernel_regularizer=self.kernel_regularizer,
            name='dense3'
        )
        self.dense4 = keras.layers.Dense(
            self.units,
            use_bias=self.use_bias,
            activation=self.activation,
            kernel_initializer=self.kernel_initializer,
            kernel_regularizer=self.kernel_regularizer,
            name='dense4'
        )

        # Theta projection layers
        # For multivariate, we generate unique thetas for each feature
        # Shape: thetas_dim * input_dim (or output_dim)
        self.theta_backcast = keras.layers.Dense(
            self.thetas_dim * self.input_dim,
            activation='linear',
            use_bias=False,
            kernel_initializer=self.theta_initializer,
            kernel_regularizer=self.theta_regularizer,
            name='theta_backcast'
        )
        self.theta_forecast = keras.layers.Dense(
            self.thetas_dim * self.output_dim,
            activation='linear',
            use_bias=False,
            kernel_initializer=self.theta_initializer,
            kernel_regularizer=self.theta_regularizer,
            name='theta_forecast'
        )

    def build(self, input_shape: Tuple[Optional[int], ...]) -> None:
        """Build the layer weights and explicitly build all sub-layers."""
        # Validate input shape
        if len(input_shape) != 2:
            raise ValueError(f"Expected 2D input shape, got {len(input_shape)}D: {input_shape}")

        # BUILD all sub-layers explicitly for proper serialization
        self.dense1.build(input_shape)

        # Subsequent layers take the output of the previous dense layer
        dense_output_shape = (input_shape[0], self.units)
        self.dense2.build(dense_output_shape)
        self.dense3.build(dense_output_shape)
        self.dense4.build(dense_output_shape)
        self.theta_backcast.build(dense_output_shape)
        self.theta_forecast.build(dense_output_shape)

        # Call parent build at the end
        super().build(input_shape)

    def call(
            self,
            inputs: keras.KerasTensor,
            training: Optional[bool] = None
    ) -> Tuple[keras.KerasTensor, keras.KerasTensor]:
        """
        Forward pass with performance optimizations.

        Args:
            inputs: Input tensor of shape (batch_size, backcast_length * input_dim).
            training: Boolean indicating training mode.

        Returns:
            Tuple of (backcast, forecast) tensors.
        """
        # Validate input shape at runtime
        input_shape = ops.shape(inputs)
        if len(input_shape) != 2:
            raise ValueError(f"Expected 2D input, got shape: {input_shape}")

        # Pass through four fully connected layers
        x = self.dense1(inputs, training=training)
        if self.use_normalization:
            x = self.norm1(x)
        x = self.dense2(x, training=training)
        if self.use_normalization:
            x = self.norm2(x)
        x = self.dense3(x, training=training)
        if self.use_normalization:
            x = self.norm3(x)
        x = self.dense4(x, training=training)
        if self.use_normalization:
            x = self.norm4(x)

        # Generate theta parameters
        # Shape: (batch_size, thetas_dim * dim)
        theta_b = self.theta_backcast(x, training=training)
        theta_f = self.theta_forecast(x, training=training)

        # Generate backcast and forecast using basis functions (implemented by subclasses)
        backcast = self._generate_backcast(theta_b)
        forecast = self._generate_forecast(theta_f)

        return backcast, forecast

    @abstractmethod
    def _generate_backcast(self, theta: keras.KerasTensor) -> keras.KerasTensor:
        """
        Generate backcast from theta parameters using basis functions.

        Args:
            theta: Theta parameters for backcast generation.

        Returns:
            Backcast tensor of shape (batch, backcast_length * input_dim).
        """
        pass

    @abstractmethod
    def _generate_forecast(self, theta: keras.KerasTensor) -> keras.KerasTensor:
        """
        Generate forecast from theta parameters using basis functions.

        Args:
            theta: Theta parameters for forecast generation.

        Returns:
            Forecast tensor of shape (batch, forecast_length * output_dim).
        """
        pass

    def compute_output_shape(
            self,
            input_shape: Tuple[Optional[int], ...]
    ) -> Tuple[Tuple[Optional[int], ...], Tuple[Optional[int], ...]]:
        """
        Compute the output shape of the layer.

        Args:
            input_shape: Shape of the input tensor.

        Returns:
            Tuple of (backcast_shape, forecast_shape).
        """
        if len(input_shape) != 2:
            raise ValueError(f"Expected 2D input shape, got {len(input_shape)}D: {input_shape}")

        batch_size = input_shape[0]
        backcast_shape = (batch_size, self.backcast_length * self.input_dim)
        forecast_shape = (batch_size, self.forecast_length * self.output_dim)
        return backcast_shape, forecast_shape

    def get_config(self) -> dict:
        """
        Get layer configuration for serialization.

        Returns:
            Configuration dictionary.
        """
        config = super().get_config()
        config.update({
            'units': self.units,
            'thetas_dim': self.thetas_dim,
            'backcast_length': self.backcast_length,
            'forecast_length': self.forecast_length,
            'input_dim': self.input_dim,
            'output_dim': self.output_dim,
            'share_weights': self.share_weights,
            'activation': self.activation,
            'use_bias': self.use_bias,
            'use_normalization': self.use_normalization,
            'kernel_initializer': keras.initializers.serialize(self.kernel_initializer),
            'theta_initializer': keras.initializers.serialize(self.theta_initializer),
            'kernel_regularizer': keras.regularizers.serialize(self.kernel_regularizer),
            'theta_regularizer': keras.regularizers.serialize(self.theta_regularizer),
        })
        return config


# ---------------------------------------------------------------------


@keras.saving.register_keras_serializable()
class GenericBlock(NBeatsBlock):
    """
    Generic N-BEATS block with learnable linear transformations for flexible pattern modeling.

    This block uses trainable Dense layers as basis functions, allowing the model to learn
    arbitrary linear transformations from theta parameters to backcast/forecast outputs.
    It provides maximum flexibility by not constraining the basis functions to specific
    mathematical forms (unlike Trend or Seasonality blocks).

    **Intent**: Provide a flexible N-BEATS block that can learn any linear patterns
    without mathematical constraints, suitable for complex time series that don't
    follow clear trend or seasonal patterns.

    **Architecture**:
    ```
    Input → NBeatsBlock (4 Dense + 2 Theta) → [theta_b, theta_f]
                                               ↓         ↓
                                         BasisBackcast  BasisForecast
                                         (Dense Linear) (Dense Linear)
                                               ↓         ↓
                                           backcast   forecast
    ```

    **Basis Function Details**:
    - **Backcast Basis**: Dense(thetas_dim*in_dim → backcast_len*in_dim, linear)
    - **Forecast Basis**: Dense(thetas_dim*out_dim → forecast_len*out_dim, linear)
    - **Initialization**: Orthogonal with small gain (0.1) for stable training

    Args:
        basis_initializer: String or Initializer, initializer for basis matrices.
            Defaults to 'glorot_uniform'. Consider 'orthogonal' for better stability.
        basis_regularizer: Optional regularizer for basis matrices.
            Can help prevent overfitting in the learned basis functions.
        **kwargs: Arguments passed to parent NBeatsBlock.

    Attributes:
        backcast_basis: Dense layer for backcast basis function.
        forecast_basis: Dense layer for forecast basis function.
    """

    def __init__(
            self,
            basis_initializer: Union[str, keras.initializers.Initializer] = 'glorot_uniform',
            basis_regularizer: Optional[keras.regularizers.Regularizer] = None,
            **kwargs: Any
    ) -> None:
        super().__init__(**kwargs)

        # Store configuration
        self.basis_initializer = keras.initializers.get(basis_initializer)
        self.basis_regularizer = keras.regularizers.get(basis_regularizer)

        # CREATE sub-layers specific to GenericBlock in __init__
        # Use orthogonal initialization with small gain for stability
        orthogonal_init = keras.initializers.Orthogonal(gain=0.1)

        # For Generic Block, the projection is just a large matrix multiplication
        # We project from flattened theta to flattened time series directly.
        self.backcast_basis = keras.layers.Dense(
            self.backcast_length * self.input_dim,
            activation='linear',
            use_bias=False,
            kernel_initializer=orthogonal_init,
            kernel_regularizer=self.basis_regularizer,
            name='backcast_basis'
        )
        self.forecast_basis = keras.layers.Dense(
            self.forecast_length * self.output_dim,
            activation='linear',
            use_bias=False,
            kernel_initializer=orthogonal_init,
            kernel_regularizer=self.basis_regularizer,
            name='forecast_basis'
        )

    def build(self, input_shape: Tuple[Optional[int], ...]) -> None:
        """Build the generic block and its sub-layers."""
        # BUILD the GenericBlock-specific sub-layers
        # Their input is theta, which has shape (batch_size, thetas_dim * dim)
        theta_backcast_shape = (input_shape[0], self.thetas_dim * self.input_dim)
        theta_forecast_shape = (input_shape[0], self.thetas_dim * self.output_dim)

        self.backcast_basis.build(theta_backcast_shape)
        self.forecast_basis.build(theta_forecast_shape)

        # Call parent build method (which builds the Dense stack)
        super().build(input_shape)

    def _generate_backcast(self, theta: keras.KerasTensor) -> keras.KerasTensor:
        """
        Generate backcast using learnable basis functions.

        Args:
            theta: Theta parameters (batch, thetas_dim * input_dim).

        Returns:
            Backcast tensor (batch, backcast_length * input_dim).
        """
        return self.backcast_basis(theta)

    def _generate_forecast(self, theta: keras.KerasTensor) -> keras.KerasTensor:
        """
        Generate forecast using learnable basis functions.

        Args:
            theta: Theta parameters (batch, thetas_dim * output_dim).

        Returns:
            Forecast tensor (batch, forecast_length * output_dim).
        """
        return self.forecast_basis(theta)

    def get_config(self) -> dict:
        """
        Get layer configuration for serialization.

        Returns:
            Configuration dictionary.
        """
        config = super().get_config()
        config.update({
            'basis_initializer': keras.initializers.serialize(self.basis_initializer),
            'basis_regularizer': keras.regularizers.serialize(self.basis_regularizer),
        })
        return config


# ---------------------------------------------------------------------


@keras.saving.register_keras_serializable()
class TrendBlock(NBeatsBlock):
    """
    Trend N-BEATS block with polynomial basis functions for modeling trending behavior.

    This block uses mathematically-defined polynomial basis functions to explicitly
    model trend patterns in time series data. In multivariate settings, it learns
    separate polynomial coefficients (thetas) for each feature.

    **Intent**: Provide a specialized N-BEATS block for explicitly modeling trend
    components in time series, using polynomial basis functions that guarantee
    mathematical continuity and smooth extrapolation.

    **Architecture & Mathematical Foundation**:
    ```
    Input → NBeatsBlock (4 Dense + 2 Theta) → [theta_b, theta_f]
                                               ↓         ↓
                                         PolyBasis_b  PolyBasis_f
                                         (degree 0,1,2,...)
                                               ↓         ↓
                                           backcast   forecast

    Polynomial Basis Functions:
    - Degree 0: f₀(t) = 1              (constant/level)
    - Degree 1: f₁(t) = t              (linear trend)
    - Degree 2: f₂(t) = t²             (quadratic trend)
    - ...
    - Degree n-1: fₙ₋₁(t) = t^(n-1)    (higher-order trends)

    Time normalization: t ∈ [-1, 1] centered at backcast-forecast transition
    ```

    **Multivariate Logic**:
    Theta is reshaped to `(Batch, FeatureDim, Degree)`. The basis matrix is
    `(Degree, Time)`. Matrix multiplication produces `(Batch, FeatureDim, Time)`.
    This preserves the temporal structure of the trend per-feature before flattening.

    Args:
        normalize_basis: Boolean, whether to normalize polynomial basis functions
            for better numerical conditioning. Defaults to True.
        **kwargs: Arguments passed to parent NBeatsBlock.

    Attributes:
        backcast_basis_matrix: Non-trainable weight matrix for backcast polynomial basis.
        forecast_basis_matrix: Non-trainable weight matrix for forecast polynomial basis.
    """

    def __init__(
            self,
            normalize_basis: bool = True,
            **kwargs: Any
    ) -> None:
        super().__init__(**kwargs)

        # Store configuration
        self.normalize_basis = normalize_basis

        # Weights created in build() - these are not sub-layers
        self.backcast_basis_matrix = None
        self.forecast_basis_matrix = None

    def build(self, input_shape: Tuple[Optional[int], ...]) -> None:
        """Build the trend block with corrected basis functions."""
        if self.thetas_dim < 1:
            raise ValueError(f"thetas_dim must be at least 1 for TrendBlock, got {self.thetas_dim}")

        # Create polynomial basis matrices (these are weights, not sub-layers)
        self._create_polynomial_basis()

        # Call parent's build method to handle Dense layers
        super().build(input_shape)

    def _create_polynomial_basis(self) -> None:
        """Create mathematically correct polynomial basis matrices with continuity."""
        # Create continuous time vector for proper polynomial extrapolation
        total_length = self.backcast_length + self.forecast_length

        # Create continuous time indices
        time_indices = np.arange(total_length, dtype=np.float32)

        # Normalize to [-1, 1] range for better numerical stability
        # This centers the polynomial around the transition point
        time_normalized = 2.0 * (time_indices - self.backcast_length) / total_length

        # Split into backcast and forecast portions
        backcast_time = time_normalized[:self.backcast_length]
        forecast_time = time_normalized[self.backcast_length:]

        # Initialize basis matrices
        backcast_basis = np.zeros((self.thetas_dim, self.backcast_length), dtype=np.float32)
        forecast_basis = np.zeros((self.thetas_dim, self.forecast_length), dtype=np.float32)

        # Generate polynomial terms with improved numerical stability
        for degree in range(self.thetas_dim):
            if degree == 0:
                # Constant term
                backcast_basis[degree] = np.ones_like(backcast_time)
                forecast_basis[degree] = np.ones_like(forecast_time)
            else:
                # Polynomial terms: t^degree
                backcast_basis[degree] = np.power(backcast_time, degree)
                forecast_basis[degree] = np.power(forecast_time, degree)

            # Optional normalization for better conditioning
            if self.normalize_basis and degree > 0:
                # Normalize based on the expected range of values
                scale_factor = np.sqrt(degree + 1)  # Simple scaling based on degree
                backcast_basis[degree] /= scale_factor
                forecast_basis[degree] /= scale_factor

        # Store as non-trainable weights
        # Shape: (thetas_dim, time)
        self.backcast_basis_matrix = self.add_weight(
            name='backcast_basis_matrix',
            shape=(self.thetas_dim, self.backcast_length),
            initializer='zeros',
            trainable=False
        )
        self.forecast_basis_matrix = self.add_weight(
            name='forecast_basis_matrix',
            shape=(self.thetas_dim, self.forecast_length),
            initializer='zeros',
            trainable=False
        )

        # Set the values
        self.backcast_basis_matrix.assign(backcast_basis)
        self.forecast_basis_matrix.assign(forecast_basis)

    def _generate_backcast(self, theta: keras.KerasTensor) -> keras.KerasTensor:
        """
        Generate backcast using polynomial basis functions.

        Process:
        1. Reshape theta: (Batch, Flat) -> (Batch, InputDim, PolyDegree)
        2. MatMul: (Batch, InputDim, PolyDegree) @ (PolyDegree, Time) -> (Batch, InputDim, Time)
        3. Transpose: (Batch, InputDim, Time) -> (Batch, Time, InputDim)
        4. Flatten: -> (Batch, Time * InputDim)

        This ensures that when the residual stream is reconstructed, index (t, d) corresponds correctly.

        Args:
            theta: Theta parameters (batch, thetas_dim * input_dim).

        Returns:
            Backcast tensor (batch, backcast_length * input_dim).
        """
        # 1. Reshape to separate features and polynomial degrees
        theta_reshaped = ops.reshape(theta, (-1, self.input_dim, self.thetas_dim))

        # 2. Apply basis function (broadcasts over batch and input_dim)
        result = ops.matmul(theta_reshaped, self.backcast_basis_matrix)

        # 3. Transpose to (Batch, Time, InputDim) to match flattened order
        result = ops.transpose(result, (0, 2, 1))

        # 4. Flatten back to residual stream format
        return ops.reshape(result, (-1, self.backcast_length * self.input_dim))

    def _generate_forecast(self, theta: keras.KerasTensor) -> keras.KerasTensor:
        """
        Generate forecast using polynomial basis functions.

        Args:
            theta: Theta parameters (batch, thetas_dim * output_dim).

        Returns:
            Forecast tensor (batch, forecast_length * output_dim).
        """
        # 1. Reshape to separate features and polynomial degrees
        theta_reshaped = ops.reshape(theta, (-1, self.output_dim, self.thetas_dim))

        # 2. Apply basis function
        result = ops.matmul(theta_reshaped, self.forecast_basis_matrix)

        # 3. Transpose to (Batch, Time, OutputDim)
        result = ops.transpose(result, (0, 2, 1))

        # 4. Flatten
        return ops.reshape(result, (-1, self.forecast_length * self.output_dim))

    def get_config(self) -> dict:
        """
        Get layer configuration for serialization.

        Returns:
            Configuration dictionary.
        """
        config = super().get_config()
        config.update({
            'normalize_basis': self.normalize_basis,
        })
        return config


# ---------------------------------------------------------------------


@keras.saving.register_keras_serializable()
class SeasonalityBlock(NBeatsBlock):
    """
    Seasonality N-BEATS block with corrected Fourier basis functions for periodic patterns.

    This block uses mathematically-defined Fourier (sine/cosine) basis functions to
    explicitly model seasonal and periodic patterns. In multivariate settings, it learns
    separate harmonic coefficients for each feature.

    **Intent**: Provide a specialized N-BEATS block for explicitly modeling seasonal
    and periodic components in time series, using Fourier basis functions that
    guarantee mathematical continuity and proper frequency relationships.

    **Architecture & Mathematical Foundation**:
    ```
    Input → NBeatsBlock (4 Dense + 2 Theta) → [theta_b, theta_f]
                                               ↓         ↓
                                         FourierBasis_b  FourierBasis_f
                                         (harmonics 1,2,3,...)
                                               ↓         ↓
                                           backcast   forecast
    ```

    **Multivariate Logic**:
    Similar to TrendBlock, theta is reshaped to `(Batch, FeatureDim, Harmonics)`
    before applying the basis matrix. The result is transposed to `(Batch, Time, FeatureDim)`
    before flattening to ensure correct alignment with the input stream.

    Args:
        normalize_basis: Boolean, whether to normalize Fourier basis functions
            based on their full-sequence energy for better numerical stability.
            Defaults to True.
        **kwargs: Arguments passed to parent NBeatsBlock.

    Attributes:
        backcast_basis_matrix: Non-trainable weight matrix for backcast Fourier basis.
        forecast_basis_matrix: Non-trainable weight matrix for forecast Fourier basis.
    """

    def __init__(
            self,
            normalize_basis: bool = True,
            **kwargs: Any
    ) -> None:
        super().__init__(**kwargs)

        # Store configuration
        self.normalize_basis = normalize_basis

        # Weights created in build() - these are not sub-layers
        self.backcast_basis_matrix = None
        self.forecast_basis_matrix = None

    def build(self, input_shape: Tuple[Optional[int], ...]) -> None:
        """Build the seasonality block with corrected basis functions."""
        if self.thetas_dim < 2:
            logger.warning(f"thetas_dim ({self.thetas_dim}) < 2 for SeasonalityBlock. Consider using even numbers.")

        # Create Fourier basis matrices (these are weights, not sub-layers)
        self._create_fourier_basis()

        # Call parent's build method to handle Dense layers
        super().build(input_shape)

    def _create_fourier_basis(self) -> None:
        """Create mathematically correct Fourier basis matrices with continuity."""
        # Number of harmonics (sin/cos pairs)
        num_harmonics = self.thetas_dim // 2

        # Create continuous time indices for proper frequency relationship
        backcast_indices = np.arange(self.backcast_length, dtype=np.float32)
        forecast_indices = np.arange(
            self.backcast_length,
            self.backcast_length + self.forecast_length,
            dtype=np.float32
        )

        # Initialize basis matrices
        backcast_basis = np.zeros((self.thetas_dim, self.backcast_length), dtype=np.float32)
        forecast_basis = np.zeros((self.thetas_dim, self.forecast_length), dtype=np.float32)

        # Use total sequence length for period calculation
        total_length = self.backcast_length + self.forecast_length

        # Generate Fourier terms with correct frequencies
        basis_idx = 0

        for harmonic in range(1, num_harmonics + 1):
            if basis_idx >= self.thetas_dim:
                break

            # Frequency for this harmonic
            frequency = 2.0 * np.pi * harmonic / total_length

            # Cosine component with continuous normalization
            if basis_idx < self.thetas_dim:
                cos_backcast = np.cos(frequency * backcast_indices)
                cos_forecast = np.cos(frequency * forecast_indices)

                # Normalize based on COMBINED sequence for continuity
                if self.normalize_basis:
                    # Create full continuous cosine for norm calculation
                    full_indices = np.arange(total_length, dtype=np.float32)
                    full_cosine = np.cos(frequency * full_indices)
                    full_norm = np.linalg.norm(full_cosine)

                    if full_norm > 1e-8:
                        cos_backcast /= full_norm
                        cos_forecast /= full_norm

                backcast_basis[basis_idx] = cos_backcast
                forecast_basis[basis_idx] = cos_forecast
                basis_idx += 1

            # Sine component with continuous normalization
            if basis_idx < self.thetas_dim:
                sin_backcast = np.sin(frequency * backcast_indices)
                sin_forecast = np.sin(frequency * forecast_indices)

                # Normalize based on COMBINED sequence for continuity
                if self.normalize_basis:
                    # Create full continuous sine for norm calculation
                    full_indices = np.arange(total_length, dtype=np.float32)
                    full_sine = np.sin(frequency * full_indices)
                    full_norm = np.linalg.norm(full_sine)

                    if full_norm > 1e-8:
                        sin_backcast /= full_norm
                        sin_forecast /= full_norm

                backcast_basis[basis_idx] = sin_backcast
                forecast_basis[basis_idx] = sin_forecast
                basis_idx += 1

        # Handle odd theta_dim: add DC component
        if self.thetas_dim % 2 == 1 and basis_idx < self.thetas_dim:
            backcast_basis[basis_idx] = 1.0  # DC component
            forecast_basis[basis_idx] = 1.0
            basis_idx += 1

        # Store as non-trainable weights
        self.backcast_basis_matrix = self.add_weight(
            name='backcast_basis_matrix',
            shape=(self.thetas_dim, self.backcast_length),
            initializer='zeros',
            trainable=False
        )
        self.forecast_basis_matrix = self.add_weight(
            name='forecast_basis_matrix',
            shape=(self.thetas_dim, self.forecast_length),
            initializer='zeros',
            trainable=False
        )

        # Set the values
        self.backcast_basis_matrix.assign(backcast_basis)
        self.forecast_basis_matrix.assign(forecast_basis)

    def _generate_backcast(self, theta: keras.KerasTensor) -> keras.KerasTensor:
        """
        Generate backcast using Fourier basis functions.

        Process:
        1. Reshape theta: (Batch, Flat) -> (Batch, InputDim, Harmonics)
        2. MatMul: (Batch, InputDim, Harmonics) @ (Harmonics, Time) -> (Batch, InputDim, Time)
        3. Transpose: (Batch, InputDim, Time) -> (Batch, Time, InputDim)
        4. Flatten: -> (Batch, Time * InputDim)

        Args:
            theta: Theta parameters (batch, thetas_dim * input_dim).

        Returns:
            Backcast tensor (batch, backcast_length * input_dim).
        """
        # 1. Reshape
        theta_reshaped = ops.reshape(theta, (-1, self.input_dim, self.thetas_dim))

        # 2. Apply basis function
        result = ops.matmul(theta_reshaped, self.backcast_basis_matrix)

        # 3. Transpose
        result = ops.transpose(result, (0, 2, 1))

        # 4. Flatten
        return ops.reshape(result, (-1, self.backcast_length * self.input_dim))

    def _generate_forecast(self, theta: keras.KerasTensor) -> keras.KerasTensor:
        """
        Generate forecast using Fourier basis functions.

        Args:
            theta: Theta parameters (batch, thetas_dim * output_dim).

        Returns:
            Forecast tensor (batch, forecast_length * output_dim).
        """
        # 1. Reshape
        theta_reshaped = ops.reshape(theta, (-1, self.output_dim, self.thetas_dim))

        # 2. Apply basis function
        result = ops.matmul(theta_reshaped, self.forecast_basis_matrix)

        # 3. Transpose
        result = ops.transpose(result, (0, 2, 1))

        # 4. Flatten
        return ops.reshape(result, (-1, self.forecast_length * self.output_dim))

    def get_config(self) -> dict:
        """
        Get layer configuration for serialization.

        Returns:
            Configuration dictionary.
        """
        config = super().get_config()
        config.update({
            'normalize_basis': self.normalize_basis,
        })
        return config

# ---------------------------------------------------------------------
