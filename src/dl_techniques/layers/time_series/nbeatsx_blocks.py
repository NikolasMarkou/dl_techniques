import keras
from keras import ops, layers

# ---------------------------------------------------------------------
# local imports
# ---------------------------------------------------------------------

from .nbeats_blocks import NBeatsBlock
from .temporal_convolutional_network import TemporalConvNet

# ---------------------------------------------------------------------

@keras.saving.register_keras_serializable()
class ExogenousBlock(NBeatsBlock):
    """
    N-BEATSx Exogenous Block.

    This block incorporates exogenous variables (covariates) to generate backcast
    and forecast. Unlike standard N-BEATS blocks where the basis is fixed (Trend/Seasonality)
    or static (Generic), here the basis is dynamic and generated by an encoder (TCN)
    running on the exogenous variables $X$.

    Reference: Eq (9) in Olivares et al. (2021).

    **Architecture**:
    1.  Dense Stack (from NBeatsBlock) processes the residual Target $y$.
        -> Produces Theta coefficients.
    2.  Encoder (TCN) processes Exogenous $X$ (Backcast + Forecast periods).
        -> Produces Context Basis $C$.
    3.  Projection: Output = $\sum \theta \cdot C$.

    Args:
        exogenous_dim: Integer, number of exogenous features.
        tcn_filters: Integer, number of filters in the TCN encoder.
            Usually equals thetas_dim.
        tcn_kernel_size: Integer, kernel size for TCN.
        tcn_dropout: Float, dropout for TCN.
        use_tcn: Boolean, if True uses TCN basis (NBEATSx-G).
            If False, uses linear projection of X (NBEATSx-I).
        **kwargs: Arguments passed to NBeatsBlock.
    """

    def __init__(
            self,
            exogenous_dim: int,
            tcn_filters: int = 16,
            tcn_kernel_size: int = 2,
            tcn_dropout: float = 0.0,
            use_tcn: bool = True,
            **kwargs
    ):
        # In NBEATSx, theta dimension corresponds to the channels of the basis/exog
        # Ensure kwargs['thetas_dim'] matches or is set correctly if not provided
        super().__init__(**kwargs)

        self.exogenous_dim = exogenous_dim
        self.use_tcn = use_tcn
        self.tcn_filters = tcn_filters
        self.tcn_kernel_size = tcn_kernel_size
        self.tcn_dropout = tcn_dropout

        # The encoder for exogenous variables
        if self.use_tcn:
            self.encoder = TemporalConvNet(
                filters=tcn_filters,
                kernel_size=tcn_kernel_size,
                dropout_rate=tcn_dropout
            )
        else:
            # For Interpretable (NBEATSx-I), the basis is just X itself.
            # No complex encoder needed, but we might need projection if dims don't match
            self.encoder = None

        # Redefine Theta layers to match TCN/Exog dimensionality
        # Theta must project to the dimension of the Basis channels
        basis_channels = tcn_filters if use_tcn else exogenous_dim

        self.theta_backcast = layers.Dense(
            basis_channels,
            use_bias=False,
            name='theta_backcast_exog'
        )
        self.theta_forecast = layers.Dense(
            basis_channels,
            use_bias=False,
            name='theta_forecast_exog'
        )

    def build(self, input_shape):
        # We need to build the Dense stack from the parent
        # input_shape here refers to the residual input (y)
        super().build(input_shape)

        # Build encoder
        # Input to encoder is (Batch, Time, Exog_Dim)
        total_len = self.backcast_length + self.forecast_length
        if self.use_tcn:
            self.encoder.build((None, total_len, self.exogenous_dim))

    def call(self, inputs, training=None, exogenous_inputs=None):
        """
        Forward pass for Exogenous Block.

        Args:
            inputs: Residual target signal (batch, backcast_len * input_dim).
            training: Boolean.
            exogenous_inputs: Tuple (x_backcast, x_forecast).
                x_backcast: (batch, backcast_len, exog_dim)
                x_forecast: (batch, forecast_len, exog_dim)
        """
        if exogenous_inputs is None:
            raise ValueError("ExogenousBlock requires 'exogenous_inputs' passed to call.")

        x_back, x_fore = exogenous_inputs

        # 1. Process Target Residual through Dense Stack (Parent Logic)
        # -----------------------------------------------------------
        # Note: We duplicate the FC stack logic here because the parent .call()
        # calls _generate_backcast immediately, but we need the exogenous basis first.

        x = self.dense1(inputs, training=training)
        if self.use_normalization: x = self.norm1(x)
        x = self.dense2(x, training=training)
        if self.use_normalization: x = self.norm2(x)
        x = self.dense3(x, training=training)
        if self.use_normalization: x = self.norm3(x)
        x = self.dense4(x, training=training)
        if self.use_normalization: x = self.norm4(x)

        # 2. Generate Theta Coefficients (Weights for the Basis)
        # -----------------------------------------------------------
        # Shape: (Batch, Basis_Channels)
        # Note: Unlike standard N-BEATS, these are global weights per sample,
        # not per time-step. They scale the TCN basis vectors.
        theta_b = self.theta_backcast(x, training=training)
        theta_f = self.theta_forecast(x, training=training)

        # 3. Generate Basis from Exogenous Variables
        # -----------------------------------------------------------
        # Concatenate history and future exogenous: (Batch, Total_Len, Exog_Dim)
        x_full = ops.concatenate([x_back, x_fore], axis=1)

        if self.use_tcn:
            # Basis C = TCN(X)
            # Shape: (Batch, Total_Len, TCN_Filters)
            basis_full = self.encoder(x_full, training=training)
        else:
            # Basis = X (Interpretable configuration)
            basis_full = x_full

        # Split basis back into backcast and forecast periods
        basis_b = basis_full[:, :self.backcast_length, :]
        basis_f = basis_full[:, self.backcast_length:, :]

        # 4. Projection (Eq 9)
        # -----------------------------------------------------------
        # Backcast = Basis_b * theta_b
        # Ops: Element-wise multiplication along the channel axis (broadcasting theta)
        # or Einstein summation.
        # basis: (B, T, C), theta: (B, C) -> result: (B, T) (summed over C)
        # Standard NBEATSx sums over channels to get the univariate signal.

        # If output_dim > 1 (multivariate target), we need theta to be (B, C, OutDim)
        # But here we simplified theta to (B, C).
        # We assume 1-1 mapping or summation for univariate targets.

        backcast = ops.einsum('btc,bc->bt', basis_b, theta_b)
        forecast = ops.einsum('btc,bc->bt', basis_f, theta_f)

        # Reshape to flatten (Batch, Time) -> (Batch, Time * 1) if input_dim=1
        # If input_dim > 1, this specific block implementation assumes univariate projection
        # or shared exogenous effects. For strict consistency with provided NBeatsBlock:
        backcast = ops.reshape(backcast, (-1, self.backcast_length * 1))  # Assuming 1D output contribution
        forecast = ops.reshape(forecast, (-1, self.forecast_length * 1))

        # If the model expects input_dim > 1, we might need to broadcast or tile this
        # to match the residual shape, but typically Exog block models the primary signal.

        return backcast, forecast

    def _generate_backcast(self, theta):
        # Not used, overridden in call
        pass

    def _generate_forecast(self, theta):
        # Not used, overridden in call
        pass

    def get_config(self):
        config = super().get_config()
        config.update({
            'exogenous_dim': self.exogenous_dim,
            'tcn_filters': self.tcn_filters,
            'tcn_kernel_size': self.tcn_kernel_size,
            'tcn_dropout': self.tcn_dropout,
            'use_tcn': self.use_tcn
        })
        return config

# ---------------------------------------------------------------------
