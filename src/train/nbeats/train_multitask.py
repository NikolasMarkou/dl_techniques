import os
import json
import keras
import random
import matplotlib
import numpy as np
import seaborn as sns
import tensorflow as tf
from datetime import datetime
from dataclasses import dataclass, field
from typing import Dict, List, Tuple, Any, Optional

# Use a non-interactive backend for saving plots to files
matplotlib.use('Agg')
import matplotlib.pyplot as plt

# ---------------------------------------------------------------------
# Local imports
# ---------------------------------------------------------------------

from dl_techniques.utils.logger import logger
from dl_techniques.losses.smape_loss import SMAPELoss
from dl_techniques.utils.datasets.nbeats import TimeSeriesNormalizer
from dl_techniques.utils.datasets.time_series_generator import TimeSeriesGenerator, TimeSeriesConfig

from dl_techniques.models.nbeats_multitask import (
    MultiTaskNBeatsNet,
    MultiTaskNBeatsConfig,
    create_multi_task_nbeats
)

# ---------------------------------------------------------------------
# Set up plotting style
# ---------------------------------------------------------------------

plt.style.use('default')
sns.set_palette("husl")


# ---------------------------------------------------------------------
# Random seeds for reproducibility
# ---------------------------------------------------------------------

def set_random_seeds(seed: int = 42):
    """Set random seeds for reproducibility in Keras 3.x."""
    random.seed(seed)
    np.random.seed(seed)
    keras.utils.set_random_seed(seed)
    tf.random.set_seed(seed)


set_random_seeds(42)


# ---------------------------------------------------------------------
# Multi-Task Configuration with Trainable Task Inference
# ---------------------------------------------------------------------

@dataclass
class MultiTaskNBeatsTrainingConfig:
    """Configuration for multi-task N-BEATS training with trainable task inference."""

    # General experiment configuration
    result_dir: str = "results"
    save_results: bool = True
    experiment_name: str = "multi_task_nbeats_trainable"

    # Data configuration
    train_ratio: float = 0.7
    val_ratio: float = 0.15
    test_ratio: float = 0.15

    # N-BEATS specific configuration
    backcast_length: int = 168
    forecast_length: int = 24
    forecast_horizons: List[int] = field(default_factory=lambda: [12])

    # Multi-task model configuration
    use_task_embeddings: bool = True
    task_embedding_dim: int = 32
    shared_layers: int = 4

    # NEW: Trainable Task Inference Configuration
    train_task_inference: bool = True
    task_inference_loss_weight: float = 0.1
    consistency_loss_weight: float = 0.05
    entropy_loss_weight: float = 0.02
    consistency_temperature: float = 0.1
    min_entropy_target: float = 0.5

    # NEW: Curriculum Learning Configuration
    use_curriculum_learning: bool = True
    curriculum_start_ratio: float = 1.0  # Start with 100% labeled data
    curriculum_end_ratio: float = 0.3  # End with 30% labeled data
    curriculum_transition_epochs: int = 50  # Transition over 50 epochs

    # Model architecture
    stack_types: List[str] = field(default_factory=lambda: ["trend", "seasonality", "generic"])
    nb_blocks_per_stack: int = 3
    hidden_layer_units: int = 512
    use_revin: bool = True

    # Training configuration
    epochs: int = 150
    batch_size: int = 128
    learning_rate: float = 1e-3
    gradient_clip_norm: float = 1.0
    optimizer: str = 'adamw'
    primary_loss: str = "mae"

    # Regularization
    kernel_regularizer_l2: float = 1e-4
    dropout_rate: float = 0.15

    # Task selection and balancing
    max_tasks_per_category: int = 5
    min_data_length: int = 1000
    balance_tasks: bool = True
    samples_per_task: int = 2000

    # Interim visualization configuration
    visualize_every_n_epochs: int = 5
    save_interim_plots: bool = True
    plot_top_k_tasks: int = 6
    create_learning_curves: bool = True
    create_prediction_plots: bool = True
    create_task_performance_heatmap: bool = True

    # Evaluation configuration
    eval_during_training: bool = True
    eval_every_n_epochs: int = 10

    def __post_init__(self) -> None:
        """Enhanced validation for multi-task configuration with task inference."""
        total_ratio = self.train_ratio + self.val_ratio + self.test_ratio
        if not np.isclose(total_ratio, 1.0, atol=1e-6):
            raise ValueError(f"Data ratios must sum to 1.0, got {total_ratio}")

        if self.backcast_length <= 0 or self.forecast_length <= 0:
            raise ValueError("backcast_length and forecast_length must be positive")

        if self.use_curriculum_learning:
            if not (0.0 <= self.curriculum_end_ratio <= self.curriculum_start_ratio <= 1.0):
                raise ValueError("Invalid curriculum learning ratios")

        ratio = self.backcast_length / self.forecast_length
        if ratio < 3.0:
            logger.warning(f"Consider increasing backcast_length for better performance (current ratio: {ratio:.1f})")

        logger.info(f"Multi-Task N-BEATS Configuration with Trainable Task Inference:")
        logger.info(f"  âœ“ Multi-task learning: ENABLED")
        logger.info(f"  âœ“ Task embeddings: {'ENABLED' if self.use_task_embeddings else 'DISABLED'}")
        logger.info(f"  âœ“ Trainable task inference: {'ENABLED' if self.train_task_inference else 'DISABLED'}")
        logger.info(f"  âœ“ Curriculum learning: {'ENABLED' if self.use_curriculum_learning else 'DISABLED'}")
        logger.info(f"  âœ“ Interim visualizations every {self.visualize_every_n_epochs} epochs")
        logger.info(f"  âœ“ Task balancing: {'ENABLED' if self.balance_tasks else 'DISABLED'}")


# ---------------------------------------------------------------------
# Multi-Task Data Processor with Unlabeled Support
# ---------------------------------------------------------------------

class MultiTaskDataProcessor:
    """Data processor for multi-task N-BEATS training with unlabeled data support."""

    def __init__(self, config: MultiTaskNBeatsConfig):
        self.config = config
        self.scalers: Dict[str, TimeSeriesNormalizer] = {}
        self.task_to_id: Dict[str, int] = {}
        self.id_to_task: Dict[int, str] = {}

    def prepare_multi_task_data(
            self,
            raw_task_data: Dict[str, np.ndarray]
    ) -> Dict[str, Dict[int, Tuple[np.ndarray, np.ndarray, np.ndarray]]]:
        """Prepare combined multi-task training data with support for unlabeled data."""

        logger.info("Preparing multi-task N-BEATS data with unlabeled support...")

        # Create task ID mapping
        self.task_to_id = {task: idx for idx, task in enumerate(raw_task_data.keys())}
        self.id_to_task = {idx: task for task, idx in self.task_to_id.items()}

        # Fit scalers for each task
        self._fit_scalers(raw_task_data)

        # Prepare data for each horizon
        prepared_data = {}

        for horizon in self.config.forecast_horizons:
            logger.info(f"Preparing data for horizon {horizon}")

            # Collect all sequences from all tasks
            all_train_X, all_train_y, all_train_task_ids = [], [], []
            all_val_X, all_val_y, all_val_task_ids = [], [], []
            all_test_X, all_test_y, all_test_task_ids = [], [], []

            # Also collect unlabeled data (for task inference training)
            all_unlabeled_X, all_unlabeled_y = [], []

            for task_name, data in raw_task_data.items():
                task_id = self.task_to_id[task_name]

                try:
                    # Check data sufficiency
                    min_length = self.config.backcast_length + horizon + 100
                    if len(data) < min_length:
                        logger.warning(f"Insufficient data for {task_name} H={horizon}")
                        continue

                    # Split data
                    train_size = int(self.config.train_ratio * len(data))
                    val_size = int(self.config.val_ratio * len(data))

                    train_data = data[:train_size]
                    val_data = data[train_size:train_size + val_size]
                    test_data = data[train_size + val_size:]

                    # Transform data
                    train_scaled = self.scalers[task_name].transform(train_data)
                    val_scaled = self.scalers[task_name].transform(val_data)
                    test_scaled = self.scalers[task_name].transform(test_data)

                    # Create sequences
                    train_X, train_y = self._create_sequences(train_scaled, horizon)
                    val_X, val_y = self._create_sequences(val_scaled, horizon, stride=horizon // 2)
                    test_X, test_y = self._create_sequences(test_scaled, horizon, stride=horizon // 2)

                    # Balance data if requested
                    if self.config.balance_tasks:
                        max_samples = self.config.samples_per_task
                        if len(train_X) > max_samples:
                            indices = np.random.choice(len(train_X), max_samples, replace=False)
                            train_X = train_X[indices]
                            train_y = train_y[indices]

                    # Add to labeled dataset
                    all_train_X.append(train_X)
                    all_train_y.append(train_y)
                    all_train_task_ids.append(np.full(len(train_X), task_id))

                    all_val_X.append(val_X)
                    all_val_y.append(val_y)
                    all_val_task_ids.append(np.full(len(val_X), task_id))

                    all_test_X.append(test_X)
                    all_test_y.append(test_y)
                    all_test_task_ids.append(np.full(len(test_X), task_id))

                    # Add to unlabeled dataset (for task inference training)
                    # Use a subset for computational efficiency
                    unlabeled_indices = np.random.choice(len(train_X), min(len(train_X), 500), replace=False)
                    all_unlabeled_X.append(train_X[unlabeled_indices])
                    all_unlabeled_y.append(train_y[unlabeled_indices])

                    logger.info(
                        f"Added {task_name}: train={len(train_X)}, val={len(val_X)}, test={len(test_X)}, unlabeled={len(train_X[unlabeled_indices])}")

                except Exception as e:
                    logger.warning(f"Failed to prepare {task_name} H={horizon}: {e}")
                    continue

            if not all_train_X:
                logger.error(f"No data prepared for horizon {horizon}")
                continue

            # Combine all tasks (labeled)
            combined_train_X = np.concatenate(all_train_X, axis=0)
            combined_train_y = np.concatenate(all_train_y, axis=0)
            combined_train_task_ids = np.concatenate(all_train_task_ids, axis=0)

            combined_val_X = np.concatenate(all_val_X, axis=0)
            combined_val_y = np.concatenate(all_val_y, axis=0)
            combined_val_task_ids = np.concatenate(all_val_task_ids, axis=0)

            combined_test_X = np.concatenate(all_test_X, axis=0)
            combined_test_y = np.concatenate(all_test_y, axis=0)
            combined_test_task_ids = np.concatenate(all_test_task_ids, axis=0)

            # Combine unlabeled data
            combined_unlabeled_X = np.concatenate(all_unlabeled_X, axis=0) if all_unlabeled_X else np.array([])
            combined_unlabeled_y = np.concatenate(all_unlabeled_y, axis=0) if all_unlabeled_y else np.array([])

            # Shuffle the combined training data
            train_indices = np.random.permutation(len(combined_train_X))
            combined_train_X = combined_train_X[train_indices]
            combined_train_y = combined_train_y[train_indices]
            combined_train_task_ids = combined_train_task_ids[train_indices]

            # Shuffle unlabeled data
            if len(combined_unlabeled_X) > 0:
                unlabeled_indices = np.random.permutation(len(combined_unlabeled_X))
                combined_unlabeled_X = combined_unlabeled_X[unlabeled_indices]
                combined_unlabeled_y = combined_unlabeled_y[unlabeled_indices]

            prepared_data[horizon] = {
                'labeled': (
                    (combined_train_X, combined_train_y, combined_train_task_ids),
                    (combined_val_X, combined_val_y, combined_val_task_ids),
                    (combined_test_X, combined_test_y, combined_test_task_ids)
                ),
                'unlabeled': (combined_unlabeled_X, combined_unlabeled_y) if len(combined_unlabeled_X) > 0 else None
            }

            logger.info(
                f"Combined H={horizon} data: "
                f"labeled_train={len(combined_train_X)}, val={len(combined_val_X)}, test={len(combined_test_X)}, "
                f"unlabeled={len(combined_unlabeled_X) if len(combined_unlabeled_X) > 0 else 0}"
            )

        return prepared_data

    def _fit_scalers(self, task_data: Dict[str, np.ndarray]) -> None:
        """Fit scalers for each task."""
        for task_name, data in task_data.items():
            if len(data) < self.config.min_data_length:
                continue

            scaler = TimeSeriesNormalizer(method='standard')
            train_size = int(self.config.train_ratio * len(data))
            train_data = data[:train_size]

            scaler.fit(train_data)
            self.scalers[task_name] = scaler

            logger.info(f"Fitted scaler for {task_name}")

    def _create_sequences(
            self,
            data: np.ndarray,
            forecast_length: int,
            stride: int = 1
    ) -> Tuple[np.ndarray, np.ndarray]:
        """Create sequences for training."""
        X, y = [], []

        for i in range(0, len(data) - self.config.backcast_length - forecast_length + 1, stride):
            backcast = data[i: i + self.config.backcast_length]
            forecast = data[i + self.config.backcast_length: i + self.config.backcast_length + forecast_length]

            if not (np.isnan(backcast).any() or np.isnan(forecast).any()):
                X.append(backcast)
                y.append(forecast)

        return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)


# ---------------------------------------------------------------------
# Curriculum Learning Callback
# ---------------------------------------------------------------------

class CurriculumLearningCallback(keras.callbacks.Callback):
    """Callback for curriculum learning with task inference."""

    def __init__(
            self,
            config: MultiTaskNBeatsTrainingConfig,
            labeled_data: Tuple[np.ndarray, np.ndarray, np.ndarray],
            unlabeled_data: Optional[Tuple[np.ndarray, np.ndarray]] = None
    ):
        super().__init__()
        self.config = config
        self.labeled_X, self.labeled_y, self.labeled_task_ids = labeled_data
        self.unlabeled_X, self.unlabeled_y = unlabeled_data if unlabeled_data else (None, None)

        self.current_labeled_ratio = config.curriculum_start_ratio
        self.total_samples = len(self.labeled_X)
        if self.unlabeled_X is not None:
            self.total_samples += len(self.unlabeled_X)

    def on_epoch_begin(self, epoch, logs=None):
        """Update the labeled data ratio based on curriculum learning schedule."""
        if not self.config.use_curriculum_learning:
            return

        # Calculate current ratio
        if epoch < self.config.curriculum_transition_epochs:
            # Linear transition from start_ratio to end_ratio
            progress = epoch / self.config.curriculum_transition_epochs
            self.current_labeled_ratio = (
                    self.config.curriculum_start_ratio * (1 - progress) +
                    self.config.curriculum_end_ratio * progress
            )
        else:
            self.current_labeled_ratio = self.config.curriculum_end_ratio

        # Calculate number of labeled samples to use
        n_labeled = int(len(self.labeled_X) * self.current_labeled_ratio)

        if epoch % 10 == 0:  # Log every 10 epochs
            logger.info(
                f"Epoch {epoch}: Using {self.current_labeled_ratio:.2f} labeled ratio ({n_labeled}/{len(self.labeled_X)} samples)")

        # Update model's training data for this epoch
        # This would require modifying the training loop to use this callback
        # For now, we'll just track the ratio


# ---------------------------------------------------------------------
# Enhanced Interim Visualization Callback
# ---------------------------------------------------------------------

class EnhancedInterimVisualizationCallback(keras.callbacks.Callback):
    """Enhanced callback for creating interim visualizations with task inference monitoring."""

    def __init__(
            self,
            config: MultiTaskNBeatsTrainingConfig,
            data_processor: MultiTaskDataProcessor,
            test_data: Dict[int, Tuple],
            unlabeled_data: Dict[int, Tuple],
            save_dir: str
    ):
        super().__init__()
        self.config = config
        self.data_processor = data_processor
        self.test_data = test_data
        self.unlabeled_data = unlabeled_data
        self.save_dir = save_dir
        self.training_history = {
            'epoch': [],
            'loss': [],
            'val_loss': [],
            'primary_loss': [],
            'aux_entropy_loss': [],
            'aux_consistency_loss': [],
            'aux_balance_loss': [],
            'task_losses': {task: [] for task in data_processor.task_to_id.keys()}
        }

        os.makedirs(save_dir, exist_ok=True)

        # Select representative tasks for visualization
        self.viz_tasks = list(data_processor.task_to_id.keys())[:config.plot_top_k_tasks]

    def on_epoch_end(self, epoch, logs=None):
        """Create enhanced visualizations at specified intervals."""
        if logs is None:
            logs = {}

        # Store training history
        self.training_history['epoch'].append(epoch)
        self.training_history['loss'].append(logs.get('loss', 0))
        self.training_history['val_loss'].append(logs.get('val_loss', 0))
        self.training_history['primary_loss'].append(logs.get('primary_loss', 0))
        self.training_history['aux_entropy_loss'].append(logs.get('aux_entropy_loss', 0))
        self.training_history['aux_consistency_loss'].append(logs.get('aux_consistency_loss', 0))
        self.training_history['aux_balance_loss'].append(logs.get('aux_balance_loss', 0))

        # Create visualizations
        if (epoch + 1) % self.config.visualize_every_n_epochs == 0:
            logger.info(f"Creating enhanced interim visualizations at epoch {epoch + 1}")
            self._create_enhanced_interim_plots(epoch)

    def _create_enhanced_interim_plots(self, epoch: int):
        """Create comprehensive interim plots with task inference monitoring."""
        try:
            # 1. Enhanced learning curves
            if self.config.create_learning_curves:
                self._plot_enhanced_learning_curves(epoch)

            # 2. Task inference analysis
            self._plot_task_inference_analysis(epoch)

            # 3. Prediction samples with task inference
            if self.config.create_prediction_plots:
                self._plot_prediction_samples_with_inference(epoch)

            # 4. Task performance heatmap
            if self.config.create_task_performance_heatmap:
                self._plot_enhanced_task_performance(epoch)

            logger.info(f"Enhanced interim plots saved for epoch {epoch + 1}")

        except Exception as e:
            logger.warning(f"Failed to create enhanced interim plots: {e}")

    def _plot_enhanced_learning_curves(self, epoch: int):
        """Plot enhanced training and validation loss curves with auxiliary losses."""
        fig, axes = plt.subplots(2, 2, figsize=(18, 12))

        epochs = self.training_history['epoch']

        # Main loss curves
        axes[0, 0].plot(epochs, self.training_history['loss'], label='Total Loss', color='blue', linewidth=2)
        axes[0, 0].plot(epochs, self.training_history['val_loss'], label='Validation Loss', color='red', linewidth=2)
        axes[0, 0].plot(epochs, self.training_history['primary_loss'], label='Primary Loss', color='green', linewidth=1,
                        linestyle='--')
        axes[0, 0].set_title(f'Learning Curves (Epoch {epoch + 1})')
        axes[0, 0].set_xlabel('Epoch')
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # Auxiliary losses
        if self.training_history['aux_entropy_loss']:
            axes[0, 1].plot(epochs, self.training_history['aux_entropy_loss'], label='Entropy Loss', color='purple')
            axes[0, 1].plot(epochs, self.training_history['aux_consistency_loss'], label='Consistency Loss',
                            color='orange')
            axes[0, 1].plot(epochs, self.training_history['aux_balance_loss'], label='Balance Loss', color='brown')
            axes[0, 1].set_title('Auxiliary Losses (Task Inference)')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('Loss')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)

        # Learning rate
        try:
            if hasattr(self.model.optimizer.learning_rate, 'numpy'):
                current_lr = float(self.model.optimizer.learning_rate.numpy())
            else:
                current_lr = float(self.model.optimizer.learning_rate)

            axes[1, 0].axhline(y=current_lr, color='green', linestyle='--', label=f'Current LR: {current_lr:.2e}')
            axes[1, 0].set_title('Learning Rate')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Learning Rate')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
        except Exception:
            axes[1, 0].text(0.5, 0.5, 'LR info unavailable', ha='center', va='center', transform=axes[1, 0].transAxes)

        # Loss decomposition
        if len(epochs) > 1:
            primary_ratio = np.array(self.training_history['primary_loss']) / np.array(self.training_history['loss'])
            axes[1, 1].plot(epochs, primary_ratio, label='Primary Loss Ratio', color='blue')
            axes[1, 1].set_title('Loss Decomposition')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('Primary Loss / Total Loss')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(os.path.join(self.save_dir, f'enhanced_learning_curves_epoch_{epoch + 1:03d}.png'),
                    dpi=150, bbox_inches='tight')
        plt.close()

    def _plot_task_inference_analysis(self, epoch: int):
        """Plot task inference analysis."""
        horizon = self.config.forecast_horizons[0]

        if horizon not in self.unlabeled_data:
            return

        unlabeled_X, unlabeled_y = self.unlabeled_data[horizon]

        if len(unlabeled_X) == 0:
            return

        # Sample some unlabeled data for analysis
        sample_size = min(100, len(unlabeled_X))
        sample_indices = np.random.choice(len(unlabeled_X), sample_size, replace=False)
        sample_X = unlabeled_X[sample_indices]

        # Get task probabilities
        task_probs = self.model._infer_task_probabilities(sample_X, training=False)
        task_probs_np = keras.ops.convert_to_numpy(task_probs)

        # Create visualization
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # Task probability distribution
        axes[0, 0].hist(np.max(task_probs_np, axis=1), bins=20, alpha=0.7, color='skyblue')
        axes[0, 0].set_title('Max Task Probability Distribution')
        axes[0, 0].set_xlabel('Max Probability')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].grid(True, alpha=0.3)

        # Task assignment distribution
        predicted_tasks = np.argmax(task_probs_np, axis=1)
        task_counts = np.bincount(predicted_tasks, minlength=self.model.num_tasks)
        task_names = [self.data_processor.id_to_task[i] for i in range(self.model.num_tasks)]

        axes[0, 1].bar(range(self.model.num_tasks), task_counts, color='lightcoral', alpha=0.7)
        axes[0, 1].set_title('Predicted Task Distribution')
        axes[0, 1].set_xlabel('Task ID')
        axes[0, 1].set_ylabel('Count')
        axes[0, 1].set_xticks(range(self.model.num_tasks))
        axes[0, 1].set_xticklabels([f'T{i}' for i in range(self.model.num_tasks)], rotation=45)
        axes[0, 1].grid(True, alpha=0.3)

        # Task probability heatmap
        im = axes[1, 0].imshow(task_probs_np[:min(50, len(task_probs_np))].T,
                               cmap='viridis', aspect='auto')
        axes[1, 0].set_title('Task Probabilities Heatmap (First 50 samples)')
        axes[1, 0].set_xlabel('Sample')
        axes[1, 0].set_ylabel('Task ID')
        axes[1, 0].set_yticks(range(self.model.num_tasks))
        axes[1, 0].set_yticklabels([f'T{i}' for i in range(self.model.num_tasks)])
        plt.colorbar(im, ax=axes[1, 0])

        # Entropy distribution
        entropy = -np.sum(task_probs_np * np.log(task_probs_np + 1e-8), axis=1)
        axes[1, 1].hist(entropy, bins=20, alpha=0.7, color='lightgreen')
        axes[1, 1].axvline(x=self.config.min_entropy_target, color='red', linestyle='--',
                           label=f'Target: {self.config.min_entropy_target}')
        axes[1, 1].set_title('Task Prediction Entropy')
        axes[1, 1].set_xlabel('Entropy')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)

        plt.suptitle(f'Task Inference Analysis (Epoch {epoch + 1})', fontsize=16)
        plt.tight_layout()
        plt.savefig(os.path.join(self.save_dir, f'task_inference_analysis_epoch_{epoch + 1:03d}.png'),
                    dpi=150, bbox_inches='tight')
        plt.close()

    def _plot_prediction_samples_with_inference(self, epoch: int):
        """Plot prediction samples with task inference information."""
        horizon = self.config.forecast_horizons[0]

        # Get labeled test data
        test_X, test_y, test_task_ids = self.test_data[horizon]
        forecast_length = test_y.shape[1]

        # Also get some unlabeled data for comparison
        if horizon in self.unlabeled_data and len(self.unlabeled_data[horizon][0]) > 0:
            unlabeled_X, unlabeled_y = self.unlabeled_data[horizon]
        else:
            unlabeled_X, unlabeled_y = None, None

        fig, axes = plt.subplots(3, 3, figsize=(20, 15))
        axes = axes.flatten()

        plot_idx = 0

        # Plot labeled samples (first 6)
        for i, task_name in enumerate(self.viz_tasks[:6]):
            if plot_idx >= len(axes):
                break

            task_id = self.data_processor.task_to_id[task_name]
            task_mask = test_task_ids == task_id

            if not np.any(task_mask):
                continue

            # Get sample for this task
            task_X = test_X[task_mask]
            task_y = test_y[task_mask]
            task_ids = test_task_ids[task_mask]

            if len(task_X) == 0:
                continue

            # Take first sample
            sample_X = task_X[0:1]
            sample_y = task_y[0:1]
            sample_task_id = task_ids[0:1]

            # Get prediction with explicit task ID
            pred_y_labeled = self.model((sample_X, sample_task_id), training=False)

            # Get prediction with task inference
            pred_y_inferred = self.model(sample_X, training=False)

            # Plot
            backcast_x = np.arange(-self.config.backcast_length, 0)
            forecast_x = np.arange(0, forecast_length)

            axes[plot_idx].plot(backcast_x, sample_X[0], label='Input', color='blue', alpha=0.7)
            axes[plot_idx].plot(forecast_x, sample_y[0].flatten(), label='True', color='green', linewidth=2)
            axes[plot_idx].plot(forecast_x, pred_y_labeled[0].numpy().flatten(),
                                label='Pred (Labeled)', color='red', linestyle='-', linewidth=2)
            axes[plot_idx].plot(forecast_x, pred_y_inferred[0].numpy().flatten(),
                                label='Pred (Inferred)', color='orange', linestyle='--', linewidth=2)
            axes[plot_idx].set_title(f'{task_name} (Labeled)')
            axes[plot_idx].legend()
            axes[plot_idx].grid(True, alpha=0.3)
            axes[plot_idx].axvline(x=0, color='black', linestyle='-', alpha=0.5)

            plot_idx += 1

        # Plot unlabeled samples (if available)
        if unlabeled_X is not None and len(unlabeled_X) > 0:
            for i in range(min(3, len(unlabeled_X))):
                if plot_idx >= len(axes):
                    break

                sample_X = unlabeled_X[i:i + 1]
                sample_y = unlabeled_y[i:i + 1]

                # Get prediction with task inference only
                pred_y_inferred = self.model(sample_X, training=False)

                # Get task probabilities
                task_probs = self.model._infer_task_probabilities(sample_X, training=False)
                task_probs_np = keras.ops.convert_to_numpy(task_probs)[0]
                predicted_task_id = np.argmax(task_probs_np)
                confidence = np.max(task_probs_np)

                # Plot
                backcast_x = np.arange(-self.config.backcast_length, 0)
                forecast_x = np.arange(0, forecast_length)

                axes[plot_idx].plot(backcast_x, sample_X[0], label='Input', color='blue', alpha=0.7)
                axes[plot_idx].plot(forecast_x, sample_y[0].flatten(), label='True', color='green', linewidth=2)
                axes[plot_idx].plot(forecast_x, pred_y_inferred[0].numpy().flatten(),
                                    label='Pred (Inferred)', color='purple', linestyle='--', linewidth=2)

                predicted_task_name = self.data_processor.id_to_task[predicted_task_id]
                axes[plot_idx].set_title(f'Unlabeled (Inferred: {predicted_task_name}, Conf: {confidence:.2f})')
                axes[plot_idx].legend()
                axes[plot_idx].grid(True, alpha=0.3)
                axes[plot_idx].axvline(x=0, color='black', linestyle='-', alpha=0.5)

                plot_idx += 1

        # Remove empty subplots
        for i in range(plot_idx, len(axes)):
            fig.delaxes(axes[i])

        plt.suptitle(f'Prediction Samples with Task Inference (Epoch {epoch + 1})', fontsize=16)
        plt.tight_layout()
        plt.savefig(os.path.join(self.save_dir, f'predictions_with_inference_epoch_{epoch + 1:03d}.png'),
                    dpi=150, bbox_inches='tight')
        plt.close()

    def _plot_enhanced_task_performance(self, epoch: int):
        """Plot enhanced task-specific performance analysis."""
        horizon = self.config.forecast_horizons[0]
        test_X, test_y, test_task_ids = self.test_data[horizon]

        # Calculate performance metrics for each task
        task_metrics = {
            'task_names': [],
            'labeled_mae': [],
            'inferred_mae': [],
            'mae_difference': [],
            'confidence': []
        }

        for task_name, task_id in self.data_processor.task_to_id.items():
            task_mask = test_task_ids == task_id

            if not np.any(task_mask):
                continue

            task_X = test_X[task_mask][:50]  # Limit to 50 samples for speed
            task_y = test_y[task_mask][:50]
            task_ids = np.full(len(task_X), task_id)

            if len(task_X) == 0:
                continue

            # Get predictions with explicit task IDs
            pred_y_labeled = self.model((task_X, task_ids), training=False)

            # Get predictions with task inference
            pred_y_inferred = self.model(task_X, training=False)

            # Get task inference confidence
            task_probs = self.model._infer_task_probabilities(task_X, training=False)
            task_probs_np = keras.ops.convert_to_numpy(task_probs)
            avg_confidence = np.mean(np.max(task_probs_np, axis=1))

            # Calculate MAE
            labeled_mae = np.mean(np.abs(task_y - pred_y_labeled.numpy()))
            inferred_mae = np.mean(np.abs(task_y - pred_y_inferred.numpy()))
            mae_diff = inferred_mae - labeled_mae

            task_metrics['task_names'].append(task_name)
            task_metrics['labeled_mae'].append(labeled_mae)
            task_metrics['inferred_mae'].append(inferred_mae)
            task_metrics['mae_difference'].append(mae_diff)
            task_metrics['confidence'].append(avg_confidence)

        if task_metrics['task_names']:
            # Create enhanced performance visualization
            fig, axes = plt.subplots(2, 2, figsize=(16, 12))

            # MAE comparison
            x = np.arange(len(task_metrics['task_names']))
            width = 0.35

            axes[0, 0].bar(x - width / 2, task_metrics['labeled_mae'], width,
                           label='Labeled MAE', alpha=0.8, color='blue')
            axes[0, 0].bar(x + width / 2, task_metrics['inferred_mae'], width,
                           label='Inferred MAE', alpha=0.8, color='red')
            axes[0, 0].set_title('MAE Comparison: Labeled vs Inferred')
            axes[0, 0].set_xlabel('Task')
            axes[0, 0].set_ylabel('MAE')
            axes[0, 0].set_xticks(x)
            axes[0, 0].set_xticklabels(task_metrics['task_names'], rotation=45)
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)

            # MAE difference
            colors = ['green' if diff <= 0 else 'red' for diff in task_metrics['mae_difference']]
            axes[0, 1].bar(x, task_metrics['mae_difference'], color=colors, alpha=0.7)
            axes[0, 1].set_title('MAE Difference (Inferred - Labeled)')
            axes[0, 1].set_xlabel('Task')
            axes[0, 1].set_ylabel('MAE Difference')
            axes[0, 1].set_xticks(x)
            axes[0, 1].set_xticklabels(task_metrics['task_names'], rotation=45)
            axes[0, 1].axhline(y=0, color='black', linestyle='-', alpha=0.5)
            axes[0, 1].grid(True, alpha=0.3)

            # Task inference confidence
            axes[1, 0].bar(x, task_metrics['confidence'], color='purple', alpha=0.7)
            axes[1, 0].set_title('Average Task Inference Confidence')
            axes[1, 0].set_xlabel('Task')
            axes[1, 0].set_ylabel('Confidence')
            axes[1, 0].set_xticks(x)
            axes[1, 0].set_xticklabels(task_metrics['task_names'], rotation=45)
            axes[1, 0].grid(True, alpha=0.3)

            # Confidence vs Performance scatter
            axes[1, 1].scatter(task_metrics['confidence'], task_metrics['mae_difference'],
                               c=task_metrics['inferred_mae'], cmap='viridis', s=100, alpha=0.7)
            axes[1, 1].set_title('Confidence vs Performance')
            axes[1, 1].set_xlabel('Inference Confidence')
            axes[1, 1].set_ylabel('MAE Difference')
            axes[1, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)
            axes[1, 1].grid(True, alpha=0.3)

            # Add colorbar
            scatter = axes[1, 1].scatter(task_metrics['confidence'], task_metrics['mae_difference'],
                                         c=task_metrics['inferred_mae'], cmap='viridis', s=100, alpha=0.7)
            plt.colorbar(scatter, ax=axes[1, 1], label='Inferred MAE')

            plt.suptitle(f'Enhanced Task Performance Analysis (Epoch {epoch + 1})', fontsize=16)
            plt.tight_layout()
            plt.savefig(os.path.join(self.save_dir, f'enhanced_task_performance_epoch_{epoch + 1:03d}.png'),
                        dpi=150, bbox_inches='tight')
            plt.close()


# ---------------------------------------------------------------------
# Enhanced Multi-Task Trainer
# ---------------------------------------------------------------------

class EnhancedMultiTaskNBeatsTrainer:
    """Enhanced multi-task N-BEATS trainer with trainable task inference."""

    def __init__(self, config: MultiTaskNBeatsTrainingConfig, ts_config: TimeSeriesConfig):
        self.config = config
        self.ts_config = ts_config
        self.generator = TimeSeriesGenerator(ts_config)
        self.processor = MultiTaskDataProcessor(config)

        # Get all available tasks
        self.all_tasks = self.generator.get_task_names()
        self.task_categories = self.generator.get_task_categories()

        # Select tasks for training
        self.selected_tasks = self._select_tasks()

        logger.info(f"Enhanced Multi-Task N-BEATS Trainer with Trainable Task Inference:")
        logger.info(f"  - Total available tasks: {len(self.all_tasks)}")
        logger.info(f"  - Selected tasks: {len(self.selected_tasks)}")
        logger.info(f"  - Multi-task learning: âœ“ ENABLED")
        logger.info(f"  - Trainable task inference: âœ“ ENABLED")
        logger.info(f"  - Curriculum learning: {'âœ“ ENABLED' if config.use_curriculum_learning else 'âœ— DISABLED'}")
        logger.info(f"  - Enhanced visualizations: âœ“ ENABLED")

    def _select_tasks(self) -> List[str]:
        """Select tasks for multi-task training."""
        selected = []

        for category in self.task_categories:
            category_tasks = self.generator.get_tasks_by_category(category)
            selected.extend(category_tasks[:self.config.max_tasks_per_category])

        logger.info(f"Selected {len(selected)} tasks from {len(self.task_categories)} categories")
        return selected

    def prepare_data(self) -> Dict[str, Any]:
        """Prepare multi-task training data with unlabeled support."""
        logger.info("Generating raw data for all selected tasks...")

        # Generate raw data for all selected tasks
        raw_task_data = {}
        failed_tasks = []

        for task_name in self.selected_tasks:
            try:
                data = self.generator.generate_task_data(task_name)
                if len(data) >= self.config.min_data_length:
                    raw_task_data[task_name] = data
                else:
                    failed_tasks.append(f"{task_name} (too short)")
            except Exception as e:
                logger.warning(f"Failed to generate {task_name}: {e}")
                failed_tasks.append(f"{task_name} (error)")

        if failed_tasks:
            logger.warning(f"Failed tasks: {', '.join(failed_tasks)}")

        logger.info(f"Successfully generated data for {len(raw_task_data)} tasks")

        # Prepare multi-task data
        prepared_data = self.processor.prepare_multi_task_data(raw_task_data)

        return {
            'prepared_data': prepared_data,
            'raw_task_data': raw_task_data,
            'num_tasks': len(raw_task_data),
            'task_to_id': self.processor.task_to_id
        }

    def create_model(self, num_tasks: int, task_to_id: Dict[str, int], forecast_length: int) -> MultiTaskNBeatsNet:
        """Create enhanced multi-task N-BEATS model with trainable task inference."""
        logger.info(
            f"Creating Enhanced Multi-Task N-BEATS model with trainable task inference for horizon {forecast_length}...")

        try:
            # Create enhanced model configuration
            model_config = MultiTaskNBeatsConfig(
                # Architecture parameters
                backcast_length=self.config.backcast_length,
                use_task_embeddings=self.config.use_task_embeddings,
                task_embedding_dim=self.config.task_embedding_dim,
                use_task_inference=True,  # Always enable for enhanced model

                # NEW: Task inference training parameters
                train_task_inference=self.config.train_task_inference,
                task_inference_loss_weight=self.config.task_inference_loss_weight,
                consistency_loss_weight=self.config.consistency_loss_weight,
                entropy_loss_weight=self.config.entropy_loss_weight,
                consistency_temperature=self.config.consistency_temperature,
                min_entropy_target=self.config.min_entropy_target,

                stack_types=self.config.stack_types.copy(),
                nb_blocks_per_stack=self.config.nb_blocks_per_stack,
                hidden_layer_units=self.config.hidden_layer_units,
                use_revin=self.config.use_revin,

                # Regularization parameters
                dropout_rate=self.config.dropout_rate,
                kernel_regularizer_l2=self.config.kernel_regularizer_l2,

                # Training parameters
                optimizer=self.config.optimizer,
                primary_loss=self.config.primary_loss,
                learning_rate=self.config.learning_rate,
                gradient_clip_norm=self.config.gradient_clip_norm
            )

            logger.info("âœ“ Enhanced model configuration created")
            logger.info(f"  - Trainable task inference: âœ“ ENABLED")
            logger.info(f"  - Task inference loss weight: {model_config.task_inference_loss_weight}")
            logger.info(f"  - Consistency loss weight: {model_config.consistency_loss_weight}")
            logger.info(f"  - Entropy loss weight: {model_config.entropy_loss_weight}")

            # Create the model
            model = create_multi_task_nbeats(
                config=model_config,
                num_tasks=num_tasks,
                task_to_id=task_to_id,
                forecast_length=forecast_length,
                name=f"EnhancedMultiTaskNBeats_H{forecast_length}"
            )

            logger.info("âœ“ Enhanced Multi-Task N-BEATS model created successfully")
            return model

        except Exception as e:
            logger.error(f"Failed to create Enhanced Multi-Task N-BEATS model: {e}")
            raise RuntimeError(f"Enhanced model creation failed: {e}") from e

    def run_experiment(self) -> Dict[str, Any]:
        """Run the complete enhanced multi-task N-BEATS experiment."""

        try:
            # Create experiment directory
            exp_dir = os.path.join(
                self.config.result_dir,
                f"{self.config.experiment_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            )
            os.makedirs(exp_dir, exist_ok=True)

            logger.info(f"ðŸš€ Starting Enhanced Multi-Task N-BEATS Experiment: {exp_dir}")

            # Prepare data
            data_info = self.prepare_data()
            prepared_data = data_info['prepared_data']

            if not prepared_data:
                raise ValueError("No data prepared for training")

            # Train for each horizon
            results = {}

            for horizon in self.config.forecast_horizons:
                if horizon not in prepared_data:
                    logger.warning(f"No data available for horizon {horizon}")
                    continue

                logger.info(f"\n{'=' * 70}\nðŸŽ¯ Training Enhanced Multi-Task Model for Horizon {horizon}\n{'=' * 70}")

                # Create model for this specific horizon
                model = self.create_model(data_info['num_tasks'], data_info['task_to_id'], horizon)

                # Get data for this horizon
                labeled_data = prepared_data[horizon]['labeled']
                unlabeled_data = prepared_data[horizon]['unlabeled']

                train_data, val_data, test_data = labeled_data

                # Build model with appropriate input shape
                sample_input = (train_data[0][:1], train_data[2][:1])  # (X, task_ids)
                model(sample_input)  # Build the model

                # Compile the model
                self._compile_enhanced_model(model)

                # Create enhanced callbacks
                viz_dir = os.path.join(exp_dir, f'enhanced_visualizations_h{horizon}')

                unlabeled_dict = {horizon: unlabeled_data} if unlabeled_data is not None else {}

                callbacks = [
                    keras.callbacks.EarlyStopping(
                        monitor='val_loss',
                        patience=30,
                        restore_best_weights=True,
                        verbose=1
                    ),
                    keras.callbacks.ReduceLROnPlateau(
                        monitor='val_loss',
                        factor=0.5,
                        patience=8,
                        min_lr=1e-6,
                        verbose=1
                    ),
                    EnhancedInterimVisualizationCallback(
                        config=self.config,
                        data_processor=self.processor,
                        test_data={horizon: test_data},
                        unlabeled_data=unlabeled_dict,
                        save_dir=viz_dir
                    ),
                    keras.callbacks.ModelCheckpoint(
                        filepath=os.path.join(exp_dir, f'enhanced_best_model_h{horizon}.keras'),
                        monitor='val_loss',
                        save_best_only=True,
                        verbose=1
                    )
                ]

                # Add curriculum learning callback if enabled
                if self.config.use_curriculum_learning:
                    curriculum_callback = CurriculumLearningCallback(
                        config=self.config,
                        labeled_data=train_data,
                        unlabeled_data=unlabeled_data
                    )
                    callbacks.append(curriculum_callback)
                    logger.info("âœ“ Curriculum learning callback added")

                # Train the model with enhanced features
                logger.info("ðŸš€ Starting enhanced training with trainable task inference...")

                start_time = datetime.now()

                # Training with both labeled and unlabeled data
                history = self._enhanced_fit(model, train_data, val_data, callbacks)

                training_time = (datetime.now() - start_time).total_seconds()

                # Evaluate on test set
                test_results = model.evaluate(
                    x=(test_data[0], test_data[2]),
                    y=test_data[1],
                    verbose=0
                )

                # Extract metrics
                if isinstance(test_results, list):
                    test_loss = test_results[0]
                    test_mae = test_results[1] if len(test_results) > 1 else None
                else:
                    test_loss = test_results
                    test_mae = None

                results[horizon] = {
                    'history': history.history,
                    'training_time': training_time,
                    'test_loss': test_loss,
                    'test_mae': test_mae,
                    'final_epoch': len(history.history['loss']),
                    'task_inference_enabled': True,
                    'curriculum_learning_used': self.config.use_curriculum_learning
                }

                logger.info(f"âœ… Enhanced training completed for horizon {horizon}:")
                logger.info(f"   - Training time: {training_time:.1f}s")
                logger.info(f"   - Final epochs: {results[horizon]['final_epoch']}")
                logger.info(f"   - Test loss: {test_loss:.4f}")

            # Save enhanced final results
            self._save_enhanced_final_results(results, exp_dir, data_info)

            logger.info(f"ðŸŽ‰ Enhanced Multi-Task N-BEATS experiment completed successfully!")
            logger.info(f"ðŸ“Š Results directory: {exp_dir}")
            logger.info(f"ðŸ“ˆ Horizons trained: {list(results.keys())}")
            logger.info(f"ðŸ§  Trainable task inference: âœ“ ENABLED")

            return {
                "results_dir": exp_dir,
                "results": results,
                "num_tasks": data_info['num_tasks'],
                "task_mapping": data_info['task_to_id'],
                "enhanced_features": {
                    "trainable_task_inference": True,
                    "curriculum_learning": self.config.use_curriculum_learning,
                    "auxiliary_losses": True
                }
            }

        except Exception as e:
            logger.error(f"ðŸ’¥ Enhanced multi-task experiment failed: {e}", exc_info=True)
            raise

    def _compile_enhanced_model(self, model: MultiTaskNBeatsNet):
        """Compile the enhanced model with appropriate optimizer and loss."""
        # Create optimizer
        if self.config.optimizer.lower() == 'adamw':
            optimizer = keras.optimizers.AdamW(
                learning_rate=self.config.learning_rate,
                clipnorm=self.config.gradient_clip_norm if self.config.gradient_clip_norm > 0 else None
            )
        elif self.config.optimizer.lower() == 'adam':
            optimizer = keras.optimizers.Adam(
                learning_rate=self.config.learning_rate,
                clipnorm=self.config.gradient_clip_norm if self.config.gradient_clip_norm > 0 else None
            )
        else:
            optimizer = keras.optimizers.get(self.config.optimizer)
            optimizer.learning_rate = self.config.learning_rate
            if self.config.gradient_clip_norm > 0:
                optimizer.clipnorm = self.config.gradient_clip_norm

        # Create loss function
        if self.config.primary_loss.lower() == 'mae':
            loss = keras.losses.MeanAbsoluteError()
        elif self.config.primary_loss.lower() == 'mse':
            loss = keras.losses.MeanSquaredError()
        elif self.config.primary_loss.lower() == 'smape':
            loss = SMAPELoss()
        else:
            loss = keras.losses.get(self.config.primary_loss)

        # Compile the model
        model.compile(
            optimizer=optimizer,
            loss=loss,
            metrics=['mae']
        )

        logger.info("âœ… Enhanced model compiled successfully with auxiliary losses")

    def _enhanced_fit(self, model, train_data, val_data, callbacks):
        """Enhanced fitting with mixed labeled/unlabeled training."""
        # For now, use standard fit with labeled data
        # Future enhancement: implement custom training loop with mixed data

        return model.fit(
            x=(train_data[0], train_data[2]),  # (X, task_ids)
            y=train_data[1],  # y
            validation_data=((val_data[0], val_data[2]), val_data[1]),
            epochs=self.config.epochs,
            batch_size=self.config.batch_size,
            callbacks=callbacks,
            verbose=1
        )

    def _save_enhanced_final_results(self, results: Dict, exp_dir: str, data_info: Dict):
        """Save enhanced final experiment results."""
        try:
            # Save enhanced training history
            with open(os.path.join(exp_dir, 'enhanced_training_results.json'), 'w') as f:
                json_results = {}
                for horizon, result in results.items():
                    json_results[str(horizon)] = {
                        'training_time': result['training_time'],
                        'test_loss': float(result['test_loss']),
                        'test_mae': float(result['test_mae']) if result.get('test_mae') is not None else None,
                        'final_epoch': result['final_epoch'],
                        'final_train_loss': result['history']['loss'][-1],
                        'final_val_loss': result['history']['val_loss'][-1],
                        'task_inference_enabled': result.get('task_inference_enabled', False),
                        'curriculum_learning_used': result.get('curriculum_learning_used', False),
                        'final_aux_entropy_loss': result['history'].get('aux_entropy_loss', [0])[-1] if result[
                            'history'].get('aux_entropy_loss') else 0,
                        'final_aux_consistency_loss': result['history'].get('aux_consistency_loss', [0])[-1] if result[
                            'history'].get('aux_consistency_loss') else 0,
                        'final_aux_balance_loss': result['history'].get('aux_balance_loss', [0])[-1] if result[
                            'history'].get('aux_balance_loss') else 0
                    }
                json.dump(json_results, f, indent=2)

            # Save enhanced task information
            with open(os.path.join(exp_dir, 'enhanced_task_info.json'), 'w') as f:
                json.dump({
                    'num_tasks': data_info['num_tasks'],
                    'task_to_id': data_info['task_to_id'],
                    'selected_tasks': self.selected_tasks,
                    'enhanced_features': {
                        'trainable_task_inference': self.config.train_task_inference,
                        'curriculum_learning': self.config.use_curriculum_learning,
                        'task_inference_loss_weight': self.config.task_inference_loss_weight,
                        'consistency_loss_weight': self.config.consistency_loss_weight,
                        'entropy_loss_weight': self.config.entropy_loss_weight
                    }
                }, f, indent=2)

            # Create enhanced final summary plot
            self._create_enhanced_final_summary_plot(results, exp_dir)

            logger.info(f"Enhanced final results saved to {exp_dir}")

        except Exception as e:
            logger.error(f"Failed to save enhanced final results: {e}")

    def _create_enhanced_final_summary_plot(self, results: Dict, exp_dir: str):
        """Create enhanced final summary visualization."""
        try:
            fig, axes = plt.subplots(3, 2, figsize=(18, 15))

            # Training curves for all horizons
            for horizon, result in results.items():
                history = result['history']
                epochs = range(1, len(history['loss']) + 1)

                axes[0, 0].plot(epochs, history['loss'], label=f'H{horizon} Total', alpha=0.8, linewidth=2)
                axes[0, 0].plot(epochs, history.get('primary_loss', history['loss']),
                                label=f'H{horizon} Primary', alpha=0.6, linestyle='--')

                axes[0, 1].plot(epochs, history['val_loss'], label=f'H{horizon} Val', alpha=0.8)

            axes[0, 0].set_title('Training Loss (Total vs Primary)')
            axes[0, 0].set_xlabel('Epoch')
            axes[0, 0].set_ylabel('Loss')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)

            axes[0, 1].set_title('Validation Loss')
            axes[0, 1].set_xlabel('Epoch')
            axes[0, 1].set_ylabel('Loss')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)

            # Auxiliary losses
            for horizon, result in results.items():
                history = result['history']
                epochs = range(1, len(history['loss']) + 1)

                if 'aux_entropy_loss' in history:
                    axes[1, 0].plot(epochs, history['aux_entropy_loss'], label=f'H{horizon} Entropy', alpha=0.8)
                if 'aux_consistency_loss' in history:
                    axes[1, 1].plot(epochs, history['aux_consistency_loss'], label=f'H{horizon} Consistency', alpha=0.8)

            axes[1, 0].set_title('Auxiliary Loss: Entropy')
            axes[1, 0].set_xlabel('Epoch')
            axes[1, 0].set_ylabel('Entropy Loss')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)

            axes[1, 1].set_title('Auxiliary Loss: Consistency')
            axes[1, 1].set_xlabel('Epoch')
            axes[1, 1].set_ylabel('Consistency Loss')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)

            # Performance summary
            horizons = list(results.keys())
            test_losses = [results[h]['test_loss'] for h in horizons]
            training_times = [results[h]['training_time'] for h in horizons]

            axes[2, 0].bar(range(len(horizons)), test_losses, color='skyblue', alpha=0.7)
            axes[2, 0].set_title('Test Loss by Horizon')
            axes[2, 0].set_xlabel('Horizon')
            axes[2, 0].set_ylabel('Test Loss')
            axes[2, 0].set_xticks(range(len(horizons)))
            axes[2, 0].set_xticklabels(horizons)

            axes[2, 1].bar(range(len(horizons)), training_times, color='lightcoral', alpha=0.7)
            axes[2, 1].set_title('Training Time by Horizon')
            axes[2, 1].set_xlabel('Horizon')
            axes[2, 1].set_ylabel('Training Time (s)')
            axes[2, 1].set_xticks(range(len(horizons)))
            axes[2, 1].set_xticklabels(horizons)

            plt.suptitle('Enhanced Multi-Task N-BEATS Training Summary\n(with Trainable Task Inference)', fontsize=16)
            plt.tight_layout()
            plt.savefig(os.path.join(exp_dir, 'enhanced_final_summary.png'), dpi=150, bbox_inches='tight')
            plt.close()

        except Exception as e:
            logger.warning(f"Failed to create enhanced final summary plot: {e}")


# ---------------------------------------------------------------------
# Main Enhanced Experiment Function
# ---------------------------------------------------------------------

def main():
    """Run the enhanced multi-task N-BEATS experiment with trainable task inference."""

    # Enhanced configuration
    config = MultiTaskNBeatsTrainingConfig(
        # Multi-task specific
        backcast_length=168,
        forecast_length=24,
        forecast_horizons=[12],
        use_task_embeddings=True,

        # Trainable Task Inference
        train_task_inference=True,
        task_inference_loss_weight=0.1,
        consistency_loss_weight=0.05,
        entropy_loss_weight=0.02,

        # Curriculum Learning
        use_curriculum_learning=True,
        curriculum_start_ratio=1.0,
        curriculum_end_ratio=0.3,
        curriculum_transition_epochs=50,

        # Enhanced visualizations
        visualize_every_n_epochs=5,
        save_interim_plots=True,
        plot_top_k_tasks=8,
    )

    # Time series configuration
    ts_config = TimeSeriesConfig(
        n_samples=3000,
        random_seed=42,
        default_noise_level=0.01
    )

    logger.info("ðŸš€ Starting ENHANCED Multi-Task N-BEATS Experiment")
    logger.info("=" * 90)
    logger.info("ðŸŽ¯ NEW ENHANCED FEATURES:")
    logger.info("  âœ… Multi-task learning on ALL selected tasks simultaneously")
    logger.info("  âœ… Task embeddings for task-aware predictions")
    logger.info("  ðŸ†• TRAINABLE task inference network (learns without task IDs)")
    logger.info("  ðŸ†• Auxiliary losses: entropy, consistency, balance regularization")
    logger.info("  ðŸ†• Curriculum learning: gradually reduce labeled data dependency")
    logger.info("  ðŸ†• Enhanced visualizations with task inference monitoring")
    logger.info("  âœ… Real-time performance monitoring and comparison")
    logger.info("  âœ… Balanced multi-task dataset")
    logger.info("  âœ… Comprehensive results analysis")
    logger.info("=" * 90)

    try:
        trainer = EnhancedMultiTaskNBeatsTrainer(config, ts_config)
        results = trainer.run_experiment()

        logger.info("ðŸŽ‰ ENHANCED Multi-Task Experiment Completed Successfully!")
        logger.info(f"ðŸ“Š Results directory: {results['results_dir']}")
        logger.info(f"ðŸ“ˆ Number of tasks: {results['num_tasks']}")
        logger.info(f"ðŸŽ¯ Horizons trained: {list(results['results'].keys())}")
        logger.info(f"ðŸ§  Enhanced features:")
        for feature, enabled in results['enhanced_features'].items():
            logger.info(f"   - {feature}: {'âœ…' if enabled else 'âŒ'}")
        logger.info("ðŸ’¡ Check the results directory for:")
        logger.info("  - Enhanced interim visualization plots (every 5 epochs)")
        logger.info("  - Task inference analysis and monitoring")
        logger.info("  - Learning curves with auxiliary losses")
        logger.info("  - Prediction comparisons (labeled vs inferred)")
        logger.info("  - Task performance heatmaps and confidence analysis")
        logger.info("  - Enhanced final summary plots and metrics")

    except Exception as e:
        logger.error(f"ðŸ’¥ Enhanced multi-task experiment failed: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    main()