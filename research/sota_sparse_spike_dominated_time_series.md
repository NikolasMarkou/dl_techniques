# Forecasting the Unforeseen: State-of-the-Art Methods for Sparse, Spike-Dominated Time Series

**The fundamental challenge of sparse, spike-dominated time series—where 50-99% of values are zero with occasional random spikes—requires a complete departure from traditional forecasting methods.** Standard approaches like ARIMA or basic neural networks fail catastrophically because they optimize average error across all time points, effectively learning to predict zeros while missing the critical spike events. Recent research from 2020-2025 has converged on a powerful insight: **the most successful approaches decompose the problem into two distinct sub-tasks**: predicting spike occurrence (a classification problem) and predicting spike magnitude (a regression problem conditioned on occurrence). This two-fold architecture, combined with specialized loss functions like focal loss and quantile regression, achieves 30-50% error reduction compared to traditional methods across domains including intermittent demand forecasting, expense prediction, and rare event modeling.

The stakes are substantial. In retail and manufacturing, 60-80% of SKUs exhibit intermittent patterns, yet traditional forecasts often perform worse than naive "predict all zeros" baselines on standard metrics. Financial expense forecasting faces similar challenges with irregular payment patterns and lumpy transactions. The breakthrough innovations address three core failures of conventional methods: handling extreme class imbalance (many zeros vs few spikes), capturing temporal dependencies despite long zero sequences, and properly evaluating forecast quality when standard metrics like MAPE become undefined or misleading.

## The architecture revolution: decomposition beats end-to-end models

The dominant pattern across successful implementations is **architectural decomposition rather than monolithic prediction**. This represents a fundamental reframing validated across intermittent demand forecasting, rare event prediction, and financial applications. Traditional end-to-end models that directly predict the next value struggle because the training signal from sparse spikes gets overwhelmed by abundant zeros, leading to models that converge to predicting mostly zeros—a local optimum that's difficult to escape.

**Classical statistical decomposition** established this principle decades ago. Croston's method (1972) separates intermittent demand into two independent sequences: demand sizes (non-zero values only) and inter-demand intervals (time between occurrences). Each sequence receives separate exponential smoothing treatment, with the final forecast computed as demand size divided by average interval. This simple separation achieves 15-30% error reduction versus standard exponential smoothing on M5 Competition data with 23,401 intermittent series. The Syntetos-Boylan Approximation (2001) corrected Croston's inherent bias using a (1-α/2) adjustment factor, while the Teunter-Syntetos-Babai method (2011) improved responsiveness by updating demand probability in every period, not just when demand occurs—crucial for detecting obsolescence in declining product lines.

**Modern machine learning implementations** extend this decomposition with sophisticated non-linear models. The recommended architecture uses a binary classifier (typically LightGBM or Random Forest) to predict spike occurrence, trained only on occurrence labels and achieving AUC-ROC scores of 0.85-0.92. A separate regressor (Random Forest, XGBoost, or neural network) predicts magnitude, trained exclusively on non-zero historical values. The final forecast combines them multiplicatively: probability of occurrence × predicted magnitude. This approach consistently outperforms single-stage models by 20-40% on sparse datasets because each component can specialize—the classifier learns temporal patterns of when spikes happen, while the regressor focuses on understanding what drives spike size without being distorted by zeros.

**Deep learning architectures** have adapted this principle through sophisticated mechanisms. The Deep Renewal Processes framework (Türkmen et al., 2021) models inter-demand times and demand sizes as separate probabilistic distributions parameterized by LSTM networks, connecting to temporal point processes for continuous-time modeling. On M5 Competition data with proper hyperparameter optimization, this achieves 10% improvement in median absolute error and 5%+ improvement in 90th percentile quantile loss. The architecture explicitly represents the dual nature of sparse data: a negative binomial distribution for intervals captures clustering and periodicity, while a separate Poisson or negative binomial distribution models size variability.

Zero-inflated models provide another decomposition lens, particularly powerful for financial applications. The zero-inflated Poisson (ZIP) and zero-inflated negative binomial (ZINB) frameworks distinguish **structural zeros** (periods where events fundamentally cannot occur) from **sampling zeros** (periods where events could occur but didn't). The model combines a Bernoulli process (probability π of being in the "always zero" state) with a count distribution for actual events. For financial transaction prediction and cash flow forecasting with 40-70% zero periods, ZIP/ZINB implementations achieve 30-50% error reduction versus standard count models. Recent deep learning variants like DL-ZIACD (2021) use neural networks to parameterize time-varying components, successfully handling financial trade data with extreme zero-inflation.

## Temporal Fusion Transformers and the rise of global models

While decomposition addresses the occurrence-magnitude challenge, **global models trained across multiple related time series** solve the data scarcity problem inherent in sparse data. Individual sparse series contain insufficient information for reliable statistical estimation—a series with 5 non-zero observations in 100 periods provides little signal. Global models pool information across thousands of series simultaneously, learning common patterns like seasonality, promotional effects, and trends while allowing series-specific adjustments through learned embeddings.

The **Temporal Fusion Transformer** (Lim et al., 2019) exemplifies state-of-the-art global modeling. TFT's architecture integrates multiple innovations specifically valuable for sparse data. Variable Selection Networks with Gated Residual units dynamically identify relevant features, crucial when many potential predictors exist but only a subset matter for each series. Static covariate encoders incorporate item metadata (product category, price tier, brand) that enable predictions for new items with zero history—the cold-start problem that plagues expense forecasting for new vendors or project categories. The interpretable multi-head attention mechanism reveals which historical time steps matter most, providing transparency often required in financial applications.

TFT's quantile regression output represents another critical innovation. Rather than point forecasts, TFT minimizes quantile loss across multiple quantiles (typically 0.1, 0.5, 0.9), producing full forecast distributions. For inventory planning with intermittent demand, decision-makers need the 90th percentile (for safety stock) more than the median. Empirically, TFT achieves 7%+ lower quantile loss than DeepAR, ARIMA, and LSTM sequence-to-sequence models. Zalando's 2024 production implementation for retail demand forecasting explicitly handles cold starts, short histories, and sparsity, demonstrating practical viability at scale. The system enforces business constraints (monotonic discount-demand relationships) while training a single global model for both short and long-term horizons.

**DeepAR** (Amazon, 2019) pioneered the global probabilistic forecasting approach, using autoregressive LSTMs to parameterize likelihood distributions—typically negative binomial for count data with overdispersion common in intermittent demand. The key architectural choice is training one model across all series rather than individual per-series models. This **regularization effect prevents overfitting** to sparse individual patterns. When predicting for a sparse series, the model leverages patterns learned from denser related series, acting as implicit transfer learning. Amazon Forecast's implementation claims 45% accuracy improvement for cold-start items versus local methods.

**N-BEATS** (Oreshkin et al., ICLR 2020) offers an alternative that achieves state-of-the-art performance with remarkable simplicity and interpretability. The architecture stacks fully-connected blocks with doubly residual connections—each block produces a "backcast" (reconstruction of input) and "forecast" (future prediction), with residuals passed to the next block. For sparse data, N-BEATS' interpretable variant uses polynomial basis functions for trend and Fourier basis for seasonality, explicitly encoding structural priors that help when data is limited. With just ~1000 parameters, N-BEATS achieves 11% improvement over statistical benchmarks and 3% over M4 competition winners. The sparse connectivity acts as regularization, preventing overfitting—a critical advantage when non-zero observations are scarce. NBEATSx (2021) extends this with exogenous variables, achieving 20% improvement over vanilla N-BEATS when external regressors (promotions, pricing, holidays) are available—common in expense forecasting contexts.

Recent innovations push efficiency further. **SparseTSF** (ICML 2024 Oral, TPAMI 2025) achieves competitive accuracy with under 1,000 parameters through cross-period sparse forecasting. The method downsamples by periodicity, forecasts the simplified series, then upsamples—a form of temporal aggregation embedded in the architecture. For extremely long zero sequences (720+ lookback windows), SparseTSF's lightweight design prevents overfitting while capturing periodic structure. **Dozerformer** (2024) introduces sparse attention combining local windows, strided seasonal patterns, and dynamically selected historical subsets, reducing attention complexity from O(L²) to O(L) while achieving 40.6% lower MSE than previous transformers on multivariate benchmarks.

## Loss functions: making models care about rare events

Standard mean squared error treats all predictions equally, creating a fundamental mismatch for sparse data where rare spikes carry disproportionate importance. With 99% zeros and 1% spikes, MSE-optimized models face overwhelming gradient pressure to predict zeros—correctly predicting 99% of time steps while catastrophically missing every spike. **The solution requires loss functions that explicitly reweight errors** to emphasize spike detection and magnitude estimation.

**Focal loss** (Lin et al., 2017, adapted to time series 2020-2024) provides the gold standard for spike detection. The formulation FL(p_t) = -α_t(1-p_t)^γ log(p_t) down-weights easy examples (confident correct predictions) through the modulating factor (1-p_t)^γ. When a model correctly predicts a zero with 99% confidence, the loss contribution approaches zero. But misclassifying a spike generates high loss. With typical settings (α=0.25, γ=2), focal loss achieves 74% improvement in spike detection F1-score versus MSE on datasets with 90% zeros. The mathematical elegance is that well-classified examples contribute minimally to gradients, allowing the model to focus learning on hard examples—precisely the rare spikes that matter. Empirical comparisons show that in scenarios with 1 million negative and 10 positive examples, focal loss shifts positive class contribution from 0.46% (standard cross-entropy) to 93.74% of total loss.

**Quantile loss** (pinball loss) excels for magnitude estimation with natural asymmetry. The formulation L_τ(y, ŷ) = τ(y-ŷ) for underprediction and (1-τ)(ŷ-y) for overprediction creates differential penalties. With τ=0.9, missing a spike costs 9× more than overestimating. This naturally aligns with expense forecasting where budget shortfalls carry higher consequences than surpluses. Quantile regression doesn't just change the loss—it changes the **fundamental prediction target** from expected value to specific quantiles. For sparse data, predicting the 90th or 95th percentile explicitly targets the spike regime. Multi-quantile approaches predict 50th, 75th, 90th, 95th, and 99th percentiles simultaneously, providing full uncertainty quantification. On sparse time series with over 80% zeros, quantile regression with τ=0.9 achieves 40% better spike detection rate and 25% better magnitude estimation than MSE, with well-calibrated prediction intervals.

**Composite loss functions** combine detection and magnitude objectives. A typical formulation weighs focal loss for occurrence (λ₁=0.6-0.7) and quantile loss for magnitude (λ₂=0.3-0.4). Advanced variants use learnable uncertainty parameters: L = L_cls + exp(-s₁)·L_reg + s₁, where s₁ adapts the relative importance during training. This multi-task learning ensures the model jointly optimizes both aspects. Empirical results show composite approaches achieve 78-81% F1-scores on spike detection with 1.5-1.65 RMSE on magnitude, substantially better than either objective alone.

**HAILS** (2024) introduces distributional hierarchical consistency loss for sparse industrial demand. The innovation recognizes that different series require different likelihood functions: Poisson for sparse series (high zero ratio), Gaussian for dense series, combined with hierarchical constraints ensuring parent forecasts equal sum of children. The distribution-aware loss selects appropriate likelihoods automatically: L_sparse = -log P(y|λ) for Poisson, L_dense = -log N(y|μ,σ²) for Gaussian, plus consistency regularization using Jensen-Shannon divergence between parent and aggregated children distributions. On hierarchies with 10,000+ time series, HAILS achieves 8.5% overall improvement and 23% improvement specifically for sparse series (>30% zeros). This domain-specific innovation demonstrates how loss design can encode structural knowledge about the problem.

Training techniques complement loss functions. **Cost-sensitive learning** assigns higher misclassification costs to minority classes (spike events), typically 10:1 to 100:1 ratios. This directly modifies the learning objective without requiring data manipulation. Implementations simply add `class_weight='balanced'` in scikit-learn or `pos_weight=torch.tensor([10.0])` in PyTorch's BCEWithLogitsLoss. Empirically, cost-sensitive methods achieve 20-35% better F1-scores on minority classes and outperform SMOTE oversampling for extreme imbalance (>99:1) with lower overfitting risk. **SMOTE** (Synthetic Minority Oversampling Technique) generates synthetic rare events by interpolating between nearest neighbors, but standard SMOTE destroys temporal dependencies. Time-series adapted T-SMOTE (2022) preserves autocorrelation structure while oversampling, achieving 15-20% minority class recall improvement when combined with ensemble methods, though requiring careful validation to avoid overfitting. **Curriculum learning** trains on progressively harder examples—starting with dense regions, moving to mixed, finally emphasizing sparse spikes—improving convergence stability by establishing good initial representations before tackling extreme sparsity.

## Metrics that matter: moving beyond MAPE to business-aligned evaluation

Traditional metrics fail catastrophically for sparse data. **MAPE becomes undefined when dividing by zero actual values**—a fatal flaw rendering it completely unusable for intermittent demand. A comprehensive empirical study on 3,671 retail intermittent series found that MAE, MAPE, sMAPE, and standard MASE all incorrectly ranked a "predict all zeros" forecast as best, while MSE, RMSSE, PIS, SPEC, and CFE correctly identified it as worst. This reveals a fundamental issue: metrics focusing solely on point-wise error magnitude miss temporal and business dimensions.

**MASE** (Mean Absolute Scaled Error, Hyndman & Koehler 2006) has emerged as the gold standard for comparing forecasts across sparse series. The formulation divides forecast MAE by the MAE of a naive seasonal forecast (typically lag-1 for non-seasonal data). MASE's critical advantage: **never undefined or infinite**, works with zero values, scale-free to enable cross-series comparison, and values below 1 indicate better-than-naive performance. For sparse data where naive forecasts already struggle, MASE provides interpretable context. Rob Hyndman's research demonstrates MASE should be the standard for comparing forecast accuracy across multiple time series, particularly intermittent demand—a recommendation adopted by M5 Competition (RMSSE, the root version) and increasingly by practitioners.

**SPEC** (Stock-keeping-oriented Prediction Error Costs, Martin et al. 2020) represents a major innovation specifically addressing sparse demand's business context. SPEC measures forecast error in actual business cost terms by simulating a warehouse where forecasts determine deliveries and actuals determine customer demand. The metric tracks two cost components: opportunity costs (unfulfilled demand due to underforecasting) and stock-keeping costs (excess inventory from overforecasting), with adjustable weights α₁ and α₂. Critically, **SPEC captures both magnitude and timing errors**—a forecast of 10 units one period late incurs costs as inventory accumulates before the actual demand. The calculation involves nested loops tracking cumulative effects over the forecast horizon. Empirical validation on automotive aftermarket and retail food data shows SPEC correlates r=0.964 (p<0.001) with actual business costs, while MAE correlates just r=-0.021 (not significant). This represents perhaps the single most important metric innovation for inventory-driven sparse forecasting, directly optimizing for business outcomes rather than statistical proxies.

**Probabilistic metrics** assess forecast distributions rather than point estimates. The **Continuous Ranked Probability Score** (CRPS) generalizes MAE to distributions by integrating squared differences between predicted and observed cumulative distribution functions. CRPS reduces to MAE for point forecasts but properly rewards well-calibrated uncertainty quantification. For sparse data where uncertainty is high (any given period likely zero but occasionally large spike), providing full distributions enables better risk management than point estimates. CRPS is a strictly proper scoring rule, meaning it incentivizes honest probabilistic predictions without gaming possibilities. Quantile scores evaluate specific percentiles with asymmetric penalties, while prediction interval coverage probability checks calibration—ideally 80% of actuals fall within 80% prediction intervals. These probabilistic metrics increasingly dominate modern benchmarks as foundation models (TimeGPT, Chronos, Moirai) emphasize zero-shot probabilistic forecasting.

**Spike-specific metrics** explicitly measure detection accuracy. Binary classification metrics—precision (what proportion of predicted spikes were actual spikes), recall (what proportion of actual spikes were detected), and **F1-score** (harmonic mean balancing both)—provide complementary views. For sparse data with severe class imbalance, F1-score must be interpreted carefully; recent innovations propose **Precision Gain, Recall Gain, and F1 Gain** metrics that transform to linear scales enabling meaningful averaging. The critical decision is defining spike thresholds: statistical (>μ+2σ), percentile-based (>95th percentile), or domain-specific business rules. This threshold choice dramatically affects metric values and must be documented explicitly.

A comprehensive evaluation suite for expense forecasting should include: (1) **MASE** for scale-free accuracy comparison, (2) **SPEC** or simpler **PIS** (Periods In Stock) for business impact, (3) **CRPS** or quantile scores for uncertainty assessment, (4) **F1-score** for spike detection with clearly defined thresholds, and (5) **CFE** (Cumulative Forecast Error) with CFE_max/CFE_min to reveal systematic bias over time. Using metrics from both accuracy and bias families prevents the misleading rankings that plagued traditional single-metric evaluations.

## From theory to production: implementation patterns for sparse forecasting

The transition from research to production requires navigating practical challenges that theory papers often elide. **Data preprocessing establishes the foundation** and mistakes here cascade through the entire pipeline. The critical first step is distinguishing structural zeros (legitimate no-activity periods that should remain zero) from missing data (actual gaps requiring imputation). Standard mean/median imputation destroys sparsity patterns and must be avoided. For truly missing observations, forward-fill or carry-forward imputation preserves temporal continuity better than statistical methods for sparse series. Normalization requires care—standard scaling creates negative values and numerical instability when applied to zero-heavy data. Recommended approaches include log1p transformation (log(1+y) handles zeros naturally), robust scaling using median and IQR instead of mean and standard deviation, or no scaling for tree-based models which are scale-invariant. Never normalize before train-test split to prevent leakage; for time series, use expanding window normalization considering only past data.

**Feature engineering for sparse data** fundamentally differs from standard time series features. Classic lag-1 and lag-2 features prove nearly useless when mostly zero. Instead, construct **event-based features**: days since last non-zero value, periods between occurrences, probability of occurrence in rolling windows, and last non-zero value (forward-filled). A powerful feature hierarchy includes temporal indicators (day of week, month, holidays using cyclical encoding), occurrence patterns (count of non-zeros in past N periods, trend in inter-event intervals), non-zero statistics (mean/std/skew of non-zero values only, rolling percentiles excluding zeros), and external regressors (promotions, pricing, competitor actions, economic indicators). For expense forecasting, features like days-since-last-expense often provide more signal than historical expense lags. External variables become more important than historical patterns when data is sparse—a project-based expense category might depend more on approved budget allocations and project timelines than on its own sparse history.

**Demand pattern classification using the SBC method** (Syntetos-Boylan-Croston) should precede model selection. Calculate two metrics: CV² (coefficient of variation squared) measuring magnitude variability, and p (average demand interval) measuring timing irregularity. These define four patterns: **Smooth** (p<1.32, CV²<0.49) responds well to ARIMA/ETS; **Erratic** (p<1.32, CV²≥0.49) needs robust error modeling; **Intermittent** (p≥1.32, CV²<0.49) suits Croston/TSB or two-stage ML; **Lumpy** (p≥1.32, CV²≥0.49) often requires safety stock approaches rather than forecast accuracy optimization. This classification enables **heterogeneous model assignment**—smooth categories get standard methods, intermittent and lumpy categories get specialized approaches. Many organizations implement this as an automated router: classify incoming series, route to appropriate model ensemble, apply category-specific evaluation metrics.

**Cold-start problems**—forecasting new items or categories with minimal history—plague expense forecasting for new vendors, projects, or cost centers. Transfer learning approaches train global models on large collections of related series, learning patterns that generalize to new instances. DeepAR and Temporal Fusion Transformers excel here by incorporating static metadata: for new vendor forecasting, include vendor category, contract type, payment terms, and historical category averages. Amazon Forecast claims 45% accuracy improvement for cold-start items using metadata-enhanced global models. Hierarchical forecasting provides an alternative: forecast at aggregate level (total vendor spending by category) then disaggregate to individuals using historical proportions or metadata-based splits. Bayesian approaches use priors from similar items, updating as data accumulates—elegant but computationally intensive at scale.

**Data augmentation techniques** address limited non-zero samples for training. Standard augmentation like permutation or cropping often breaks sparse data by creating all-zero segments or destroying temporal ordering. Recommended techniques include: jittering (adding small Gaussian noise to non-zero values only, preserving zeros), scaling (multiplying non-zero values by random factors 0.9-1.1), magnitude warping (applying smooth random multiplicative function preserving temporal structure), and pattern mixing (SMOTE-like interpolation between similar demand patterns with careful alignment). Advanced approaches use variational autoencoders (VAE) to learn latent representations of sparse patterns then generate synthetic series, though this requires substantial training data. Critical principle: augmented data must preserve sparsity ratio and statistical properties of originals, validated through distribution comparison tests.

**Technology stack selection** depends on scale. For small deployments (<1000 series), StatsForecast provides fast implementations of Croston, TSB, SBA, and ADIDA with scikit-learn-compatible APIs, while XGBoost/LightGBM handle ML models. Medium-scale (1000-10,000 series) benefits from Darts or sktime which wrap multiple libraries with unified interfaces and built-in backtesting. Large-scale or cold-start heavy applications justify GluonTS or AutoGluon for GPU-accelerated deep learning with native global model and metadata support. The practical recommendation: start with StatsForecast's Croston/TSB implementations as fast baselines, then add XGBoost two-stage model, ensemble them, and only invest in deep learning infrastructure if scale justifies the complexity. Many production systems maintain this hybrid architecture: simple methods for well-behaved categories, sophisticated approaches only where necessary.

**Implementation workflow** for expense forecasting: (1) Calculate sparsity and classify patterns using SBC for each expense category. (2) Segment into smooth, erratic, intermittent, and lumpy groups with different modeling strategies. (3) Engineer features: days-since-last-expense, rolling occurrence probability, external regressors (budget cycles, project timelines, seasonal factors), expense type embeddings. (4) Train baselines: Croston/TSB for intermittent categories, ARIMA for smooth categories, using MASE evaluation. (5) Develop two-stage ML model: LightGBM classifier for occurrence, Random Forest regressor for magnitude trained on non-zero values, combined multiplicatively. (6) For cold-start categories: train global TFT with expense type metadata, use hierarchical forecasts from parent categories. (7) Ensemble: combine Croston and ML forecasts (typically 50-50 or weighted by recent performance). (8) Evaluate using layered metrics: MASE for accuracy, SPEC for business cost alignment, F1-score for spike detection, CFE for bias monitoring. (9) Production: implement monitoring for sparsity drift, automatic retraining triggers when patterns change, fallback rules for extreme sparsity (\>99% zeros use safety stock rather than forecast accuracy).

## Synthesis: the sparse forecasting paradigm shift

The convergence of techniques across intermittent demand, rare event prediction, financial forecasting, and anomaly detection reveals a **coherent paradigm for sparse spike-dominated time series** that fundamentally differs from standard forecasting. This paradigm rests on five pillars: architectural decomposition separating occurrence from magnitude, loss functions that explicitly reweight rare events, evaluation metrics aligned with business outcomes rather than statistical conventions, feature engineering focused on event-based patterns rather than value-based lags, and model selection matched to sparsity characteristics rather than one-size-fits-all approaches.

The practical implications transform expense forecasting strategies. Traditional approaches applying ARIMA or simple neural networks to sparse expense categories while evaluating with MAPE fail on two dimensions—the methods optimize the wrong objective and the metrics reward the wrong behavior. The recommended alternative: classify expense categories by sparsity pattern, route intermittent and lumpy categories to specialized two-stage models with focal loss and quantile regression, evaluate using MASE and SPEC rather than MAPE, and augment sparse history with external regressors capturing budget cycles and project timelines. For ultra-sparse categories (\>95% zeros), acknowledge that forecast accuracy may be unattainable; shift to safety stock or contingency budget approaches where the goal is managing uncertainty rather than predicting point estimates.

Recent innovations from 2020-2025 have substantially advanced the frontier. SparseTSF demonstrates that lightweight models with explicit sparsity structure can match sophisticated transformers with 1000× fewer parameters. HAILS shows how distributional awareness—automatically selecting Poisson for sparse series and Gaussian for dense—combined with hierarchical consistency constraints achieves 23% improvement on sparse series. Foundation models like TimeGPT, Chronos, and Moirai enable transfer learning and zero-shot forecasting that could revolutionize cold-start scenarios. The SPEC metric finally provides business-cost-aligned evaluation that correlates 0.964 with actual inventory costs versus 0.021 for MAE.

Yet significant gaps remain. Few studies address concept drift in sparse data where regime changes are difficult to detect with few events. Transfer learning across domains (e.g., using retail demand patterns to inform financial expense forecasting) remains underexplored despite shared sparsity characteristics. Interpretability of ensemble two-stage models poses challenges for financial applications requiring audit trails. Multi-horizon forecasting for sparse data (maintaining accuracy 6-12 months out when monthly data has 90% zeros) needs more research. These gaps represent opportunities for continued advancement.

The central insight unifying sparse forecasting methods is recognizing that sparsity is not a minor complication requiring slight adaptations of standard approaches—it fundamentally changes the problem structure. Zero-heavy data with occasional spikes has different statistical properties, requires different loss functions, needs different evaluation metrics, and demands different features than regular time series. Success requires embracing this distinction through specialized methods validated across multiple domains. For practitioners implementing expense forecasting systems, the path forward is clear: adopt the sparse forecasting paradigm, implement two-stage architectures with appropriate losses and metrics, leverage external regressors and metadata for cold-start scenarios, and continuously evaluate whether forecast accuracy or uncertainty management better serves business needs for each category. The tools, methods, and understanding now exist to reliably forecast the unforeseen—turning sparse, spike-dominated data from an intractable challenge into a solvable problem.